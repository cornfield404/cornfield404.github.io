<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Build a LLM From Scratch | Corn Field</title><meta name="author" content="Corn"><meta name="copyright" content="Corn"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Understanding LLM LLM 与传统方法相比的优势 之前的方法在比较复杂的理解和生成任务中表现不佳，例如：上下文分析、生成连贯文本，同时只适用于一些特定的任务。而 LLM 可以在一系列任务上表现出较好的性能。LLM 成功的原因包括 transformer 结构以及大量的训练数据。 Stages of building and using LLMS">
<meta property="og:type" content="article">
<meta property="og:title" content="Build a LLM From Scratch">
<meta property="og:url" content="https://cornfield404.github.io/posts/881244c3/index.html">
<meta property="og:site_name" content="Corn Field">
<meta property="og:description" content="Understanding LLM LLM 与传统方法相比的优势 之前的方法在比较复杂的理解和生成任务中表现不佳，例如：上下文分析、生成连贯文本，同时只适用于一些特定的任务。而 LLM 可以在一系列任务上表现出较好的性能。LLM 成功的原因包括 transformer 结构以及大量的训练数据。 Stages of building and using LLMS">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cornfield404.github.io/img/avatar.jpg">
<meta property="article:published_time" content="2024-11-26T05:55:59.000Z">
<meta property="article:modified_time" content="2025-01-08T08:16:53.311Z">
<meta property="article:author" content="Corn">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="读书笔记">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cornfield404.github.io/img/avatar.jpg"><link rel="shortcut icon" href="/img/butterfly-icon.png"><link rel="canonical" href="https://cornfield404.github.io/posts/881244c3/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Build a LLM From Scratch',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  isShuoshuo: false
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="loading-box" onclick="document.getElementById(&quot;loading-box&quot;).classList.add(&quot;loaded&quot;)"><div class="loading-bg"><div class="loading-img"></div><div class="loading-image-dot"></div></div></div><script>const preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',()=> { preloader.endLoading() })

if (false) {
  document.addEventListener('pjax:send', () => { preloader.initLoading() })
  document.addEventListener('pjax:complete', () => { preloader.endLoading() })
}</script><script>window.paceOptions = {
  restartOnPushState: false
}

btf.addGlobalFn('pjaxSend', () => {
  Pace.restart()
}, 'pace_restart')

</script><link rel="stylesheet" href="/css/progress_bar.css"/><script src="https://cdn.jsdelivr.net/npm/pace-js/pace.min.js"></script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">37</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">44</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-book"></i><span> 技术积累</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/%E6%8A%80%E6%9C%AF%E7%A7%AF%E7%B4%AF/%E7%BC%96%E7%A8%8B%E6%8A%80%E5%B7%A7/"><i class="fa-fw fa fa-code"></i><span> 编程技巧</span></a></li><li><a class="site-page child" href="/categories/%E6%8A%80%E6%9C%AF%E7%A7%AF%E7%B4%AF/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><i class="fa-fw fa fa-newspaper"></i><span> 论文笔记</span></a></li><li><a class="site-page child" href="/categories/%E6%8A%80%E6%9C%AF%E7%A7%AF%E7%B4%AF/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"><i class="fa-fw fa fa-book-open"></i><span> 读书笔记</span></a></li><li><a class="site-page child" href="/categories/%E6%8A%80%E6%9C%AF%E7%A7%AF%E7%B4%AF/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/"><i class="fa-fw fa fa-paperclip"></i><span> 杂七杂八</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-clapperboard"></i><span> 休闲娱乐</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/%E4%BC%91%E9%97%B2%E5%A8%B1%E4%B9%90/%E4%B9%A6%E7%B1%8D%E9%98%85%E8%AF%BB/"><i class="fa-fw fa fa-book-open-reader"></i><span> 书籍阅读</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-camera"></i><span> 生活琐事</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/%E7%94%9F%E6%B4%BB%E7%90%90%E4%BA%8B/%E7%A5%9E%E5%A5%87%E6%8A%80%E8%83%BD/"><i class="fa-fw fa fa-person-skating"></i><span> 神奇技能</span></a></li><li><a class="site-page child" href="/categories/%E7%94%9F%E6%B4%BB%E7%90%90%E4%BA%8B/%E7%82%B9%E7%82%B9%E6%BB%B4%E6%BB%B4/"><i class="fa-fw fa fa-puzzle-piece"></i><span> 点点滴滴</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-folder-open"></i><span> 整理</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-th"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-bars"></i><span> 其他</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/random.html"><i class="fa-fw fas fa-random"></i><span> 随便逛逛</span></a></li><li><a class="site-page child" href="/artitalk/"><i class="fa-fw fa fa-heartbeat"></i><span> 碎碎念</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fa fa-user"></i><span> 关于我</span></a></li><li><a class="site-page child" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></li><li><a class="site-page child" href="/sitemap.xml"><i class="fa-fw fa fa-sitemap"></i><span> 站点地图</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/background.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Corn Field</span></a><a class="nav-page-title" href="/"><span class="site-name">Build a LLM From Scratch</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-book"></i><span> 技术积累</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/%E6%8A%80%E6%9C%AF%E7%A7%AF%E7%B4%AF/%E7%BC%96%E7%A8%8B%E6%8A%80%E5%B7%A7/"><i class="fa-fw fa fa-code"></i><span> 编程技巧</span></a></li><li><a class="site-page child" href="/categories/%E6%8A%80%E6%9C%AF%E7%A7%AF%E7%B4%AF/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><i class="fa-fw fa fa-newspaper"></i><span> 论文笔记</span></a></li><li><a class="site-page child" href="/categories/%E6%8A%80%E6%9C%AF%E7%A7%AF%E7%B4%AF/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"><i class="fa-fw fa fa-book-open"></i><span> 读书笔记</span></a></li><li><a class="site-page child" href="/categories/%E6%8A%80%E6%9C%AF%E7%A7%AF%E7%B4%AF/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/"><i class="fa-fw fa fa-paperclip"></i><span> 杂七杂八</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-clapperboard"></i><span> 休闲娱乐</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/%E4%BC%91%E9%97%B2%E5%A8%B1%E4%B9%90/%E4%B9%A6%E7%B1%8D%E9%98%85%E8%AF%BB/"><i class="fa-fw fa fa-book-open-reader"></i><span> 书籍阅读</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-camera"></i><span> 生活琐事</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/%E7%94%9F%E6%B4%BB%E7%90%90%E4%BA%8B/%E7%A5%9E%E5%A5%87%E6%8A%80%E8%83%BD/"><i class="fa-fw fa fa-person-skating"></i><span> 神奇技能</span></a></li><li><a class="site-page child" href="/categories/%E7%94%9F%E6%B4%BB%E7%90%90%E4%BA%8B/%E7%82%B9%E7%82%B9%E6%BB%B4%E6%BB%B4/"><i class="fa-fw fa fa-puzzle-piece"></i><span> 点点滴滴</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-folder-open"></i><span> 整理</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-th"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-bars"></i><span> 其他</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/random.html"><i class="fa-fw fas fa-random"></i><span> 随便逛逛</span></a></li><li><a class="site-page child" href="/artitalk/"><i class="fa-fw fa fa-heartbeat"></i><span> 碎碎念</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fa fa-user"></i><span> 关于我</span></a></li><li><a class="site-page child" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></li><li><a class="site-page child" href="/sitemap.xml"><i class="fa-fw fa fa-sitemap"></i><span> 站点地图</span></a></li></ul></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">Build a LLM From Scratch</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-11-26T05:55:59.000Z" title="发表于 2024-11-26 13:55:59">2024-11-26</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-01-08T08:16:53.311Z" title="更新于 2025-01-08 16:16:53">2025-01-08</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%8A%80%E6%9C%AF%E7%A7%AF%E7%B4%AF/">技术积累</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%8A%80%E6%9C%AF%E7%A7%AF%E7%B4%AF/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">读书笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h2 id="Understanding-LLM">Understanding LLM</h2>
<h3 id="LLM-与传统方法相比的优势">LLM 与传统方法相比的优势</h3>
<p>之前的方法在比较复杂的理解和生成任务中表现不佳，例如：上下文分析、生成连贯文本，同时只适用于一些特定的任务。而 LLM 可以在一系列任务上表现出较好的性能。LLM 成功的原因包括 transformer 结构以及大量的训练数据。</p>
<h3 id="Stages-of-building-and-using-LLMS">Stages of building and using LLMS</h3>
<p>目前的研究表明，模型在特定任务或特定领域上训练后可以表现出超过通用 LLM 的性能。因此 LLM 训练通常包括两步：</p>
<ul>
<li>Pretrain 指模型在大规模数据库上训练，使模型具有语言理解能力，这一阶段的数据是不需要标注的；</li>
<li>Finetune 指模型在细分领域/细分任务的数据上训练，目前比较常用的 finetune 方式包括两类：
<ul>
<li>Instruction finetuning：标注数据由 instruction 和 answer 组成，例如：要求模型翻译某段话的 query 以及正确的翻译后的文本；</li>
<li>Classification finetuning：标注数据由文本以及对应的类别组成，例如：邮件内容以及该邮件是否是垃圾邮件的标注。<br>
<img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-11-26%2014.09.22.png" alt="截屏2024-11-26 14.09.22.png"></li>
</ul>
</li>
</ul>
<hr>
<h2 id="Working-with-text-data">Working with text data</h2>
<h3 id="Understanding-word-embeddings">Understanding word embeddings</h3>
<p>Text 需要被转换成 vector 才能被模型处理，这个过程被称为 embedding。Embedding 有很多方法：</p>
<ul>
<li>Word2vec：核心思路是在相近的上下文中出现的词应该具有相似的含义；</li>
<li>作为模型的一部分在训练期间不断更新：这种方法的优势是训练出的 embedding 与自己的任务和数据更适配。</li>
</ul>
<h3 id="Tokenizing-text">Tokenizing text</h3>
<p>每个单词，每个标点符号都作为一个单独的 token。空格是否保留可以根据需要，如果是对格式非常严格的任务，例如代码生成，那么缩进和空格就有保留的必要，对于对格式要求不严格的任务，也可以删掉以节省存储空间。<br>
<img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-11-26%2015.49.54.png" alt="截屏2024-11-26 15.49.54.png"></p>
<h3 id="Converting-tokens-into-token-IDs">Converting tokens into token IDs</h3>
<p>给文本里所有出现过的 token 一个编码，这样每次读到一段 text，就可以通过转换把 text 转换成编码。LLM 的输出结果可以通过反向转换，转换到 text。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleTokenizerV1</span>: </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab</span>): </span><br><span class="line">        <span class="variable language_">self</span>.str_to_int = vocab </span><br><span class="line">        <span class="variable language_">self</span>.int_to_str = &#123;i:s <span class="keyword">for</span> s,i <span class="keyword">in</span> vocab.items()&#125; </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, text</span>): </span><br><span class="line">        preprocessed = re.split(<span class="string">r&#x27;([,.?_!&quot;()\&#x27;]|--|\s)&#x27;</span>, text)</span><br><span class="line">        preprocessed = [ </span><br><span class="line">            item.strip() <span class="keyword">for</span> item <span class="keyword">in</span> preprocessed <span class="keyword">if</span> item.strip() </span><br><span class="line">        ] </span><br><span class="line">        ids = [<span class="variable language_">self</span>.str_to_int[s] <span class="keyword">for</span> s <span class="keyword">in</span> preprocessed] </span><br><span class="line">        <span class="keyword">return</span> ids </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode</span>(<span class="params">self, ids</span>): </span><br><span class="line">        text = <span class="string">&quot; &quot;</span>.join([<span class="variable language_">self</span>.int_to_str[i] <span class="keyword">for</span> i <span class="keyword">in</span> ids])</span><br><span class="line">        <span class="comment"># 处理标点符号前的空格 </span></span><br><span class="line">        text = re.sub(<span class="string">r&#x27;\s+([,.?!&quot;()\&#x27;])&#x27;</span>, <span class="string">r&#x27;\1&#x27;</span>, text) </span><br><span class="line">        <span class="keyword">return</span> text</span><br></pre></td></tr></table></figure>
<h3 id="Adding-special-context-tokens">Adding special context tokens</h3>
<p>为了应对 text 里可能包含的没见过的词，需要增加 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>&lt;</mo><mi mathvariant="normal">∣</mi><mi>u</mi><mi>n</mi><mi>k</mi><mi mathvariant="normal">∣</mi><mo>&gt;</mo></mrow><annotation encoding="application/x-tex">&lt;|unk|&gt;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∣</span><span class="mord mathnormal">u</span><span class="mord mathnormal" style="margin-right:0.03148em;">nk</span><span class="mord">∣</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&gt;</span></span></span></span> token。同时也可以增加一些用于帮助文本理解的 token，例如用 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>&lt;</mo><mi mathvariant="normal">∣</mi><mi>e</mi><mi>n</mi><mi>d</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi><mi mathvariant="normal">∣</mi><mo>&gt;</mo></mrow><annotation encoding="application/x-tex">&lt;|endoftext|&gt;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∣</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">d</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal">x</span><span class="mord mathnormal">t</span><span class="mord">∣</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&gt;</span></span></span></span> 表明文档的末尾，这样在用大量文本训练时，可以使模型更好地判断哪些文本是有关联的。</p>
<h3 id="Byte-pair-encoding">Byte pair encoding</h3>
<p>在 tiktoken 库（ <a target="_blank" rel="noopener" href="https://github.com/openai/tiktoken">https://github.com/openai/tiktoken</a> ）里有实现，可用 pip 安装。BPE 遇到没见过的单词会把这些词拆解成 subword 后再进行 encode，这样就不需要 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>&lt;</mo><mi mathvariant="normal">∣</mi><mi>u</mi><mi>n</mi><mi>k</mi><mi mathvariant="normal">∣</mi><mo>&gt;</mo></mrow><annotation encoding="application/x-tex">&lt;|unk|&gt;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∣</span><span class="mord mathnormal">u</span><span class="mord mathnormal" style="margin-right:0.03148em;">nk</span><span class="mord">∣</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&gt;</span></span></span></span> token 进行处理。BPE 的基本原理是首先把每个字母加入 vocabulary，然后把同时出现的概率较高的字母组合为 subword，逐步生成 word。<br>
<img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-11-27%2010.51.33.png" alt="截屏2024-11-27 10.51.33.png"></p>
<h3 id="Data-sampling-with-a-sliding-window">Data sampling with a sliding window</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader </span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GPTDatasetV1</span>(<span class="title class_ inherited__">Dataset</span>): </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, txt, tokenizer, max_length, stride</span>):</span><br><span class="line">        <span class="variable language_">self</span>.input_ids = [] </span><br><span class="line">        <span class="variable language_">self</span>.target_ids = [] </span><br><span class="line">        </span><br><span class="line">        token_ids = tokenizer.encode(txt)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(token_ids) - max_length, stride):</span><br><span class="line">            input_chunk = token_ids[i:i + max_length] </span><br><span class="line">            target_chunk = token_ids[i + <span class="number">1</span>: i + max_length + <span class="number">1</span>]</span><br><span class="line">            <span class="variable language_">self</span>.input_ids.append(torch.tensor(input_chunk))</span><br><span class="line">            <span class="variable language_">self</span>.target_ids.append(torch.tensor(target_chunk))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>): </span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.input_ids) </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>): </span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.input_ids[idx], <span class="variable language_">self</span>.target_ids[idx]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_dataloader_v1</span>(<span class="params">txt, batch_size=<span class="number">4</span>, max_length=<span class="number">256</span>, </span></span><br><span class="line"><span class="params">                         stride=<span class="number">128</span>, shuffle=<span class="literal">True</span>, drop_last=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">                         num_workers=<span class="number">0</span></span>): </span><br><span class="line">    tokenizer = tiktoken.get_encoding(<span class="string">&quot;gpt2&quot;</span>) </span><br><span class="line">    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)</span><br><span class="line">    dataloader = DataLoader( </span><br><span class="line">        dataset, </span><br><span class="line">        batch_size=batch_size, </span><br><span class="line">        shuffle=shuffle, </span><br><span class="line">        drop_last=drop_last, </span><br><span class="line">        num_workers=num_workers </span><br><span class="line">    ) </span><br><span class="line">    <span class="keyword">return</span> dataloader</span><br></pre></td></tr></table></figure>
<h3 id="Creating-token-embeddings">Creating token embeddings</h3>
<p>用 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mi>o</mi><mi>r</mi><mi>c</mi><mi>h</mi><mi mathvariant="normal">.</mi><mi>n</mi><mi>n</mi><mi mathvariant="normal">.</mi><mi>E</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>d</mi><mi>d</mi><mi>i</mi><mi>n</mi><mi>g</mi></mrow><annotation encoding="application/x-tex">torch.nn.Embedding</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">orc</span><span class="mord mathnormal">h</span><span class="mord">.</span><span class="mord mathnormal">nn</span><span class="mord">.</span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mord mathnormal">mb</span><span class="mord mathnormal">e</span><span class="mord mathnormal">dd</span><span class="mord mathnormal">in</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span></span></span></span> 将 index 映射为可训练的 embedding。</p>
<h3 id="Embedding-word-positions">Embedding word positions</h3>
<p>Position embedding 主要分为两类：</p>
<ul>
<li>Absolute position embedding：根据 token 在 sequence 里的绝对位置 embedding；</li>
<li>Relative position embedding：学习 tokens 之间的相对距离，可以使模型在不同长度的 sequence 上泛化地更好。</li>
</ul>
<hr>
<h2 id="Coding-attention-mechanisms">Coding attention mechanisms</h2>
<h3 id="The-problem-with-modeling-long-sequences">The problem with modeling long sequences</h3>
<p>在采用 attention 结构前，比较常用的语言处理模型是 RNN 结构。在 RNN 结构中，encoder 把输入文本的全部内容保存在一个 hidden state 内部，然后 decoder 用这个 hidden state 生成输出。<br>
<img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-11-27%2015.26.48.png" alt="截屏2024-11-27 15.26.48.png"><br>
这种结构的问题是在 decoder 过程中，encoder 早期的 hidden state 被全部丢弃了，仅采用了最后一个 hidden state，这会导致信息丢失。</p>
<h3 id="Implementing-self-attention-with-trainable-weights">Implementing self-attention with trainable weights</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SelfAttention_v2</span>(nn.Module): </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_in, d_out, qkv_bias=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__() </span><br><span class="line">        <span class="variable language_">self</span>.W_query = nn.Linear(d_in, d_out, bias=qkv_bias) </span><br><span class="line">        <span class="variable language_">self</span>.W_key = nn.Linear(d_in, d_out, bias=qkv_bias) </span><br><span class="line">        <span class="variable language_">self</span>.W_value = nn.Linear(d_in, d_out, bias=qkv_bias) </span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>): </span><br><span class="line">        keys = <span class="variable language_">self</span>.W_key(x) </span><br><span class="line">        queries = <span class="variable language_">self</span>.W_query(x) </span><br><span class="line">        values = <span class="variable language_">self</span>.W_value(x) </span><br><span class="line">        attn_scores = queries @ keys.T </span><br><span class="line">        attn_weights = torch.softmax( </span><br><span class="line">            attn_scores / keys.shape[-<span class="number">1</span>]**<span class="number">0.5</span>, dim=-<span class="number">1</span> </span><br><span class="line">        ) </span><br><span class="line">        context_vec = attn_weights @ values </span><br><span class="line">        <span class="keyword">return</span> context_vec</span><br></pre></td></tr></table></figure>
<p>Normalize 是为了防止点积结果过大，从而落在梯度较低的区域，影响训练速度。</p>
<h3 id="Hiding-future-words-with-causal-attention">Hiding future words with causal attention</h3>
<p>自然语言任务里，经常需要仅与前面的输入做 attention。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CausalAttention</span>(nn.Module): </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_in, d_out, context_length, </span></span><br><span class="line"><span class="params">                dropout, qkv_bias=<span class="literal">False</span></span>): </span><br><span class="line">        <span class="built_in">super</span>().__init__() </span><br><span class="line">        <span class="variable language_">self</span>.d_out = d_out </span><br><span class="line">        <span class="variable language_">self</span>.W_query = nn.Linear(d_in, d_out, bias=qkv_bias) </span><br><span class="line">        <span class="variable language_">self</span>.W_key = nn.Linear(d_in, d_out, bias=qkv_bias) </span><br><span class="line">        <span class="variable language_">self</span>.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout) </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># register_buffer可以使变量随着模型在不同device上移动，不需要手动处理</span></span><br><span class="line">        <span class="variable language_">self</span>.register_buffer( </span><br><span class="line">            <span class="string">&#x27;mask&#x27;</span>, </span><br><span class="line">            torch.triu(torch.ones(context_length, context_length),</span><br><span class="line">            diagonal=<span class="number">1</span>) </span><br><span class="line">        ) </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>): </span><br><span class="line">        b, num_tokens, d_in = x.shape </span><br><span class="line">        keys = <span class="variable language_">self</span>.W_key(x) </span><br><span class="line">        queries = <span class="variable language_">self</span>.W_query(x) </span><br><span class="line">        values = <span class="variable language_">self</span>.W_value(x) </span><br><span class="line">        </span><br><span class="line">        attn_scores = queries @ keys.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 加入mask防止与后面的token计算attention</span></span><br><span class="line">        attn_scores.masked_fill_( </span><br><span class="line">            <span class="variable language_">self</span>.mask.<span class="built_in">bool</span>()[:num_tokens, :num_tokens], -torch.inf</span><br><span class="line">        ) </span><br><span class="line">        attn_weights = torch.softmax( </span><br><span class="line">            attn_scores / keys.shape[-<span class="number">1</span>]**<span class="number">0.5</span>, dim=-<span class="number">1</span> </span><br><span class="line">        ) </span><br><span class="line">        <span class="comment"># 加入dropout防止过拟合</span></span><br><span class="line">        attn_weights = <span class="variable language_">self</span>.dropout(attn_weights)</span><br><span class="line">        </span><br><span class="line">        context_vec = attn_weights @ values </span><br><span class="line">        <span class="keyword">return</span> context_vec</span><br></pre></td></tr></table></figure>
<h3 id="Extending-single-head-attention-to-multi-head-attention">Extending single-head attention to multi-head attention</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module): </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_in, d_out, </span></span><br><span class="line"><span class="params">                 context_length, dropout, num_heads, qkv_bias=<span class="literal">False</span></span>): </span><br><span class="line">        <span class="built_in">super</span>().__init__() </span><br><span class="line">        <span class="keyword">assert</span> (d_out % num_heads == <span class="number">0</span>), \ </span><br><span class="line">            <span class="string">&quot;d_out must be divisible by num_heads&quot;</span> </span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.d_out = d_out </span><br><span class="line">        <span class="variable language_">self</span>.num_heads = num_heads </span><br><span class="line">        <span class="variable language_">self</span>.head_dim = d_out // num_heads </span><br><span class="line">        <span class="variable language_">self</span>.W_query = nn.Linear(d_in, d_out, bias=qkv_bias) </span><br><span class="line">        <span class="variable language_">self</span>.W_key = nn.Linear(d_in, d_out, bias=qkv_bias) </span><br><span class="line">        <span class="variable language_">self</span>.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)</span><br><span class="line">        <span class="variable language_">self</span>.out_proj = nn.Linear(d_out, d_out) </span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout) </span><br><span class="line">        <span class="variable language_">self</span>.register_buffer( </span><br><span class="line">            <span class="string">&quot;mask&quot;</span>, </span><br><span class="line">            torch.triu(torch.ones(context_length, context_length),</span><br><span class="line">                       diagonal=<span class="number">1</span>) </span><br><span class="line">        ) </span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>): </span><br><span class="line">        b, num_tokens, d_in = x.shape </span><br><span class="line">        keys = <span class="variable language_">self</span>.W_key(x) </span><br><span class="line">        queries = <span class="variable language_">self</span>.W_query(x) </span><br><span class="line">        values = <span class="variable language_">self</span>.W_value(x)</span><br><span class="line">        keys = keys.view(b, num_tokens, <span class="variable language_">self</span>.num_heads, <span class="variable language_">self</span>.head_dim)</span><br><span class="line">        values = values.view(</span><br><span class="line">            b, num_tokens, <span class="variable language_">self</span>.num_heads, <span class="variable language_">self</span>.head_dim</span><br><span class="line">        ) </span><br><span class="line">        queries = queries.view( </span><br><span class="line">            b, num_tokens, <span class="variable language_">self</span>.num_heads, <span class="variable language_">self</span>.head_dim </span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        keys = keys.transpose(<span class="number">1</span>, <span class="number">2</span>) </span><br><span class="line">        queries = queries.transpose(<span class="number">1</span>, <span class="number">2</span>) </span><br><span class="line">        values = values.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        attn_scores = queries @ keys.transpose(<span class="number">2</span>, <span class="number">3</span>) </span><br><span class="line">        mask_bool = <span class="variable language_">self</span>.mask.<span class="built_in">bool</span>()[:num_tokens, :num_tokens]</span><br><span class="line">        </span><br><span class="line">        attn_scores.masked_fill_(mask_bool, -torch.inf)</span><br><span class="line">        </span><br><span class="line">        attn_weights = torch.softmax( </span><br><span class="line">            attn_scores / keys.shape[-<span class="number">1</span>]**<span class="number">0.5</span>, dim=-<span class="number">1</span>) </span><br><span class="line">        attn_weights = <span class="variable language_">self</span>.dropout(attn_weights)</span><br><span class="line">        </span><br><span class="line">        context_vec = (attn_weights @ values).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        context_vec = context_vec.contiguous().view( </span><br><span class="line">            b, num_tokens, <span class="variable language_">self</span>.d_out </span><br><span class="line">        ) </span><br><span class="line">        context_vec = <span class="variable language_">self</span>.out_proj(context_vec) </span><br><span class="line">        <span class="keyword">return</span> context_vec</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="Implementing-a-GPT-model-from-scratch-to-generate-text">Implementing a GPT model from scratch to generate text</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerBlock</span>(nn.Module): </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, cfg</span>): </span><br><span class="line">        <span class="built_in">super</span>().__init__() </span><br><span class="line">        <span class="variable language_">self</span>.att = MultiHeadAttention( </span><br><span class="line">            d_in=cfg[<span class="string">&quot;emb_dim&quot;</span>], </span><br><span class="line">            d_out=cfg[<span class="string">&quot;emb_dim&quot;</span>], </span><br><span class="line">            context_length=cfg[<span class="string">&quot;context_length&quot;</span>],</span><br><span class="line">            num_heads=cfg[<span class="string">&quot;n_heads&quot;</span>], </span><br><span class="line">            dropout=cfg[<span class="string">&quot;drop_rate&quot;</span>], </span><br><span class="line">            qkv_bias=cfg[<span class="string">&quot;qkv_bias&quot;</span>]) </span><br><span class="line">        <span class="variable language_">self</span>.ff = FeedForward(cfg) </span><br><span class="line">        <span class="variable language_">self</span>.norm1 = LayerNorm(cfg[<span class="string">&quot;emb_dim&quot;</span>]) </span><br><span class="line">        <span class="variable language_">self</span>.norm2 = LayerNorm(cfg[<span class="string">&quot;emb_dim&quot;</span>]) </span><br><span class="line">        <span class="variable language_">self</span>.drop_shortcut = nn.Dropout(cfg[<span class="string">&quot;drop_rate&quot;</span>]) </span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>): </span><br><span class="line">        shortcut = x </span><br><span class="line">        x = <span class="variable language_">self</span>.norm1(x) </span><br><span class="line">        x = <span class="variable language_">self</span>.att(x) </span><br><span class="line">        x = <span class="variable language_">self</span>.drop_shortcut(x) </span><br><span class="line">        x = x + shortcut </span><br><span class="line">        </span><br><span class="line">        shortcut = x </span><br><span class="line">        x = <span class="variable language_">self</span>.norm2(x) </span><br><span class="line">        x = <span class="variable language_">self</span>.ff(x) </span><br><span class="line">        x = <span class="variable language_">self</span>.drop_shortcut(x) </span><br><span class="line">        x = x + shortcut </span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GPTModel</span>(nn.Module): </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, cfg</span>): </span><br><span class="line">        <span class="built_in">super</span>().__init__() </span><br><span class="line">        <span class="variable language_">self</span>.tok_emb = nn.Embedding(cfg[<span class="string">&quot;vocab_size&quot;</span>], cfg[<span class="string">&quot;emb_dim&quot;</span>])</span><br><span class="line">        <span class="variable language_">self</span>.pos_emb = nn.Embedding(</span><br><span class="line">            cfg[<span class="string">&quot;context_length&quot;</span>], cfg[<span class="string">&quot;emb_dim&quot;</span>]</span><br><span class="line">        ) </span><br><span class="line">        <span class="variable language_">self</span>.drop_emb = nn.Dropout(cfg[<span class="string">&quot;drop_rate&quot;</span>]) </span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.trf_blocks = nn.Sequential( </span><br><span class="line">            *[TransformerBlock(cfg) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(cfg[<span class="string">&quot;n_layers&quot;</span>])])</span><br><span class="line">            </span><br><span class="line">        <span class="variable language_">self</span>.final_norm = LayerNorm(cfg[<span class="string">&quot;emb_dim&quot;</span>]) </span><br><span class="line">        <span class="variable language_">self</span>.out_head = nn.Linear( </span><br><span class="line">            cfg[<span class="string">&quot;emb_dim&quot;</span>], cfg[<span class="string">&quot;vocab_size&quot;</span>], bias=<span class="literal">False</span> </span><br><span class="line">        ) </span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, in_idx</span>): </span><br><span class="line">        batch_size, seq_len = in_idx.shape </span><br><span class="line">        tok_embeds = <span class="variable language_">self</span>.tok_emb(in_idx) </span><br><span class="line">        </span><br><span class="line">        pos_embeds = <span class="variable language_">self</span>.pos_emb( </span><br><span class="line">            torch.arange(seq_len, device=in_idx.device) </span><br><span class="line">        ) </span><br><span class="line">        x = tok_embeds + pos_embeds </span><br><span class="line">        x = <span class="variable language_">self</span>.drop_emb(x) </span><br><span class="line">        x = <span class="variable language_">self</span>.trf_blocks(x) </span><br><span class="line">        x = <span class="variable language_">self</span>.final_norm(x) </span><br><span class="line">        logits = <span class="variable language_">self</span>.out_head(x) </span><br><span class="line">        <span class="keyword">return</span> logits</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_text_simple</span>(<span class="params">model, idx, max_new_tokens, context_size</span>): </span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(max_new_tokens): </span><br><span class="line">        idx_cond = idx[:, -context_size:] </span><br><span class="line">        <span class="keyword">with</span> torch.no_grad(): </span><br><span class="line">            logits = model(idx_cond) </span><br><span class="line">        </span><br><span class="line">        logits = logits[:, -<span class="number">1</span>, :] </span><br><span class="line">        probas = torch.softmax(logits, dim=-<span class="number">1</span>) </span><br><span class="line">        idx_next = torch.argmax(probas, dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>) </span><br><span class="line">        idx = torch.cat((idx, idx_next), dim=<span class="number">1</span>) </span><br><span class="line">    <span class="keyword">return</span> idx</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="Pretraining-on-unlabeled-data">Pretraining on unlabeled data</h2>
<h3 id="Evaluating-generative-text-models">Evaluating generative text models</h3>
<ul>
<li>采用 cross entropy loss；</li>
<li>Perplexity 也被用来评估模型，该指标用 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mi>e</mi><mi>r</mi><mi>p</mi><mi>l</mi><mi>e</mi><mi>x</mi><mi>i</mi><mi>t</mi><mi>y</mi><mo>=</mo><mi>t</mi><mi>o</mi><mi>r</mi><mi>c</mi><mi>h</mi><mi mathvariant="normal">.</mi><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mi>l</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">perplexity=torch.exp(loss)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">p</span><span class="mord mathnormal" style="margin-right:0.02778em;">er</span><span class="mord mathnormal" style="margin-right:0.01968em;">pl</span><span class="mord mathnormal">e</span><span class="mord mathnormal">x</span><span class="mord mathnormal">i</span><span class="mord mathnormal">t</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">orc</span><span class="mord mathnormal">h</span><span class="mord">.</span><span class="mord mathnormal">e</span><span class="mord mathnormal">x</span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">oss</span><span class="mclose">)</span></span></span></span> 计算，该指标的含义是模型认为有多少个可能且合理的备选，该指标越大，表示模型认为有更多可能的备选，模型不确定性越高。</li>
</ul>
<h3 id="Training-an-LLM">Training an LLM</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_model_simple</span>(<span class="params">model, train_loader, val_loader, </span></span><br><span class="line"><span class="params">                       optimizer, device, num_epochs, </span></span><br><span class="line"><span class="params">                       eval_freq, eval_iter, start_context, tokenizer</span>):</span><br><span class="line">    train_losses, val_losses, track_tokens_seen = [], [], []</span><br><span class="line">    tokens_seen, global_step = <span class="number">0</span>, -<span class="number">1</span> </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs): </span><br><span class="line">        model.train() </span><br><span class="line">        <span class="keyword">for</span> input_batch, target_batch <span class="keyword">in</span> train_loader:</span><br><span class="line">            optimizer.zero_grad() </span><br><span class="line">            loss = calc_loss_batch( </span><br><span class="line">                input_batch, target_batch, model, device </span><br><span class="line">            ) </span><br><span class="line">            loss.backward() </span><br><span class="line">            optimizer.step() </span><br><span class="line">            tokens_seen += input_batch.numel() </span><br><span class="line">            global_step += <span class="number">1</span> </span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> global_step % eval_freq == <span class="number">0</span>: </span><br><span class="line">                train_loss, val_loss = evaluate_model( </span><br><span class="line">                    model, train_loader, val_loader, device, eval_iter)</span><br><span class="line">                train_losses.append(train_loss)</span><br><span class="line">                val_losses.append(val_loss)</span><br><span class="line">                track_tokens_seen.append(tokens_seen) </span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;Ep <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span> (Step <span class="subst">&#123;global_step:06d&#125;</span>): &quot;</span> </span><br><span class="line">                      <span class="string">f&quot;Train loss <span class="subst">&#123;train_loss:<span class="number">.3</span>f&#125;</span>, &quot;</span> </span><br><span class="line">                      <span class="string">f&quot;Val loss <span class="subst">&#123;val_loss:<span class="number">.3</span>f&#125;</span>&quot;</span> </span><br><span class="line">                )</span><br><span class="line">        generate_and_print_sample( </span><br><span class="line">            model, tokenizer, device, start_context </span><br><span class="line">        ) </span><br><span class="line">    <span class="keyword">return</span> train_losses, val_losses, track_tokens_seen</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calc_loss_batch</span>(<span class="params">input_batch, target_batch, model, device</span>):</span><br><span class="line">    input_batch = input_batch.to(device) </span><br><span class="line">    target_batch = target_batch.to(device) </span><br><span class="line">    logits = model(input_batch) </span><br><span class="line">    loss = torch.nn.functional.cross_entropy( </span><br><span class="line">        logits.flatten(<span class="number">0</span>, <span class="number">1</span>), target_batch.flatten() </span><br><span class="line">    ) </span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calc_loss_loader</span>(<span class="params">data_loader, model, device, num_batches=<span class="literal">None</span></span>):</span><br><span class="line">    total_loss = <span class="number">0.</span> </span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(data_loader) == <span class="number">0</span>: </span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">float</span>(<span class="string">&quot;nan&quot;</span>) </span><br><span class="line">    <span class="keyword">elif</span> num_batches <span class="keyword">is</span> <span class="literal">None</span>: </span><br><span class="line">        num_batches = <span class="built_in">len</span>(data_loader) </span><br><span class="line">    <span class="keyword">else</span>: </span><br><span class="line">        num_batches = <span class="built_in">min</span>(num_batches, <span class="built_in">len</span>(data_loader)) </span><br><span class="line">    <span class="keyword">for</span> i, (input_batch, target_batch) <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_loader): </span><br><span class="line">        <span class="keyword">if</span> i &lt; num_batches: </span><br><span class="line">            loss = calc_loss_batch( </span><br><span class="line">                input_batch, target_batch, model, device </span><br><span class="line">            ) </span><br><span class="line">            total_loss += loss.item() </span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            <span class="keyword">break</span> </span><br><span class="line">    <span class="keyword">return</span> total_loss / num_batches</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_model</span>(<span class="params">model, train_loader, val_loader, device, eval_iter</span>):</span><br><span class="line">    model.<span class="built_in">eval</span>() </span><br><span class="line">    <span class="keyword">with</span> torch.no_grad(): </span><br><span class="line">        train_loss = calc_loss_loader( </span><br><span class="line">            train_loader, model, device, num_batches=eval_iter</span><br><span class="line">        ) </span><br><span class="line">        val_loss = calc_loss_loader( </span><br><span class="line">            val_loader, model, device, num_batches=eval_iter </span><br><span class="line">        ) </span><br><span class="line">    model.train() </span><br><span class="line">    <span class="keyword">return</span> train_loss, val_loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_and_print_sample</span>(<span class="params">model, tokenizer, device, start_context</span>):</span><br><span class="line">    model.<span class="built_in">eval</span>() </span><br><span class="line">    context_size = model.pos_emb.weight.shape[<span class="number">0</span>] </span><br><span class="line">    encoded = text_to_token_ids(start_context, tokenizer).to(device)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad(): </span><br><span class="line">        token_ids = generate_text_simple( </span><br><span class="line">            model=model, idx=encoded, </span><br><span class="line">            max_new_tokens=<span class="number">50</span>, context_size=context_size </span><br><span class="line">        ) </span><br><span class="line">    decoded_text = token_ids_to_text(token_ids, tokenizer)</span><br><span class="line">    <span class="built_in">print</span>(decoded_text.replace(<span class="string">&quot;\n&quot;</span>, <span class="string">&quot; &quot;</span>)) </span><br><span class="line">    model.train()</span><br></pre></td></tr></table></figure>
<h3 id="Decoding-strategies-to-control-randomness">Decoding strategies to control randomness</h3>
<p>之前的生成模型每次的生成结果都是一致的，为了增加多样性的同时保证合理性，可以采取两个措施：</p>
<ul>
<li>把固定选择概率最大的词作为输出，改为根据概率采样，即：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mi>o</mi><mi>r</mi><mi>c</mi><mi>h</mi><mi mathvariant="normal">.</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>m</mi><mi>a</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">torch.argmax</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">orc</span><span class="mord mathnormal">h</span><span class="mord">.</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span></span></span></span> 改为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mi>o</mi><mi>r</mi><mi>c</mi><mi>h</mi><mi mathvariant="normal">.</mi><mi>m</mi><mi>u</mi><mi>l</mi><mi>t</mi><mi>i</mi><mi>n</mi><mi>o</mi><mi>m</mi><mi>i</mi><mi>a</mi><mi>l</mi></mrow><annotation encoding="application/x-tex">torch.multinomial</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">orc</span><span class="mord mathnormal">h</span><span class="mord">.</span><span class="mord mathnormal">m</span><span class="mord mathnormal">u</span><span class="mord mathnormal">lt</span><span class="mord mathnormal">in</span><span class="mord mathnormal">o</span><span class="mord mathnormal">mia</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span></span></span></span>；</li>
<li>仅在 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mi>o</mi><mi>p</mi><mtext> </mtext><mi>k</mi></mrow><annotation encoding="application/x-tex">top\ k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">o</span><span class="mord mathnormal">p</span><span class="mspace"> </span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span> 样本中进行采样，防止采到特别离谱的结果，用 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mi>o</mi><mi>r</mi><mi>c</mi><mi>h</mi><mi mathvariant="normal">.</mi><mi>w</mi><mi>h</mi><mi>e</mi><mi>r</mi><mi>e</mi></mrow><annotation encoding="application/x-tex">torch.where</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">orc</span><span class="mord mathnormal">h</span><span class="mord">.</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mord mathnormal">h</span><span class="mord mathnormal">ere</span></span></span></span> 把非 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mi>o</mi><mi>p</mi><mtext> </mtext><mi>k</mi></mrow><annotation encoding="application/x-tex">top\ k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">o</span><span class="mord mathnormal">p</span><span class="mspace"> </span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span> 的概率置为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mi>inf</mi><mo>⁡</mo></mrow><annotation encoding="application/x-tex">-\inf</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em;"></span><span class="mord">−</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">in<span style="margin-right:0.07778em;">f</span></span></span></span></span>，这样 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">softmax</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">so</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">t</span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span></span></span></span> 后这些位置的采样概率就会变成 0。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">generate</span>(<span class="params">model, idx, max_new_tokens, context_size, </span></span><br><span class="line"><span class="params">             temperature=<span class="number">0.0</span>, top_k=<span class="literal">None</span>, eos_id=<span class="literal">None</span></span>): </span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(max_new_tokens): </span><br><span class="line">        idx_cond = idx[:, -context_size:] </span><br><span class="line">        <span class="keyword">with</span> torch.no_grad(): </span><br><span class="line">            logits = model(idx_cond) </span><br><span class="line">        logits = logits[:, -<span class="number">1</span>, :]</span><br><span class="line">        <span class="keyword">if</span> top_k <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: </span><br><span class="line">            top_logits, _ = torch.topk(logits, top_k) </span><br><span class="line">            min_val = top_logits[:, -<span class="number">1</span>] </span><br><span class="line">            logits = torch.where( </span><br><span class="line">                logits &lt; min_val, </span><br><span class="line">                torch.tensor(<span class="built_in">float</span>(<span class="string">&#x27;-inf&#x27;</span>)).to(logits.device), </span><br><span class="line">                logits </span><br><span class="line">            ) </span><br><span class="line">        <span class="keyword">if</span> temperature &gt; <span class="number">0.0</span>: </span><br><span class="line">            logits = logits / temperature </span><br><span class="line">            probs = torch.softmax(logits, dim=-<span class="number">1</span>) </span><br><span class="line">            idx_next = torch.multinomial(probs, num_samples=<span class="number">1</span>) </span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            idx_next = torch.argmax(logits, dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>) </span><br><span class="line">        <span class="keyword">if</span> idx_next == eos_id: </span><br><span class="line">            <span class="keyword">break</span> </span><br><span class="line">        idx = torch.cat((idx, idx_next), dim=<span class="number">1</span>) </span><br><span class="line">    <span class="keyword">return</span> idx</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="Fine-tuning-for-classification">Fine-tuning for classification</h2>
<p>可以把最后的 token 预测层替换为分类 head，并且每个 sample 仅取最后一个预测结果作为分类结果（文本生成需要循环预测每个 token，但是分类其实不需要，只要有一个预测结果就可以，所以可以取最后一个）。使用交叉熵损失函数计算 loss。</p>
<hr>
<h2 id="Fine-tuning-to-follow-instructions">Fine-tuning to follow instructions</h2>
<p>其实可以看做文本生成预测 next token 的任务，可以 mask 掉 instruction，这样模型就会 focus 在 output 上，也可以选择保留 instruction，部分论文认为这样会更好。</p>
<hr>
<h2 id="个人评价">个人评价</h2>
<p>整体上是一本比较好的大模型入门书，不需要有太多基础知识，只要大概熟悉 python 就能看懂。用比较简单的例子，说明了大模型的各个模块的功能以及代码。但是整体确实比较浅，如果已经有过一定经验的话，再看意义就不大。</p>
<hr>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://cornfield404.github.io">Corn</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://cornfield404.github.io/posts/881244c3/">https://cornfield404.github.io/posts/881244c3/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://cornfield404.github.io" target="_blank">Corn Field</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/LLM/">LLM</a><a class="post-meta__tags" href="/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">读书笔记</a></div><div class="post-share"><div class="social-share" data-image="/img/avatar.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/posts/44658eea/" title="串联所有单词的子串"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">串联所有单词的子串</div></div><div class="info-2"><div class="info-item-1">题目 给定一个字符串 s 和一个字符串数组 words。words 中所有字符串长度相同。 s 中的串联子串是指一个包含 words 中所有字符串以任意顺序排列连接起来的子串。 返回所有串联子串在 s 中的开始索引。你可以以任意顺序返回答案。  思路 利用所有字符串等长的特点，逐位置搜索。  代码 1234567891011121314151617181920212223class Solution:    def findSubstring(self, s: str, words: List[str]) -&gt; List[int]:        from collections import Counter        if not s or not words:            return []                one_word = len(words[0])        all_len = len(words) * one_word        n = len(s)                words =...</div></div></div></a><a class="pagination-related" href="/posts/8215b0f1/" title="那种可能性早已料及"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">那种可能性早已料及</div></div><div class="info-2"><div class="info-item-1">故事梗概 自称渡良濑莉世的委托人怀疑自己十年前杀过人，希望委托侦探上苙丞调查确认。十年前，莉世和堂仁被各自的母亲带到教团&quot;血赎&quot;的居住地，二人都希望从村里逃走。村里的出入口只有一个，除交易日外不常打开，悬崖的岩石很脆不能攀爬，村里材料不足无法制造梯子或高台等工具，但堂仁依旧在为逃走准备，并承诺带莉世一起。村里共三十三人，一名教祖，其余是信徒。 村里的猪身上都带有号码牌，按照顺序决定下次吃那一头。某天堂仁发现莉世把 12 号的号码牌藏了起来，莉世回答自己喜欢 12...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/posts/57e95ee9/" title="InstructSeg"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-26</div><div class="info-item-2">InstructSeg</div></div><div class="info-2"><div class="info-item-1">题目： InstructSeg: Unifying Instructed Visual Segmentation with Multi-modal Large Language Models 链接： https://arxiv.org/pdf/2412.14006 代码： https://github.com/congvvc/InstructSeg 来源： arxiv 单位： Tsinghua Shenzhen International Graduate School, Tsinghua University；Meituan Inc.  本文目的 为图像和视频的 referring segmentation 和 reasoning segmentation 设计一个统一的 end-to-end 框架。  Method  Object-aware Video Perceiver  该模块根据输入的待分割视频帧及文本信息，提取 query...</div></div></div></a><a class="pagination-related" href="/posts/28a6509c/" title="ViLLa"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-25</div><div class="info-item-2">ViLLa</div></div><div class="info-2"><div class="info-item-1">题目： ViLLa: Video Reasoning Segmentation with Large Language Model 链接： https://arxiv.org/pdf/2407.14500 代码： https://github.com/rkzheng99/ViLLa 来源： arxiv 单位： The University of Hong Kong；University of California, Merced；Shanghai Artificial Intelligence Laboratory；SenseTime Research  Video Reasoning Segmentation 根据文本，输出视频对应的分割 mask。其中，文本可能是很复杂的描述，例如：a type of African mammal known for their distinctive black and white striped coat patterns walking at the end of its...</div></div></div></a><a class="pagination-related" href="/posts/44b7472c/" title="Decoder-Only优势"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-23</div><div class="info-item-2">Decoder-Only优势</div></div><div class="info-2"><div class="info-item-1">问题 为什么目前的模型大多数采用 decoder only 架构而非 encoder decoder 模型？  参考 为什么现在的 LLM 都是 Decoder only 的架构？ - Sam 聊算法的回答 - 知乎 https://www.zhihu.com/question/588325646/answer/3357252612  实验证据  Zero shot 任务里，decoder only 模型泛化性更好 wang22u.pdf Few shot 任务里，也是一样 2212.10559   理论说明  为什么现在的LLM都是Decoder-only的架构？ - 科学空间|Scientific Spaces 里认为可能是由于 encoder decoder 模型可能导致 attention map 是低秩的，但 decoder only 模型里采用 causal attention 的 attention map 一定是满秩的，所以有更强的表达能力； 为什么现在的 LLM 都是 Decoder only 的架构？ - yili 的回答 -...</div></div></div></a><a class="pagination-related" href="/posts/49eb3a2b/" title="Decoder-Only or Encoder-Decoder"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-23</div><div class="info-item-2">Decoder-Only or Encoder-Decoder</div></div><div class="info-2"><div class="info-item-1">题目： Decoder-Only or Encoder-Decoder? Interpreting Language Model as a Regularized Encoder-Decoder 链接： https://arxiv.org/pdf/2304.04052 来源： arxiv 单位： Language Technology Lab, University of Cambridge；The Chinese University of Hong Kong；JD. Com；Department of Computer Science and Technology, Tsinghua University  本文目的 以前的论文里有的认为 decoder only 结构更好，有的认为 encoder decoder 结构更好，本文希望进一步分析比较这两种模型。  Method Regularized Encoder-Decoder 目的 对传统 encoder decoder 模型进行了变体，使模型在行为上更接近 decoder only...</div></div></div></a><a class="pagination-related" href="/posts/5fae19d/" title="qwen2-vl"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-18</div><div class="info-item-2">qwen2-vl</div></div><div class="info-2"><div class="info-item-1">题目： Qwen 2-VL: Enhancing Vision-Language Model’s Perception of the World at Any Resolution 链接： https://arxiv.org/pdf/2409.12191 代码： https://github.com/QwenLM/Qwen2-VL 来源： arxiv 单位： Qwen Team Alibaba Group  目前方法的缺陷及 qwen2-vl 的解决方法  当前大多数 LVLM 会固定输入图像的尺寸，这会导致高分辨率图像信息的丢失。Qwen2-vl 在训练过程中引入动态分辨率； 当前大多数模型在处理视频时，会把视频帧看作互不关联的图像。Qwen2-vl 设计了 multimodal rotary position embedding 处理时空维度的信息。   Method 模型尺寸  模型结构   Naive Dynamic Resolution：采用 Navit 以及 2D-RoPE； M-RoPE： Unified Image and Video...</div></div></div></a><a class="pagination-related" href="/posts/9dfc0fd2/" title="MiniDrive"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">MiniDrive</div></div><div class="info-2"><div class="info-item-1">题目： MiniDrive: More Efficient Vision-Language Models with Multi-Level 2 D Features as Text Tokens for Autonomous Driving 链接： https://arxiv.org/pdf/2409.07267 代码： https://github.com/EMZucas/minidrive 来源： arxiv 单位： School of Artificial Intelligence, University of Chinese Academy of Sciences；State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of...</div></div></div></a></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="waline-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Corn</div><div class="author-info-description">来者可追</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">37</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">44</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Understanding-LLM"><span class="toc-number">1.</span> <span class="toc-text">Understanding LLM</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#LLM-%E4%B8%8E%E4%BC%A0%E7%BB%9F%E6%96%B9%E6%B3%95%E7%9B%B8%E6%AF%94%E7%9A%84%E4%BC%98%E5%8A%BF"><span class="toc-number">1.1.</span> <span class="toc-text">LLM 与传统方法相比的优势</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Stages-of-building-and-using-LLMS"><span class="toc-number">1.2.</span> <span class="toc-text">Stages of building and using LLMS</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Working-with-text-data"><span class="toc-number">2.</span> <span class="toc-text">Working with text data</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Understanding-word-embeddings"><span class="toc-number">2.1.</span> <span class="toc-text">Understanding word embeddings</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Tokenizing-text"><span class="toc-number">2.2.</span> <span class="toc-text">Tokenizing text</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Converting-tokens-into-token-IDs"><span class="toc-number">2.3.</span> <span class="toc-text">Converting tokens into token IDs</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Adding-special-context-tokens"><span class="toc-number">2.4.</span> <span class="toc-text">Adding special context tokens</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Byte-pair-encoding"><span class="toc-number">2.5.</span> <span class="toc-text">Byte pair encoding</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Data-sampling-with-a-sliding-window"><span class="toc-number">2.6.</span> <span class="toc-text">Data sampling with a sliding window</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Creating-token-embeddings"><span class="toc-number">2.7.</span> <span class="toc-text">Creating token embeddings</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Embedding-word-positions"><span class="toc-number">2.8.</span> <span class="toc-text">Embedding word positions</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Coding-attention-mechanisms"><span class="toc-number">3.</span> <span class="toc-text">Coding attention mechanisms</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#The-problem-with-modeling-long-sequences"><span class="toc-number">3.1.</span> <span class="toc-text">The problem with modeling long sequences</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Implementing-self-attention-with-trainable-weights"><span class="toc-number">3.2.</span> <span class="toc-text">Implementing self-attention with trainable weights</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hiding-future-words-with-causal-attention"><span class="toc-number">3.3.</span> <span class="toc-text">Hiding future words with causal attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Extending-single-head-attention-to-multi-head-attention"><span class="toc-number">3.4.</span> <span class="toc-text">Extending single-head attention to multi-head attention</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Implementing-a-GPT-model-from-scratch-to-generate-text"><span class="toc-number">4.</span> <span class="toc-text">Implementing a GPT model from scratch to generate text</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Pretraining-on-unlabeled-data"><span class="toc-number">5.</span> <span class="toc-text">Pretraining on unlabeled data</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Evaluating-generative-text-models"><span class="toc-number">5.1.</span> <span class="toc-text">Evaluating generative text models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Training-an-LLM"><span class="toc-number">5.2.</span> <span class="toc-text">Training an LLM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Decoding-strategies-to-control-randomness"><span class="toc-number">5.3.</span> <span class="toc-text">Decoding strategies to control randomness</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Fine-tuning-for-classification"><span class="toc-number">6.</span> <span class="toc-text">Fine-tuning for classification</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Fine-tuning-to-follow-instructions"><span class="toc-number">7.</span> <span class="toc-text">Fine-tuning to follow instructions</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%AA%E4%BA%BA%E8%AF%84%E4%BB%B7"><span class="toc-number">8.</span> <span class="toc-text">个人评价</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/96382a8f/" title="深度学习相关代码">深度学习相关代码</a><time datetime="2025-01-08T05:39:02.000Z" title="发表于 2025-01-08 13:39:02">2025-01-08</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/e3ed51ba/" title="交错字符串">交错字符串</a><time datetime="2025-01-07T02:56:06.000Z" title="发表于 2025-01-07 10:56:06">2025-01-07</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/5ea7715c/" title="2025愿望清单">2025愿望清单</a><time datetime="2025-01-03T11:09:47.000Z" title="发表于 2025-01-03 19:09:47">2025-01-03</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/826780ab/" title="SDXL">SDXL</a><time datetime="2024-12-27T05:23:39.000Z" title="发表于 2024-12-27 13:23:39">2024-12-27</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/f0775513/" title="DDPM">DDPM</a><time datetime="2024-12-26T10:35:25.000Z" title="发表于 2024-12-26 18:35:25">2024-12-26</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By Corn</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    document.querySelectorAll('#article-container .katex').forEach(el => el.classList.add('katex-show'))
  }

  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css')
    if (true) {
      await btf.getScript('https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js')
    }
  }

  showKatex()
})()</script><script>(() => {
  let initFn = window.walineFn || null
  const isShuoshuo = GLOBAL_CONFIG_SITE.isShuoshuo
  const option = null

  const destroyWaline = ele => ele.destroy()

  const initWaline = (Fn, el = document, path = window.location.pathname) => {
    const waline = Fn({
      el: el.querySelector('#waline-wrap'),
      serverURL: 'hexowaline-ten.vercel.app',
      pageview: false,
      dark: 'html[data-theme="dark"]',
      comment: false,
      ...option,
      path: isShuoshuo ? path : (option && option.path) || path
    })

    if (isShuoshuo) {
      window.shuoshuoComment.destroyWaline = () => {
        destroyWaline(waline)
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }
  }

  const loadWaline = (el, path) => {
    if (initFn) initWaline(initFn, el, path)
    else {
      btf.getCSS('https://cdn.jsdelivr.net/npm/@waline/client/dist/waline.min.css')
        .then(() => import('https://cdn.jsdelivr.net/npm/@waline/client/dist/waline.min.js'))
        .then(({ init }) => {
          initFn = init || Waline.init
          initWaline(initFn, el, path)
          window.walineFn = initFn
        })
    }
  }

  if (isShuoshuo) {
    'Waline' === 'Waline'
      ? window.shuoshuoComment = { loadComment: loadWaline } 
      : window.loadOtherComment = loadWaline
    return
  }

  if ('Waline' === 'Waline' || !false) {
    if (false) btf.loadComment(document.getElementById('waline-wrap'),loadWaline)
    else setTimeout(loadWaline, 0)
  } else {
    window.loadOtherComment = loadWaline
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/koharu.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true}});</script></body></html>