<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!--pjax：防止跳转页面音乐暂停-->
  <script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.js"></script>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha256-5eIC48iZUHmSlSUz9XtjRyK2mzQkHScZY1WdMaoz74E=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"cornfield404.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.21.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Understanding LLMLLM 与传统方法相比的优势之前的方法在比较复杂的理解和生成任务中表现不佳，例如：上下文分析、生成连贯文本，同时只适用于一些特定的任务。而 LLM 可以在一系列任务上表现出较好的性能。LLM 成功的原因包括 transformer 结构以及大量的训练数据。 Stages of building and using LLMS">
<meta property="og:type" content="article">
<meta property="og:title" content="Build a LLM From Scratch">
<meta property="og:url" content="https://cornfield404.github.io/posts/881244c3/index.html">
<meta property="og:site_name" content="Corn Field">
<meta property="og:description" content="Understanding LLMLLM 与传统方法相比的优势之前的方法在比较复杂的理解和生成任务中表现不佳，例如：上下文分析、生成连贯文本，同时只适用于一些特定的任务。而 LLM 可以在一系列任务上表现出较好的性能。LLM 成功的原因包括 transformer 结构以及大量的训练数据。 Stages of building and using LLMS">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-11-26%2014.09.22.png">
<meta property="og:image" content="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-11-26%2015.49.54.png">
<meta property="og:image" content="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-11-27%2010.51.33.png">
<meta property="og:image" content="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-11-27%2015.26.48.png">
<meta property="article:published_time" content="2024-11-26T05:55:59.000Z">
<meta property="article:modified_time" content="2024-12-04T09:24:34.367Z">
<meta property="article:author" content="Corn">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="读书笔记">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-11-26%2014.09.22.png">


<link rel="canonical" href="https://cornfield404.github.io/posts/881244c3/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://cornfield404.github.io/posts/881244c3/","path":"posts/881244c3/","title":"Build a LLM From Scratch"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Build a LLM From Scratch | Corn Field</title>
  







<link rel="dns-prefetch" href="hexowaline-ten.vercel.app">
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Corn Field</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-技术积累"><a href="/categories/%E6%8A%80%E6%9C%AF%E7%A7%AF%E7%B4%AF/" rel="section"><i class="fa fa-book fa-fw"></i>技术积累</a></li><li class="menu-item menu-item-休闲娱乐"><a href="/categories/%E4%BC%91%E9%97%B2%E5%A8%B1%E4%B9%90/" rel="section"><i class="fa fa-film fa-fw"></i>休闲娱乐</a></li><li class="menu-item menu-item-生活琐事"><a href="/categories/%E7%94%9F%E6%B4%BB%E7%90%90%E4%BA%8B/" rel="section"><i class="fa fa-camera fa-fw"></i>生活琐事</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-random"><a href="/random.html" rel="section"><i class="fas fa-random fa-fw"></i>随便逛逛</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>站点地图</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <!--网易云音乐插件-->
    <!-- require APlayer -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css">
    <script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script>
    <!-- require MetingJS-->
    <script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script> 
    <!--网易云playlist外链地址-->   
    <meting-js
        server="netease"
        type="playlist" 
        id="12848243996"
        mini="false"
        fixed="false"
        list-folded="true"
        autoplay="false"
        volume="0.7"
        theme="#FADFA3"
        order="list"
        loop="all"
        preload="auto"
        mutex="true">
    </meting-js>

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#Understanding-LLM"><span class="nav-number">1.</span> <span class="nav-text">Understanding LLM</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#LLM-%E4%B8%8E%E4%BC%A0%E7%BB%9F%E6%96%B9%E6%B3%95%E7%9B%B8%E6%AF%94%E7%9A%84%E4%BC%98%E5%8A%BF"><span class="nav-number">1.1.</span> <span class="nav-text">LLM 与传统方法相比的优势</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Stages-of-building-and-using-LLMS"><span class="nav-number">1.2.</span> <span class="nav-text">Stages of building and using LLMS</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Working-with-text-data"><span class="nav-number">2.</span> <span class="nav-text">Working with text data</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Understanding-word-embeddings"><span class="nav-number">2.1.</span> <span class="nav-text">Understanding word embeddings</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Tokenizing-text"><span class="nav-number">2.2.</span> <span class="nav-text">Tokenizing text</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Converting-tokens-into-token-IDs"><span class="nav-number">2.3.</span> <span class="nav-text">Converting tokens into token IDs</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Adding-special-context-tokens"><span class="nav-number">2.4.</span> <span class="nav-text">Adding special context tokens</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Byte-pair-encoding"><span class="nav-number">2.5.</span> <span class="nav-text">Byte pair encoding</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Data-sampling-with-a-sliding-window"><span class="nav-number">2.6.</span> <span class="nav-text">Data sampling with a sliding window</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Creating-token-embeddings"><span class="nav-number">2.7.</span> <span class="nav-text">Creating token embeddings</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Embedding-word-positions"><span class="nav-number">2.8.</span> <span class="nav-text">Embedding word positions</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Coding-attention-mechanisms"><span class="nav-number">3.</span> <span class="nav-text">Coding attention mechanisms</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#The-problem-with-modeling-long-sequences"><span class="nav-number">3.1.</span> <span class="nav-text">The problem with modeling long sequences</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Implementing-self-attention-with-trainable-weights"><span class="nav-number">3.2.</span> <span class="nav-text">Implementing self-attention with trainable weights</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Hiding-future-words-with-causal-attention"><span class="nav-number">3.3.</span> <span class="nav-text">Hiding future words with causal attention</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Extending-single-head-attention-to-multi-head-attention"><span class="nav-number">3.4.</span> <span class="nav-text">Extending single-head attention to multi-head attention</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Implementing-a-GPT-model-from-scratch-to-generate-text"><span class="nav-number">4.</span> <span class="nav-text">Implementing a GPT model from scratch to generate text</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Pretraining-on-unlabeled-data"><span class="nav-number">5.</span> <span class="nav-text">Pretraining on unlabeled data</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Evaluating-generative-text-models"><span class="nav-number">5.1.</span> <span class="nav-text">Evaluating generative text models</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Training-an-LLM"><span class="nav-number">5.2.</span> <span class="nav-text">Training an LLM</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Decoding-strategies-to-control-randomness"><span class="nav-number">5.3.</span> <span class="nav-text">Decoding strategies to control randomness</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Fine-tuning-for-classification"><span class="nav-number">6.</span> <span class="nav-text">Fine-tuning for classification</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Corn</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">15</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">22</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

  <div class="links-of-recent-posts motion-element">
    <div class="links-of-recent-posts-title">
      <i class="fa fa-history fa-fw"></i>
      最近文章
    </div>
    <ul class="links-of-recent-posts-list">
        <li class="links-of-recent-posts-item">
          <a href="/posts/6b36b444/" title="posts&#x2F;6b36b444&#x2F;">水母不会冻结</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/posts/66e39f70/" title="posts&#x2F;66e39f70&#x2F;">接雨水</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/posts/1cb194a8/" title="posts&#x2F;1cb194a8&#x2F;">缺失的第一个正数</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/posts/6ce289f7/" title="posts&#x2F;6ce289f7&#x2F;">花样滑冰</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/posts/de1f6638/" title="posts&#x2F;de1f6638&#x2F;">解数独</a>
        </li>
    </ul>
  </div>
        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://cornfield404.github.io/posts/881244c3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Corn">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Corn Field">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Build a LLM From Scratch | Corn Field">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Build a LLM From Scratch
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">

  
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-11-26 13:55:59" itemprop="dateCreated datePublished" datetime="2024-11-26T13:55:59+08:00">2024-11-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-12-04 17:24:34" itemprop="dateModified" datetime="2024-12-04T17:24:34+08:00">2024-12-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%8A%80%E6%9C%AF%E7%A7%AF%E7%B4%AF/" itemprop="url" rel="index"><span itemprop="name">技术积累</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%8A%80%E6%9C%AF%E7%A7%AF%E7%B4%AF/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">读书笔记</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
  
  <span class="post-meta-item">
    
    <span class="post-meta-item-icon">
      <i class="far fa-comment"></i>
    </span>
    <span class="post-meta-item-text">Waline：</span>
  
    <a title="waline" href="/posts/881244c3/#waline" itemprop="discussionUrl">
      <span class="post-comments-count waline-comment-count" data-path="/posts/881244c3/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>12k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>11 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h4 id="Understanding-LLM"><a href="#Understanding-LLM" class="headerlink" title="Understanding LLM"></a>Understanding LLM</h4><h5 id="LLM-与传统方法相比的优势"><a href="#LLM-与传统方法相比的优势" class="headerlink" title="LLM 与传统方法相比的优势"></a>LLM 与传统方法相比的优势</h5><p>之前的方法在比较复杂的理解和生成任务中表现不佳，例如：上下文分析、生成连贯文本，同时只适用于一些特定的任务。而 LLM 可以在一系列任务上表现出较好的性能。LLM 成功的原因包括 transformer 结构以及大量的训练数据。</p>
<h5 id="Stages-of-building-and-using-LLMS"><a href="#Stages-of-building-and-using-LLMS" class="headerlink" title="Stages of building and using LLMS"></a>Stages of building and using LLMS</h5><p>目前的研究表明，模型在特定任务或特定领域上训练后可以表现出超过通用 LLM 的性能。因此 LLM 训练通常包括两步：</p>
<ul>
<li>Pretrain 指模型在大规模数据库上训练，使模型具有语言理解能力，这一阶段的数据是不需要标注的；</li>
<li>Finetune 指模型在细分领域&#x2F;细分任务的数据上训练，目前比较常用的 finetune 方式包括两类：<ul>
<li>Instruction finetuning：标注数据由 instruction 和 answer 组成，例如：要求模型翻译某段话的 query 以及正确的翻译后的文本；</li>
<li>Classification finetuning：标注数据由文本以及对应的类别组成，例如：邮件内容以及该邮件是否是垃圾邮件的标注。<br><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-11-26%2014.09.22.png" alt="截屏2024-11-26 14.09.22.png"></li>
</ul>
</li>
</ul>
<hr>
<h4 id="Working-with-text-data"><a href="#Working-with-text-data" class="headerlink" title="Working with text data"></a>Working with text data</h4><h5 id="Understanding-word-embeddings"><a href="#Understanding-word-embeddings" class="headerlink" title="Understanding word embeddings"></a>Understanding word embeddings</h5><p>Text 需要被转换成 vector 才能被模型处理，这个过程被称为 embedding。Embedding 有很多方法：</p>
<ul>
<li>Word2vec：核心思路是在相近的上下文中出现的词应该具有相似的含义；</li>
<li>作为模型的一部分在训练期间不断更新：这种方法的优势是训练出的 embedding 与自己的任务和数据更适配。</li>
</ul>
<h5 id="Tokenizing-text"><a href="#Tokenizing-text" class="headerlink" title="Tokenizing text"></a>Tokenizing text</h5><p>每个单词，每个标点符号都作为一个单独的 token。空格是否保留可以根据需要，如果是对格式非常严格的任务，例如代码生成，那么缩进和空格就有保留的必要，对于对格式要求不严格的任务，也可以删掉以节省存储空间。<br><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-11-26%2015.49.54.png" alt="截屏2024-11-26 15.49.54.png"></p>
<h5 id="Converting-tokens-into-token-IDs"><a href="#Converting-tokens-into-token-IDs" class="headerlink" title="Converting tokens into token IDs"></a>Converting tokens into token IDs</h5><p>给文本里所有出现过的 token 一个编码，这样每次读到一段 text，就可以通过转换把 text 转换成编码。LLM 的输出结果可以通过反向转换，转换到 text。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleTokenizerV1</span>: </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab</span>): </span><br><span class="line">        <span class="variable language_">self</span>.str_to_int = vocab </span><br><span class="line">        <span class="variable language_">self</span>.int_to_str = &#123;i:s <span class="keyword">for</span> s,i <span class="keyword">in</span> vocab.items()&#125; </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, text</span>): </span><br><span class="line">        preprocessed = re.split(<span class="string">r&#x27;([,.?_!&quot;()\&#x27;]|--|\s)&#x27;</span>, text)</span><br><span class="line">        preprocessed = [ </span><br><span class="line">            item.strip() <span class="keyword">for</span> item <span class="keyword">in</span> preprocessed <span class="keyword">if</span> item.strip() </span><br><span class="line">        ] </span><br><span class="line">        ids = [<span class="variable language_">self</span>.str_to_int[s] <span class="keyword">for</span> s <span class="keyword">in</span> preprocessed] </span><br><span class="line">        <span class="keyword">return</span> ids </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode</span>(<span class="params">self, ids</span>): </span><br><span class="line">        text = <span class="string">&quot; &quot;</span>.join([<span class="variable language_">self</span>.int_to_str[i] <span class="keyword">for</span> i <span class="keyword">in</span> ids])</span><br><span class="line">        <span class="comment"># 处理标点符号前的空格 </span></span><br><span class="line">        text = re.sub(<span class="string">r&#x27;\s+([,.?!&quot;()\&#x27;])&#x27;</span>, <span class="string">r&#x27;\1&#x27;</span>, text) </span><br><span class="line">        <span class="keyword">return</span> text</span><br></pre></td></tr></table></figure>

<h5 id="Adding-special-context-tokens"><a href="#Adding-special-context-tokens" class="headerlink" title="Adding special context tokens"></a>Adding special context tokens</h5><p>为了应对 text 里可能包含的没见过的词，需要增加 $&lt;|unk|&gt;$ token。同时也可以增加一些用于帮助文本理解的 token，例如用 $&lt;|endoftext|&gt;$ 表明文档的末尾，这样在用大量文本训练时，可以使模型更好地判断哪些文本是有关联的。</p>
<h5 id="Byte-pair-encoding"><a href="#Byte-pair-encoding" class="headerlink" title="Byte pair encoding"></a>Byte pair encoding</h5><p>在 tiktoken 库（ <a target="_blank" rel="noopener" href="https://github.com/openai/tiktoken">https://github.com/openai/tiktoken</a> ）里有实现，可用 pip 安装。BPE 遇到没见过的单词会把这些词拆解成 subword 后再进行 encode，这样就不需要 $&lt;|unk|&gt;$ token 进行处理。BPE 的基本原理是首先把每个字母加入 vocabulary，然后把同时出现的概率较高的字母组合为 subword，逐步生成 word。<br><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-11-27%2010.51.33.png" alt="截屏2024-11-27 10.51.33.png"></p>
<h5 id="Data-sampling-with-a-sliding-window"><a href="#Data-sampling-with-a-sliding-window" class="headerlink" title="Data sampling with a sliding window"></a>Data sampling with a sliding window</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader </span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GPTDatasetV1</span>(<span class="title class_ inherited__">Dataset</span>): </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, txt, tokenizer, max_length, stride</span>):</span><br><span class="line">        <span class="variable language_">self</span>.input_ids = [] </span><br><span class="line">        <span class="variable language_">self</span>.target_ids = [] </span><br><span class="line">        </span><br><span class="line">        token_ids = tokenizer.encode(txt)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(token_ids) - max_length, stride):</span><br><span class="line">            input_chunk = token_ids[i:i + max_length] </span><br><span class="line">            target_chunk = token_ids[i + <span class="number">1</span>: i + max_length + <span class="number">1</span>]</span><br><span class="line">            <span class="variable language_">self</span>.input_ids.append(torch.tensor(input_chunk))</span><br><span class="line">            <span class="variable language_">self</span>.target_ids.append(torch.tensor(target_chunk))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>): </span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.input_ids) </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>): </span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.input_ids[idx], <span class="variable language_">self</span>.target_ids[idx]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_dataloader_v1</span>(<span class="params">txt, batch_size=<span class="number">4</span>, max_length=<span class="number">256</span>, </span></span><br><span class="line"><span class="params">                         stride=<span class="number">128</span>, shuffle=<span class="literal">True</span>, drop_last=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">                         num_workers=<span class="number">0</span></span>): </span><br><span class="line">    tokenizer = tiktoken.get_encoding(<span class="string">&quot;gpt2&quot;</span>) </span><br><span class="line">    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)</span><br><span class="line">    dataloader = DataLoader( </span><br><span class="line">        dataset, </span><br><span class="line">        batch_size=batch_size, </span><br><span class="line">        shuffle=shuffle, </span><br><span class="line">        drop_last=drop_last, </span><br><span class="line">        num_workers=num_workers </span><br><span class="line">    ) </span><br><span class="line">    <span class="keyword">return</span> dataloader</span><br></pre></td></tr></table></figure>

<h5 id="Creating-token-embeddings"><a href="#Creating-token-embeddings" class="headerlink" title="Creating token embeddings"></a>Creating token embeddings</h5><p>用 $torch.nn.Embedding$ 将 index 映射为可训练的 embedding。</p>
<h5 id="Embedding-word-positions"><a href="#Embedding-word-positions" class="headerlink" title="Embedding word positions"></a>Embedding word positions</h5><p>Position embedding 主要分为两类：</p>
<ul>
<li>Absolute position embedding：根据 token 在 sequence 里的绝对位置 embedding；</li>
<li>Relative position embedding：学习 tokens 之间的相对距离，可以使模型在不同长度的 sequence 上泛化地更好。</li>
</ul>
<hr>
<h4 id="Coding-attention-mechanisms"><a href="#Coding-attention-mechanisms" class="headerlink" title="Coding attention mechanisms"></a>Coding attention mechanisms</h4><h5 id="The-problem-with-modeling-long-sequences"><a href="#The-problem-with-modeling-long-sequences" class="headerlink" title="The problem with modeling long sequences"></a>The problem with modeling long sequences</h5><p>在采用 attention 结构前，比较常用的语言处理模型是 RNN 结构。在 RNN 结构中，encoder 把输入文本的全部内容保存在一个 hidden state 内部，然后 decoder 用这个 hidden state 生成输出。<br><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-11-27%2015.26.48.png" alt="截屏2024-11-27 15.26.48.png"><br>这种结构的问题是在 decoder 过程中，encoder 早期的 hidden state 被全部丢弃了，仅采用了最后一个 hidden state，这会导致信息丢失。</p>
<h5 id="Implementing-self-attention-with-trainable-weights"><a href="#Implementing-self-attention-with-trainable-weights" class="headerlink" title="Implementing self-attention with trainable weights"></a>Implementing self-attention with trainable weights</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SelfAttention_v2</span>(nn.Module): </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_in, d_out, qkv_bias=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__() </span><br><span class="line">        <span class="variable language_">self</span>.W_query = nn.Linear(d_in, d_out, bias=qkv_bias) </span><br><span class="line">        <span class="variable language_">self</span>.W_key = nn.Linear(d_in, d_out, bias=qkv_bias) </span><br><span class="line">        <span class="variable language_">self</span>.W_value = nn.Linear(d_in, d_out, bias=qkv_bias) </span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>): </span><br><span class="line">        keys = <span class="variable language_">self</span>.W_key(x) </span><br><span class="line">        queries = <span class="variable language_">self</span>.W_query(x) </span><br><span class="line">        values = <span class="variable language_">self</span>.W_value(x) </span><br><span class="line">        attn_scores = queries @ keys.T </span><br><span class="line">        attn_weights = torch.softmax( </span><br><span class="line">            attn_scores / keys.shape[-<span class="number">1</span>]**<span class="number">0.5</span>, dim=-<span class="number">1</span> </span><br><span class="line">        ) </span><br><span class="line">        context_vec = attn_weights @ values </span><br><span class="line">        <span class="keyword">return</span> context_vec</span><br></pre></td></tr></table></figure>

<p>Normalize 是为了防止点积结果过大，从而落在梯度较低的区域，影响训练速度。</p>
<h5 id="Hiding-future-words-with-causal-attention"><a href="#Hiding-future-words-with-causal-attention" class="headerlink" title="Hiding future words with causal attention"></a>Hiding future words with causal attention</h5><p>自然语言任务里，经常需要仅与前面的输入做 attention。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CausalAttention</span>(nn.Module): </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_in, d_out, context_length, </span></span><br><span class="line"><span class="params">                dropout, qkv_bias=<span class="literal">False</span></span>): </span><br><span class="line">        <span class="built_in">super</span>().__init__() </span><br><span class="line">        <span class="variable language_">self</span>.d_out = d_out </span><br><span class="line">        <span class="variable language_">self</span>.W_query = nn.Linear(d_in, d_out, bias=qkv_bias) </span><br><span class="line">        <span class="variable language_">self</span>.W_key = nn.Linear(d_in, d_out, bias=qkv_bias) </span><br><span class="line">        <span class="variable language_">self</span>.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout) </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># register_buffer可以使变量随着模型在不同device上移动，不需要手动处理</span></span><br><span class="line">        <span class="variable language_">self</span>.register_buffer( </span><br><span class="line">            <span class="string">&#x27;mask&#x27;</span>, </span><br><span class="line">            torch.triu(torch.ones(context_length, context_length),</span><br><span class="line">            diagonal=<span class="number">1</span>) </span><br><span class="line">        ) </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>): </span><br><span class="line">        b, num_tokens, d_in = x.shape </span><br><span class="line">        keys = <span class="variable language_">self</span>.W_key(x) </span><br><span class="line">        queries = <span class="variable language_">self</span>.W_query(x) </span><br><span class="line">        values = <span class="variable language_">self</span>.W_value(x) </span><br><span class="line">        </span><br><span class="line">        attn_scores = queries @ keys.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 加入mask防止与后面的token计算attention</span></span><br><span class="line">        attn_scores.masked_fill_( </span><br><span class="line">            <span class="variable language_">self</span>.mask.<span class="built_in">bool</span>()[:num_tokens, :num_tokens], -torch.inf</span><br><span class="line">        ) </span><br><span class="line">        attn_weights = torch.softmax( </span><br><span class="line">            attn_scores / keys.shape[-<span class="number">1</span>]**<span class="number">0.5</span>, dim=-<span class="number">1</span> </span><br><span class="line">        ) </span><br><span class="line">        <span class="comment"># 加入dropout防止过拟合</span></span><br><span class="line">        attn_weights = <span class="variable language_">self</span>.dropout(attn_weights)</span><br><span class="line">        </span><br><span class="line">        context_vec = attn_weights @ values </span><br><span class="line">        <span class="keyword">return</span> context_vec</span><br></pre></td></tr></table></figure>

<h5 id="Extending-single-head-attention-to-multi-head-attention"><a href="#Extending-single-head-attention-to-multi-head-attention" class="headerlink" title="Extending single-head attention to multi-head attention"></a>Extending single-head attention to multi-head attention</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module): </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_in, d_out, </span></span><br><span class="line"><span class="params">                 context_length, dropout, num_heads, qkv_bias=<span class="literal">False</span></span>): </span><br><span class="line">        <span class="built_in">super</span>().__init__() </span><br><span class="line">        <span class="keyword">assert</span> (d_out % num_heads == <span class="number">0</span>), \ </span><br><span class="line">            <span class="string">&quot;d_out must be divisible by num_heads&quot;</span> </span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.d_out = d_out </span><br><span class="line">        <span class="variable language_">self</span>.num_heads = num_heads </span><br><span class="line">        <span class="variable language_">self</span>.head_dim = d_out // num_heads </span><br><span class="line">        <span class="variable language_">self</span>.W_query = nn.Linear(d_in, d_out, bias=qkv_bias) </span><br><span class="line">        <span class="variable language_">self</span>.W_key = nn.Linear(d_in, d_out, bias=qkv_bias) </span><br><span class="line">        <span class="variable language_">self</span>.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)</span><br><span class="line">        <span class="variable language_">self</span>.out_proj = nn.Linear(d_out, d_out) </span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout) </span><br><span class="line">        <span class="variable language_">self</span>.register_buffer( </span><br><span class="line">            <span class="string">&quot;mask&quot;</span>, </span><br><span class="line">            torch.triu(torch.ones(context_length, context_length),</span><br><span class="line">                       diagonal=<span class="number">1</span>) </span><br><span class="line">        ) </span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>): </span><br><span class="line">        b, num_tokens, d_in = x.shape </span><br><span class="line">        keys = <span class="variable language_">self</span>.W_key(x) </span><br><span class="line">        queries = <span class="variable language_">self</span>.W_query(x) </span><br><span class="line">        values = <span class="variable language_">self</span>.W_value(x)</span><br><span class="line">        keys = keys.view(b, num_tokens, <span class="variable language_">self</span>.num_heads, <span class="variable language_">self</span>.head_dim)</span><br><span class="line">        values = values.view(</span><br><span class="line">            b, num_tokens, <span class="variable language_">self</span>.num_heads, <span class="variable language_">self</span>.head_dim</span><br><span class="line">        ) </span><br><span class="line">        queries = queries.view( </span><br><span class="line">            b, num_tokens, <span class="variable language_">self</span>.num_heads, <span class="variable language_">self</span>.head_dim </span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        keys = keys.transpose(<span class="number">1</span>, <span class="number">2</span>) </span><br><span class="line">        queries = queries.transpose(<span class="number">1</span>, <span class="number">2</span>) </span><br><span class="line">        values = values.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        attn_scores = queries @ keys.transpose(<span class="number">2</span>, <span class="number">3</span>) </span><br><span class="line">        mask_bool = <span class="variable language_">self</span>.mask.<span class="built_in">bool</span>()[:num_tokens, :num_tokens]</span><br><span class="line">        </span><br><span class="line">        attn_scores.masked_fill_(mask_bool, -torch.inf)</span><br><span class="line">        </span><br><span class="line">        attn_weights = torch.softmax( </span><br><span class="line">            attn_scores / keys.shape[-<span class="number">1</span>]**<span class="number">0.5</span>, dim=-<span class="number">1</span>) </span><br><span class="line">        attn_weights = <span class="variable language_">self</span>.dropout(attn_weights)</span><br><span class="line">        </span><br><span class="line">        context_vec = (attn_weights @ values).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        context_vec = context_vec.contiguous().view( </span><br><span class="line">            b, num_tokens, <span class="variable language_">self</span>.d_out </span><br><span class="line">        ) </span><br><span class="line">        context_vec = <span class="variable language_">self</span>.out_proj(context_vec) </span><br><span class="line">        <span class="keyword">return</span> context_vec</span><br></pre></td></tr></table></figure>

<hr>
<h4 id="Implementing-a-GPT-model-from-scratch-to-generate-text"><a href="#Implementing-a-GPT-model-from-scratch-to-generate-text" class="headerlink" title="Implementing a GPT model from scratch to generate text"></a>Implementing a GPT model from scratch to generate text</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerBlock</span>(nn.Module): </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, cfg</span>): </span><br><span class="line">        <span class="built_in">super</span>().__init__() </span><br><span class="line">        <span class="variable language_">self</span>.att = MultiHeadAttention( </span><br><span class="line">            d_in=cfg[<span class="string">&quot;emb_dim&quot;</span>], </span><br><span class="line">            d_out=cfg[<span class="string">&quot;emb_dim&quot;</span>], </span><br><span class="line">            context_length=cfg[<span class="string">&quot;context_length&quot;</span>],</span><br><span class="line">            num_heads=cfg[<span class="string">&quot;n_heads&quot;</span>], </span><br><span class="line">            dropout=cfg[<span class="string">&quot;drop_rate&quot;</span>], </span><br><span class="line">            qkv_bias=cfg[<span class="string">&quot;qkv_bias&quot;</span>]) </span><br><span class="line">        <span class="variable language_">self</span>.ff = FeedForward(cfg) </span><br><span class="line">        <span class="variable language_">self</span>.norm1 = LayerNorm(cfg[<span class="string">&quot;emb_dim&quot;</span>]) </span><br><span class="line">        <span class="variable language_">self</span>.norm2 = LayerNorm(cfg[<span class="string">&quot;emb_dim&quot;</span>]) </span><br><span class="line">        <span class="variable language_">self</span>.drop_shortcut = nn.Dropout(cfg[<span class="string">&quot;drop_rate&quot;</span>]) </span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>): </span><br><span class="line">        shortcut = x </span><br><span class="line">        x = <span class="variable language_">self</span>.norm1(x) </span><br><span class="line">        x = <span class="variable language_">self</span>.att(x) </span><br><span class="line">        x = <span class="variable language_">self</span>.drop_shortcut(x) </span><br><span class="line">        x = x + shortcut </span><br><span class="line">        </span><br><span class="line">        shortcut = x </span><br><span class="line">        x = <span class="variable language_">self</span>.norm2(x) </span><br><span class="line">        x = <span class="variable language_">self</span>.ff(x) </span><br><span class="line">        x = <span class="variable language_">self</span>.drop_shortcut(x) </span><br><span class="line">        x = x + shortcut </span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GPTModel</span>(nn.Module): </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, cfg</span>): </span><br><span class="line">        <span class="built_in">super</span>().__init__() </span><br><span class="line">        <span class="variable language_">self</span>.tok_emb = nn.Embedding(cfg[<span class="string">&quot;vocab_size&quot;</span>], cfg[<span class="string">&quot;emb_dim&quot;</span>])</span><br><span class="line">        <span class="variable language_">self</span>.pos_emb = nn.Embedding(</span><br><span class="line">            cfg[<span class="string">&quot;context_length&quot;</span>], cfg[<span class="string">&quot;emb_dim&quot;</span>]</span><br><span class="line">        ) </span><br><span class="line">        <span class="variable language_">self</span>.drop_emb = nn.Dropout(cfg[<span class="string">&quot;drop_rate&quot;</span>]) </span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.trf_blocks = nn.Sequential( </span><br><span class="line">            *[TransformerBlock(cfg) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(cfg[<span class="string">&quot;n_layers&quot;</span>])])</span><br><span class="line">            </span><br><span class="line">        <span class="variable language_">self</span>.final_norm = LayerNorm(cfg[<span class="string">&quot;emb_dim&quot;</span>]) </span><br><span class="line">        <span class="variable language_">self</span>.out_head = nn.Linear( </span><br><span class="line">            cfg[<span class="string">&quot;emb_dim&quot;</span>], cfg[<span class="string">&quot;vocab_size&quot;</span>], bias=<span class="literal">False</span> </span><br><span class="line">        ) </span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, in_idx</span>): </span><br><span class="line">        batch_size, seq_len = in_idx.shape </span><br><span class="line">        tok_embeds = <span class="variable language_">self</span>.tok_emb(in_idx) </span><br><span class="line">        </span><br><span class="line">        pos_embeds = <span class="variable language_">self</span>.pos_emb( </span><br><span class="line">            torch.arange(seq_len, device=in_idx.device) </span><br><span class="line">        ) </span><br><span class="line">        x = tok_embeds + pos_embeds </span><br><span class="line">        x = <span class="variable language_">self</span>.drop_emb(x) </span><br><span class="line">        x = <span class="variable language_">self</span>.trf_blocks(x) </span><br><span class="line">        x = <span class="variable language_">self</span>.final_norm(x) </span><br><span class="line">        logits = <span class="variable language_">self</span>.out_head(x) </span><br><span class="line">        <span class="keyword">return</span> logits</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_text_simple</span>(<span class="params">model, idx, max_new_tokens, context_size</span>): </span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(max_new_tokens): </span><br><span class="line">        idx_cond = idx[:, -context_size:] </span><br><span class="line">        <span class="keyword">with</span> torch.no_grad(): </span><br><span class="line">            logits = model(idx_cond) </span><br><span class="line">        </span><br><span class="line">        logits = logits[:, -<span class="number">1</span>, :] </span><br><span class="line">        probas = torch.softmax(logits, dim=-<span class="number">1</span>) </span><br><span class="line">        idx_next = torch.argmax(probas, dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>) </span><br><span class="line">        idx = torch.cat((idx, idx_next), dim=<span class="number">1</span>) </span><br><span class="line">    <span class="keyword">return</span> idx</span><br></pre></td></tr></table></figure>

<hr>
<h4 id="Pretraining-on-unlabeled-data"><a href="#Pretraining-on-unlabeled-data" class="headerlink" title="Pretraining on unlabeled data"></a>Pretraining on unlabeled data</h4><h5 id="Evaluating-generative-text-models"><a href="#Evaluating-generative-text-models" class="headerlink" title="Evaluating generative text models"></a>Evaluating generative text models</h5><ul>
<li>采用 cross entropy loss；</li>
<li>Perplexity 也被用来评估模型，该指标用 $perplexity&#x3D;torch.exp(loss)$ 计算，该指标的含义是模型认为有多少个可能且合理的备选，该指标越大，表示模型认为有更多可能的备选，模型不确定性越高。</li>
</ul>
<h5 id="Training-an-LLM"><a href="#Training-an-LLM" class="headerlink" title="Training an LLM"></a>Training an LLM</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_model_simple</span>(<span class="params">model, train_loader, val_loader, </span></span><br><span class="line"><span class="params">                       optimizer, device, num_epochs, </span></span><br><span class="line"><span class="params">                       eval_freq, eval_iter, start_context, tokenizer</span>):</span><br><span class="line">    train_losses, val_losses, track_tokens_seen = [], [], []</span><br><span class="line">    tokens_seen, global_step = <span class="number">0</span>, -<span class="number">1</span> </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs): </span><br><span class="line">        model.train() </span><br><span class="line">        <span class="keyword">for</span> input_batch, target_batch <span class="keyword">in</span> train_loader:</span><br><span class="line">            optimizer.zero_grad() </span><br><span class="line">            loss = calc_loss_batch( </span><br><span class="line">                input_batch, target_batch, model, device </span><br><span class="line">            ) </span><br><span class="line">            loss.backward() </span><br><span class="line">            optimizer.step() </span><br><span class="line">            tokens_seen += input_batch.numel() </span><br><span class="line">            global_step += <span class="number">1</span> </span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> global_step % eval_freq == <span class="number">0</span>: </span><br><span class="line">                train_loss, val_loss = evaluate_model( </span><br><span class="line">                    model, train_loader, val_loader, device, eval_iter)</span><br><span class="line">                train_losses.append(train_loss)</span><br><span class="line">                val_losses.append(val_loss)</span><br><span class="line">                track_tokens_seen.append(tokens_seen) </span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;Ep <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span> (Step <span class="subst">&#123;global_step:06d&#125;</span>): &quot;</span> </span><br><span class="line">                      <span class="string">f&quot;Train loss <span class="subst">&#123;train_loss:<span class="number">.3</span>f&#125;</span>, &quot;</span> </span><br><span class="line">                      <span class="string">f&quot;Val loss <span class="subst">&#123;val_loss:<span class="number">.3</span>f&#125;</span>&quot;</span> </span><br><span class="line">                )</span><br><span class="line">        generate_and_print_sample( </span><br><span class="line">            model, tokenizer, device, start_context </span><br><span class="line">        ) </span><br><span class="line">    <span class="keyword">return</span> train_losses, val_losses, track_tokens_seen</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calc_loss_batch</span>(<span class="params">input_batch, target_batch, model, device</span>):</span><br><span class="line">    input_batch = input_batch.to(device) </span><br><span class="line">    target_batch = target_batch.to(device) </span><br><span class="line">    logits = model(input_batch) </span><br><span class="line">    loss = torch.nn.functional.cross_entropy( </span><br><span class="line">        logits.flatten(<span class="number">0</span>, <span class="number">1</span>), target_batch.flatten() </span><br><span class="line">    ) </span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calc_loss_loader</span>(<span class="params">data_loader, model, device, num_batches=<span class="literal">None</span></span>):</span><br><span class="line">    total_loss = <span class="number">0.</span> </span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(data_loader) == <span class="number">0</span>: </span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">float</span>(<span class="string">&quot;nan&quot;</span>) </span><br><span class="line">    <span class="keyword">elif</span> num_batches <span class="keyword">is</span> <span class="literal">None</span>: </span><br><span class="line">        num_batches = <span class="built_in">len</span>(data_loader) </span><br><span class="line">    <span class="keyword">else</span>: </span><br><span class="line">        num_batches = <span class="built_in">min</span>(num_batches, <span class="built_in">len</span>(data_loader)) </span><br><span class="line">    <span class="keyword">for</span> i, (input_batch, target_batch) <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_loader): </span><br><span class="line">        <span class="keyword">if</span> i &lt; num_batches: </span><br><span class="line">            loss = calc_loss_batch( </span><br><span class="line">                input_batch, target_batch, model, device </span><br><span class="line">            ) </span><br><span class="line">            total_loss += loss.item() </span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            <span class="keyword">break</span> </span><br><span class="line">    <span class="keyword">return</span> total_loss / num_batches</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_model</span>(<span class="params">model, train_loader, val_loader, device, eval_iter</span>):</span><br><span class="line">    model.<span class="built_in">eval</span>() </span><br><span class="line">    <span class="keyword">with</span> torch.no_grad(): </span><br><span class="line">        train_loss = calc_loss_loader( </span><br><span class="line">            train_loader, model, device, num_batches=eval_iter</span><br><span class="line">        ) </span><br><span class="line">        val_loss = calc_loss_loader( </span><br><span class="line">            val_loader, model, device, num_batches=eval_iter </span><br><span class="line">        ) </span><br><span class="line">    model.train() </span><br><span class="line">    <span class="keyword">return</span> train_loss, val_loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_and_print_sample</span>(<span class="params">model, tokenizer, device, start_context</span>):</span><br><span class="line">    model.<span class="built_in">eval</span>() </span><br><span class="line">    context_size = model.pos_emb.weight.shape[<span class="number">0</span>] </span><br><span class="line">    encoded = text_to_token_ids(start_context, tokenizer).to(device)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad(): </span><br><span class="line">        token_ids = generate_text_simple( </span><br><span class="line">            model=model, idx=encoded, </span><br><span class="line">            max_new_tokens=<span class="number">50</span>, context_size=context_size </span><br><span class="line">        ) </span><br><span class="line">    decoded_text = token_ids_to_text(token_ids, tokenizer)</span><br><span class="line">    <span class="built_in">print</span>(decoded_text.replace(<span class="string">&quot;\n&quot;</span>, <span class="string">&quot; &quot;</span>)) </span><br><span class="line">    model.train()</span><br></pre></td></tr></table></figure>

<h5 id="Decoding-strategies-to-control-randomness"><a href="#Decoding-strategies-to-control-randomness" class="headerlink" title="Decoding strategies to control randomness"></a>Decoding strategies to control randomness</h5><p>之前的生成模型每次的生成结果都是一致的，为了增加多样性的同时保证合理性，可以采取两个措施：</p>
<ul>
<li>把固定选择概率最大的词作为输出，改为根据概率采样，即：$torch.argmax$ 改为 $torch.multinomial$；</li>
<li>仅在 $top\ k$ 样本中进行采样，防止采到特别离谱的结果，用 $torch.where$ 把非 $top\ k$ 的概率置为 $-\inf$，这样 $softmax$ 后这些位置的采样概率就会变成 0。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">generate</span>(<span class="params">model, idx, max_new_tokens, context_size, </span></span><br><span class="line"><span class="params">             temperature=<span class="number">0.0</span>, top_k=<span class="literal">None</span>, eos_id=<span class="literal">None</span></span>): </span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(max_new_tokens): </span><br><span class="line">        idx_cond = idx[:, -context_size:] </span><br><span class="line">        <span class="keyword">with</span> torch.no_grad(): </span><br><span class="line">            logits = model(idx_cond) </span><br><span class="line">        logits = logits[:, -<span class="number">1</span>, :]</span><br><span class="line">        <span class="keyword">if</span> top_k <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: </span><br><span class="line">            top_logits, _ = torch.topk(logits, top_k) </span><br><span class="line">            min_val = top_logits[:, -<span class="number">1</span>] </span><br><span class="line">            logits = torch.where( </span><br><span class="line">                logits &lt; min_val, </span><br><span class="line">                torch.tensor(<span class="built_in">float</span>(<span class="string">&#x27;-inf&#x27;</span>)).to(logits.device), </span><br><span class="line">                logits </span><br><span class="line">            ) </span><br><span class="line">        <span class="keyword">if</span> temperature &gt; <span class="number">0.0</span>: </span><br><span class="line">            logits = logits / temperature </span><br><span class="line">            probs = torch.softmax(logits, dim=-<span class="number">1</span>) </span><br><span class="line">            idx_next = torch.multinomial(probs, num_samples=<span class="number">1</span>) </span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            idx_next = torch.argmax(logits, dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>) </span><br><span class="line">        <span class="keyword">if</span> idx_next == eos_id: </span><br><span class="line">            <span class="keyword">break</span> </span><br><span class="line">        idx = torch.cat((idx, idx_next), dim=<span class="number">1</span>) </span><br><span class="line">    <span class="keyword">return</span> idx</span><br></pre></td></tr></table></figure>

<hr>
<h4 id="Fine-tuning-for-classification"><a href="#Fine-tuning-for-classification" class="headerlink" title="Fine-tuning for classification"></a>Fine-tuning for classification</h4><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>
    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/LLM/" rel="tag"><i class="fa fa-tag"></i> LLM</a>
              <a href="/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/" rel="tag"><i class="fa fa-tag"></i> 读书笔记</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/posts/8215b0f1/" rel="prev" title="那种可能性早已料及">
                  <i class="fa fa-angle-left"></i> 那种可能性早已料及
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/posts/44658eea/" rel="next" title="串联所有单词的子串">
                  串联所有单词的子串 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments" id="waline"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Corn</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">31k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">28 分钟</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script><script src="/js/pjax.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  <script src="/js/third-party/pace.js"></script>


  
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="waline" type="application/json">{"lang":"zh-cn","enable":true,"serverURL":"hexowaline-ten.vercel.app","cssUrl":"https://unpkg.com/@waline/client@v2/dist/waline.css","commentCount":true,"pageview":false,"placeholder":"(发表评论)","avatar":"mm","meta":["nick","mail","link"],"pageSize":10,"visitor":true,"comment_count":true,"requiredFields":[],"libUrl":"//unpkg.com/@waline/client@v2/dist/waline.js","el":"#waline","comment":true,"path":"/posts/881244c3/"}</script>
<link rel="stylesheet" href="https://unpkg.com/@waline/client@v2/dist/waline.css">
<script>
document.addEventListener('page:loaded', () => {
  NexT.utils.loadComments(CONFIG.waline.el).then(() =>
    NexT.utils.getScript(CONFIG.waline.libUrl, { condition: window.Waline })
  ).then(() => 
    Waline.init(Object.assign({}, CONFIG.waline,{ el: document.querySelector(CONFIG.waline.el) }))
  );
});
</script>

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/koharu.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true}});</script></body>
</html>
