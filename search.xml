<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>SDXL</title>
    <url>/posts/826780ab/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>题目：</strong> SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis<br><strong>链接：</strong> <a href="https://arxiv.org/pdf/2307.01952">https://arxiv.org/pdf/2307.01952</a><br><strong>代码：</strong> <a href="https://github.com/Stability-AI/generative-models">https://github.com/Stability-AI/generative-models</a><br><strong>来源：</strong> ICLR 2024<br><strong>单位：</strong> Stability AI, Applied Research</p>
<hr>
<h4 id="本文目的"><a href="#本文目的" class="headerlink" title="本文目的"></a>本文目的</h4><p>在 stable diffusion 基础上，提升生成效果。</p>
<hr>
<h4 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h4><h5 id="模型相比之前的主要调整"><a href="#模型相比之前的主要调整" class="headerlink" title="模型相比之前的主要调整"></a>模型相比之前的主要调整</h5><p><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-12-30%2016.53.54.png" alt="截屏2024-12-30 16.53.54.png"></p>
<ul>
<li>模型结构上，删掉 unet lowest level，highest level 里不加 transformer block，中间两层 level 分别加入 2 个和 10 个 transformer block；</li>
<li>Text encoder 采用两个 encoder 输出 concat 的结果；</li>
<li>加入了 pooled text embedding。</li>
</ul>
<h5 id="Conditioning-the-Model-on-Image-Size"><a href="#Conditioning-the-Model-on-Image-Size" class="headerlink" title="Conditioning the Model on Image Size"></a>Conditioning the Model on Image Size</h5><p>因为 <a href="https://cornfield404.github.io/posts/16f61640/">latent diffusion model</a> 模型分为两个阶段（auto encoder 以及 diffusion），所以如果训练图像尺寸比较小可能会导致经过 auto encoder 下采样的图像不足以用来训练 diffusion。之前解决该问题主要有两个方案：丢掉尺寸比较小的图像（可能导致很多图不可用影响训练效果）以及对图像上采样（可能导致图像里有 artifacts，影响图片生成效果）。</p>
<p>本文采用了另一种方式：把图像原始尺寸作为 condition embedding，和 timestep embedding 一起输入网络。</p>
<h5 id="Conditioning-the-Model-on-Cropping-Parameters"><a href="#Conditioning-the-Model-on-Cropping-Parameters" class="headerlink" title="Conditioning the Model on Cropping Parameters"></a>Conditioning the Model on Cropping Parameters</h5><p>之前的生成模型可能有物体生成不全的情况，这很可能是因为模型训练时为了保证每个 batch 的尺寸一致，需要对输入图像进行 crop，crop 之后的物体是不全的，而这种特征被生成模型学到了。</p>
<p>本文采用的解决方案：训练时把 crop 的值进行 embedding 并输入网络，测试时把 crop 设为 $(0,0)$。</p>
<h5 id="Multi-Aspect-Training"><a href="#Multi-Aspect-Training" class="headerlink" title="Multi-Aspect Training"></a>Multi-Aspect Training</h5><p>之前的生成模型主要都是在方图上训练，但是这种方式对于屏幕来说不是很自然，所以本文预设了不同的纵横比作为 target size，把图片划分到不同的 target size 里，每个 batch 从同一 target size 里采样图片，且 target size 作为一个 embedding 输入模型。</p>
<h5 id="Improved-Autoencoder"><a href="#Improved-Autoencoder" class="headerlink" title="Improved Autoencoder"></a>Improved Autoencoder</h5><p>优化 VAE 的训练：采用更大的 batch size，并用 EMA 更新模型权重。</p>
<h5 id="Putting-Everything-Together"><a href="#Putting-Everything-Together" class="headerlink" title="Putting Everything Together"></a>Putting Everything Together</h5><p>训练过程分三步：$256\times 256$，$512\times 512$，multi-aspect training。在这三步之后模型在一些细节上的效果可能还是不太好，因此本文采用了一个 refinement stage。Refinement stage 在 base model 输出的基础上进行优化。<br><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-12-30%2019.19.11.png" alt="截屏2024-12-30 19.19.11.png"></p>
<hr>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <categories>
        <category>技术积累</category>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>Diffusion</tag>
        <tag>Image_Generation</tag>
        <tag>高引</tag>
        <tag>ICLR2024</tag>
      </tags>
  </entry>
  <entry>
    <title>DDPM</title>
    <url>/posts/f0775513/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>题目：</strong> Denoising Diffusion Probabilistic Models<br><strong>链接：</strong> <a href="https://arxiv.org/format/2006.11239">https://arxiv.org/format/2006.11239</a><br><strong>代码：</strong> <a href="https://github.com/hojonathanho/diffusion">https://github.com/hojonathanho/diffusion</a><br><strong>来源：</strong> NIPS 2020<br><strong>单位：</strong> UC Berkeley</p>
<hr>
<h4 id="模型总览"><a href="#模型总览" class="headerlink" title="模型总览"></a>模型总览</h4><p><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%25E6%2588%25AA%25E5%25B1%258F2023-08-11%252017.10.55.png" alt="截屏2023-08-11 17.10.55.png"></p>
<p>DDPM 模型主要分为两个过程：forward 加噪过程（从右往左）和 reverse 去噪过程（从左往右）。加噪过程意思是指向数据集的真实图片中逐步加入高斯噪声，而去噪过程是指对加了噪声的图片逐步去噪，从而还原出真实图片。加噪过程满足一定的数学规律，而去噪过程则采用神经网络来学习。这么一来，神经网络就可以从一堆杂乱无章的噪声图片中生成真实图片了。</p>
<hr>
<h4 id="Diffusion-扩散过程（Forward-加噪过程）"><a href="#Diffusion-扩散过程（Forward-加噪过程）" class="headerlink" title="Diffusion 扩散过程（Forward 加噪过程）"></a>Diffusion 扩散过程（Forward 加噪过程）</h4><h5 id="数学基础"><a href="#数学基础" class="headerlink" title="数学基础"></a>数学基础</h5><h6 id="马尔可夫性质"><a href="#马尔可夫性质" class="headerlink" title="马尔可夫性质"></a>马尔可夫性质</h6><p>马尔可夫性质是概率论中的一个概念，因为俄国数学家安德雷·马尔可夫得名。当一个随机过程在给定现在状态及所有过去状态情况下，其未来状态的条件概率分布仅依赖于当前状态；换句话说，在给定现在状态时，它与过去状态（即该过程的历史路径）是条件独立的，那么此随机过程即具有马尔可夫性质。具有马尔可夫性质的过程通常称之为马尔可夫过程。</p>
<h6 id="高斯分布的独立可加性"><a href="#高斯分布的独立可加性" class="headerlink" title="高斯分布的独立可加性"></a>高斯分布的独立可加性</h6><p>若：</p>

$$
\begin{align}
X &\sim N(\mu_X,\sigma_X^2) \\
Y &\sim N(\mu_Y,\sigma_Y^2) \\
Z &= X+Y
\end{align}
$$

<p>则：<br>$$<br>\begin{align}<br>Z&amp;\sim N(\mu_X+\mu_Y,\sigma_X^2+\sigma_Y^2)<br>\end{align}<br>$$</p>
<h5 id="逐步加噪过程"><a href="#逐步加噪过程" class="headerlink" title="逐步加噪过程"></a>逐步加噪过程</h5><p>向数据分布中逐渐加入高斯噪声，加噪过程持续​ $T$ 次，产生一系列带噪声的图片 $x_1,…,x_T$。在由 $x_{t-1}$ 加噪至 $x_t$ 的过程中，噪声的标准差/方差是以一个在区间 $(0,1)$ 内的固定值 $\beta_T$ 来确定的，均值是以固定值 $\beta_T$ 和当前时刻的图片数据 $x_{t-1}$ 来确定的。<br>以上描述的加噪过程可以写成公式：</p>

$$
\begin{align}
q(x_t|x_{t-1})&=\mathcal{N}(x_t;\sqrt{1-\beta_t}x_{t-1},\beta_t\mathbf{I}) \\
q(x_{1:T}|x_0)&=\prod_{t=1}^Tq(x_t|x_{t-1}) 
\end{align}
$$

<p>这个噪声只由 $\beta_T$ 和 $x_{t-1}$ 决定，是一个固定值，而非可学习过程。因此，只要有了 $x_0$，并且提前确定每一步的固定值 $\beta_1,…,\beta_T$，就可以推出任意一步的加噪数据 $x_1,…,x_T$。整个加噪过程是一个马尔可夫过程。</p>
<h5 id="加噪结果"><a href="#加噪结果" class="headerlink" title="加噪结果"></a>加噪结果</h5><p>随着​ $t$ 的不断增大，最终原始数据​ $x_0$ 会逐步失去它的特征。最终当 $T\rightarrow\infty$ ​时，$x_T$ 趋近于一个各向独立的高斯分布。从视觉上来看，就是将原本一张完好的照片加噪很多步后，图片几乎变成了一张完全是噪声的图片。</p>
<h5 id="任意时刻数据-x-t-的计算"><a href="#任意时刻数据-x-t-的计算" class="headerlink" title="任意时刻数据 $x_t$ 的计算"></a>任意时刻数据 $x_t$ 的计算</h5><p>在逐步加噪的过程中，其实并不需要一步一步地从 $x_0,x_1,…$ 去迭代得到 $x_t$，可以直接从 $x_0$ 和固定序列值 $\{\beta_T\in(0,1)\}_{t=1}^T$ 计算得到。</p>

$$
\begin{align}
x_t&=\sqrt{1-\beta_t}x_{t-1}+\sqrt{\beta_t}z_1,&where\ z_1,z_2,\ldots,\sim\mathcal{N}(0,\mathbf{I}) \\
&=\sqrt{\alpha_t}x_{t-1}+\sqrt{1-\alpha_t}z_1\\
&=\sqrt{\alpha_t}(\sqrt{\alpha_{t-1}}x_{t-2}+\sqrt{1-\alpha_{t-1}}z_2)+\sqrt{1-\alpha_t}z_1\\
&=\sqrt{\alpha_t\alpha_{t-1}}x_{t-2}+\sqrt{\alpha_t}\sqrt{1-\alpha_{t-1}}z_2+\sqrt{1-\alpha_t}z_1\\
&=\sqrt{\alpha_t\alpha_{t-1}}x_{t-2}+\sqrt{1-a_ta_{t-1}}\overline{z_2},&\overline{z_2}\sim \mathcal{N}(0,\mathbf{I})\\
&=\ldots\\
&=\sqrt{\alpha_t\alpha_{t-1}\ldots\alpha_1}x_0+\sqrt{1-a_ta_{t-1}\ldots\alpha_1}\overline{z_t}\\
&=\sqrt{\overline{\alpha_t}}x_0+\sqrt{1-\overline{\alpha_t}}\cdot\overline{z_t}
\end{align}
$$

<p>由上式可得：</p>

$$
q(x_t|x_0)=\mathcal{N}(x_t;\sqrt{\overline{\alpha_t}}x_0,(1-\overline{\alpha_t})\mathbf{I})
$$

<p>由此公式可以看出，只要有了 $x_0$ 和固定值序列 $\{\beta_T\in(0,1)\}_{t=1} ^T$ ，再从标准分布 $N(0,1)$ 采样出一个 $z$，就可以直接计算出 $x_t$ 了。</p>
<p>在最终的等式中，$x_t=\sqrt{\overline{a_t}}x_0+\sqrt{1-\overline{a_t}}\cdot \overline{z_t}$，其中 $\overline{a_t}=\prod_{i=1}^T\alpha_i$ 在 $T$ 次连乘之后接近于 0。因此 $x_t$ 接近于 $0\times x_0+\sqrt{1-0}\overline{z_t}=\overline{z_t}$，即：$\mathcal{N}(0,\mathbf{I})$ 的正态分布。</p>
<h5 id="beta-t-的取值设置"><a href="#beta-t-的取值设置" class="headerlink" title="$\beta_t$ 的取值设置"></a>$\beta_t$ 的取值设置</h5><p>一般地，随着​ $t$ 的增大，数据越接近随机的高斯分布，$\beta_t$ 的取值就越大。所以 $\beta_1&lt;\beta_2&lt;\ldots&lt;\beta_T$。同时，由于 $\{\beta_T\in(0,1)\}_{t=1}^T$，即 $\beta_1,\beta_2,\ldots,\beta_T$ 的取值区间在 $(0,1)$ 内，因此 $\alpha_1,\alpha_2,\ldots,\alpha_T$ 的取值也在 $(0,1)$ 内。所以，$\overline{\alpha_1}&gt;\overline{\alpha_2}&gt;\ldots&gt;\overline{\alpha_T}$。</p>
<hr>
<h4 id="Reverse-Diffusion-逆扩散过程（Reverse-去噪过程）"><a href="#Reverse-Diffusion-逆扩散过程（Reverse-去噪过程）" class="headerlink" title="Reverse Diffusion 逆扩散过程（Reverse 去噪过程）"></a>Reverse Diffusion 逆扩散过程（Reverse 去噪过程）</h4><h5 id="数学基础-1"><a href="#数学基础-1" class="headerlink" title="数学基础"></a>数学基础</h5><h6 id="贝叶斯公式"><a href="#贝叶斯公式" class="headerlink" title="贝叶斯公式"></a>贝叶斯公式</h6><p>用来描述两个条件概率之间的关系。按照乘法法则：<br>$$<br>P(A\cap B)=P(A)\times P(B|A)=P(B)\times P(A|B)<br>$$<br>变形可得：<br>$$<br>P(A|B)=P(A)\times P(B|A)/P(B)<br>$$</p>
<h5 id="后验扩散条件概率-q-x-t-1-x-t-x-0"><a href="#后验扩散条件概率-q-x-t-1-x-t-x-0" class="headerlink" title="后验扩散条件概率 $q(x_{t-1}|x_t,x_0)$"></a>后验扩散条件概率 $q(x_{t-1}|x_t,x_0)$</h5><p>在逆扩散过程中，如果给定了 $x_t$ 和 $x_0$，就可以计算出 $x_{t-1}$，即后验条件概率 $q(x_{t-1}|x_t,x_0)$ 是可以计算的：</p>

$$
\begin{align}
q(x_{t-1}|x_t,x_0)&=q(x_t|x_{t-1},x_0)\frac{q(x_{t-1}|x_0)}{q(x_t|x_0)}
\end{align}
$$

<p>由正向加噪过程可知：</p>

$$
\begin{align}
q(x_t|x_{t-1},x_0)&=\sqrt{\alpha_t}x_{t-1}+\sqrt{1-\alpha_t}z_1&～\mathcal{N}(\sqrt{\alpha_t}x_{t-1},1-\alpha_t) \\
q(x_{t-1}|x_0)&=\sqrt{\overline{\alpha_{t-1}}}x_0+\sqrt{1-\overline{\alpha_{t-1}}}\cdot\overline{z_{t-1}}&～\mathcal{N}(\sqrt{\overline{\alpha_{t-1}}}x_0,1-\overline{\alpha_{t-1}}) \\
q(x_t|x_0)&=\sqrt{\overline{\alpha_t}}x_0+\sqrt{1-\overline{\alpha_t}}\cdot\overline{z_{t-1}}&～\mathcal{N}(\sqrt{\overline{\alpha_t}}x_0,1-\overline{\alpha_t})
\end{align}
$$

<p>正态分布可以表示为：$N(\mu, \sigma ^2)\propto\exp(-\frac{(x-\mu)^2}{2\sigma ^2})$<br>推导可知：</p>

$$
\begin{align}
q(x_{t-1}|x_t,x_0)&\propto \exp(-\frac{1}{2}(\frac{(x_t-\sqrt{\alpha_t}x_{t-1})^2}{\beta_t}+\frac{(x_{t-1}-\sqrt{\overline{\alpha_{t-1}}}x_0)^2}{1-\overline{\alpha_{t-1}}}-\frac{(x_t-\sqrt{\overline{\alpha_t}}x_0)^2}{1-\overline{\alpha_t}})) \\
&\propto \exp(-\frac{1}{2}((\frac{\alpha_t}{\beta_t}+\frac{1}{1-\overline{\alpha_{t-1}}})x_{t-1}^2-(\frac{2\sqrt{\alpha_t}x_t}{\beta_t}+\frac{2\sqrt{\overline{\alpha_{t-1}}}x_0}{1-\overline{\alpha_{t-1}}})x_{t-1}+C(x_t,x_0)))
\end{align}
$$

<p>对比正态分布公式，可以推出：</p>

$$
\begin{align}
\sigma^2&=\frac{1}{\frac{\alpha_t}{\beta_t}+\frac{1}{1-\overline{\alpha_{t-1}}}}=\frac{1-\overline{\alpha_{t-1}}}{1-\overline{\alpha_t}}\beta_t \\
\mu&=(\frac{\sqrt{\alpha_t}x_t}{\beta_t}+\frac{\sqrt{\overline{\alpha_{t-1}}}x_0}{1-\overline{\alpha_{t-1}}})\cdot \frac{1}{\frac{\alpha_t}{\beta_t}+\frac{1}{1-\overline{\alpha_{t-1}}}}=\frac{(1-\overline{\alpha_{t-1}})\sqrt{\alpha_t}}{1-\overline{\alpha_t}}x_t+\frac{\sqrt{\overline{\alpha_{t-1}}}\beta_t}{1-\overline{\alpha_t}}x_0
\end{align}
$$

<p>把 $x_0$ 用 $x_t$ 替换掉可得：</p>

$$
\begin{align}
\mu&=\frac{(1-\overline{\alpha_{t-1}})\sqrt{\alpha_t}}{1-\overline{\alpha_t}}x_t+\frac{\sqrt{\overline{\alpha_{t-1}}}\beta_t}{1-\overline{\alpha_t}}x_0 \\
&=\frac{(1-\overline{\alpha_{t-1}})\sqrt{\alpha_t}}{1-\overline{\alpha_t}}x_t+\frac{\sqrt{\overline{\alpha_{t-1}}}\beta_t}{1-\overline{\alpha_t}}\cdot \frac{x_t-\sqrt{1-\overline{\alpha_t}}\cdot \overline{z_t}}{\sqrt{\overline{\alpha_t}}}\\
&=\frac{1}{\sqrt{\alpha_t}}(x_t-\frac{\beta_t}{\sqrt{1-\overline{\alpha_t}}}\cdot \overline{z_t})
\end{align}
$$

<p>这里可以看出想要推导出前一张图只有 $\overline{z_t}$ 是未知的，所以只要用网络预测出这个 $\overline{z_t}$ 就可以了。</p>
<hr>
<h4 id="个人理解"><a href="#个人理解" class="headerlink" title="个人理解"></a>个人理解</h4><p>直观上 diffusion model 是个比较好理解的模型，向图像中一步步加噪使图像变成一个完全的随机噪声，反过来，只要预测出每一步加入的噪声，就可以得到原始的图像。只不过论文中用了复杂的公式给这一过程进行了严密的论证，论证了只需要预测出噪声就可以一步步推导出上一步的图像，也论证了仅用加入的噪声作为监督信息就可以预测出原始图像。</p>
<p>从代码里也可以看出 diffusion model 的实现非常简洁：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Diffusion</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, noise_steps=<span class="number">1000</span>, beta_start=<span class="number">1e-4</span>, beta_end=<span class="number">0.02</span>, img_size=<span class="number">256</span>, device=<span class="string">&quot;cuda&quot;</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.noise_steps = noise_steps</span><br><span class="line">        <span class="variable language_">self</span>.beta_start = beta_start</span><br><span class="line">        <span class="variable language_">self</span>.beta_end = beta_end</span><br><span class="line">        <span class="variable language_">self</span>.img_size = img_size</span><br><span class="line">        <span class="variable language_">self</span>.device = device</span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.beta = <span class="variable language_">self</span>.prepare_noise_schedule().to(device)</span><br><span class="line">        <span class="variable language_">self</span>.alpha = <span class="number">1.</span> - <span class="variable language_">self</span>.beta</span><br><span class="line">        <span class="variable language_">self</span>.alpha_hat = torch.cumprod(<span class="variable language_">self</span>.alpha, dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 用于计算每一步加入噪声的强弱，这里采用了一个线性增加的方式</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">prepare_noise_schedule</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.linspace(<span class="variable language_">self</span>.beta_start, <span class="variable language_">self</span>.beta_end, <span class="variable language_">self</span>.noise_steps)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算加噪后的图像，由原始图像，噪声以及加噪比例决定</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">noise_images</span>(<span class="params">self, x, t</span>):</span><br><span class="line">        sqrt_alpha_hat = torch.sqrt(<span class="variable language_">self</span>.alpha_hat[t])[:, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>]</span><br><span class="line">        sqrt_one_minus_alpha_hat = torch.sqrt(<span class="number">1</span> - <span class="variable language_">self</span>.alpha_hat[t])[:, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>]</span><br><span class="line">        Ɛ = torch.randn_like(x)</span><br><span class="line">        <span class="keyword">return</span> sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * Ɛ, Ɛ</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sample_timesteps</span>(<span class="params">self, n</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.randint(low=<span class="number">1</span>, high=<span class="variable language_">self</span>.noise_steps, size=(n,))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据推导的逆扩散过程的公式逐步推出上一张图</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sample</span>(<span class="params">self, model, n</span>):</span><br><span class="line">        logging.info(<span class="string">f&quot;Sampling <span class="subst">&#123;n&#125;</span> new images....&quot;</span>)</span><br><span class="line">        model.<span class="built_in">eval</span>()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            x = torch.randn((n, <span class="number">3</span>, <span class="variable language_">self</span>.img_size, <span class="variable language_">self</span>.img_size)).to(<span class="variable language_">self</span>.device)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> tqdm(<span class="built_in">reversed</span>(<span class="built_in">range</span>(<span class="number">1</span>, <span class="variable language_">self</span>.noise_steps)), position=<span class="number">0</span>):</span><br><span class="line">                t = (torch.ones(n) * i).long().to(<span class="variable language_">self</span>.device)</span><br><span class="line">                predicted_noise = model(x, t)</span><br><span class="line">                alpha = <span class="variable language_">self</span>.alpha[t][:, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>]</span><br><span class="line">                alpha_hat = <span class="variable language_">self</span>.alpha_hat[t][:, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>]</span><br><span class="line">                beta = <span class="variable language_">self</span>.beta[t][:, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>]</span><br><span class="line">                <span class="keyword">if</span> i &gt; <span class="number">1</span>:</span><br><span class="line">                    noise = torch.randn_like(x)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    noise = torch.zeros_like(x)</span><br><span class="line">                x = <span class="number">1</span> / torch.sqrt(alpha) * (x - ((<span class="number">1</span> - alpha) / (torch.sqrt(<span class="number">1</span> - alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise</span><br><span class="line">        model.train()</span><br><span class="line">        x = (x.clamp(-<span class="number">1</span>, <span class="number">1</span>) + <span class="number">1</span>) / <span class="number">2</span></span><br><span class="line">        x = (x * <span class="number">255</span>).<span class="built_in">type</span>(torch.uint8)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">args</span>):</span><br><span class="line">    setup_logging(args.run_name)</span><br><span class="line">    device = args.device</span><br><span class="line">    dataloader = get_data(args)</span><br><span class="line">    model = UNet().to(device)</span><br><span class="line">    optimizer = optim.AdamW(model.parameters(), lr=args.lr)</span><br><span class="line">    mse = nn.MSELoss()</span><br><span class="line">    diffusion = Diffusion(img_size=args.image_size, device=device)</span><br><span class="line">    logger = SummaryWriter(os.path.join(<span class="string">&quot;runs&quot;</span>, args.run_name))</span><br><span class="line">    l = <span class="built_in">len</span>(dataloader)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(args.epochs):</span><br><span class="line">        logging.info(<span class="string">f&quot;Starting epoch <span class="subst">&#123;epoch&#125;</span>:&quot;</span>)</span><br><span class="line">        pbar = tqdm(dataloader)</span><br><span class="line">        <span class="keyword">for</span> i, (images, _) <span class="keyword">in</span> <span class="built_in">enumerate</span>(pbar):</span><br><span class="line">            images = images.to(device)</span><br><span class="line">            t = diffusion.sample_timesteps(images.shape[<span class="number">0</span>]).to(device)</span><br><span class="line">            x_t, noise = diffusion.noise_images(images, t)</span><br><span class="line">            predicted_noise = model(x_t, t)</span><br><span class="line">            loss = mse(noise, predicted_noise)</span><br><span class="line"></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">            pbar.set_postfix(MSE=loss.item())</span><br><span class="line">            logger.add_scalar(<span class="string">&quot;MSE&quot;</span>, loss.item(), global_step=epoch * l + i)</span><br><span class="line"></span><br><span class="line">        sampled_images = diffusion.sample(model, n=images.shape[<span class="number">0</span>])</span><br><span class="line">        save_images(sampled_images, os.path.join(<span class="string">&quot;results&quot;</span>, args.run_name, <span class="string">f&quot;<span class="subst">&#123;epoch&#125;</span>.jpg&quot;</span>))</span><br><span class="line">        torch.save(model.state_dict(), os.path.join(<span class="string">&quot;models&quot;</span>, args.run_name, <span class="string">f&quot;ckpt.pt&quot;</span>))</span><br></pre></td></tr></table></figure>

<hr>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <categories>
        <category>技术积累</category>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>Diffusion</tag>
        <tag>Image_Generation</tag>
        <tag>高引</tag>
        <tag>NIPS2020</tag>
      </tags>
  </entry>
  <entry>
    <title>InstructSeg</title>
    <url>/posts/57e95ee9/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>题目：</strong> InstructSeg: Unifying Instructed Visual Segmentation with Multi-modal Large Language Models<br><strong>链接：</strong> <a href="https://arxiv.org/pdf/2412.14006">https://arxiv.org/pdf/2412.14006</a><br><strong>代码：</strong> <a href="https://github.com/congvvc/InstructSeg">https://github.com/congvvc/InstructSeg</a><br><strong>来源：</strong> arxiv<br><strong>单位：</strong> Tsinghua Shenzhen International Graduate School, Tsinghua University；Meituan Inc.</p>
<hr>
<h4 id="本文目的"><a href="#本文目的" class="headerlink" title="本文目的"></a>本文目的</h4><p>为图像和视频的 referring segmentation 和 reasoning segmentation 设计一个统一的 end-to-end 框架。</p>
<hr>
<h4 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h4><p><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-12-26%2010.52.46.png" alt="截屏2024-12-26 10.52.46.png"></p>
<h5 id="Object-aware-Video-Perceiver"><a href="#Object-aware-Video-Perceiver" class="headerlink" title="Object-aware Video Perceiver"></a>Object-aware Video Perceiver</h5><p><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-12-26%2011.04.35.png" alt="截屏2024-12-26 11.04.35.png"><br>该模块根据输入的待分割视频帧及文本信息，提取 query 特征： </p>

$$
\begin{align}
f_r^t&=F_{CLIP}(I_r^t) \\
Q_t&=CrossAttn(Q,Concat[F_p(f_r^t), \varepsilon])
\end{align}
$$

<p>其中，$\varepsilon$ 是文本 token，$I_r^t$ 是每一帧的图像输入，$F_p$ 指对其图像及文本空间的投影模块，$Q_t$ 指每一帧的输出。</p>
<h5 id="Vision-guided-Multi-granularity-Text-Fusion"><a href="#Vision-guided-Multi-granularity-Text-Fusion" class="headerlink" title="Vision-guided Multi-granularity Text Fusion"></a>Vision-guided Multi-granularity Text Fusion</h5><p><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-12-26%2013.28.24.png" alt="截屏2024-12-26 13.28.24.png"><br>LLM 输出的 detailed text embed 通过 global average pooling 得到 global text embed $E_g$。两部分 text embed 拼在一起作为 cross attention 的一个输入，图像中提取的图像特征与 global text embed 拼在一起作为 cross attention 的另一个输入，经过多轮 cross attention 与 FFN 后，生成 multi-grained text embed 作为输出。</p>
<hr>
<h4 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h4><p><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-12-26%2013.44.41.png" alt="截屏2024-12-26 13.44.41.png"><br><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-12-26%2013.46.04.png" alt="截屏2024-12-26 13.46.04.png"></p>
<hr>
<h4 id="个人看法"><a href="#个人看法" class="headerlink" title="个人看法"></a>个人看法</h4><p>提出了一个可以同时处理 video 和 image reasoning segmentation 的算法，其中融合多帧的方式是单独提取每一帧的 learnable query 并沿时间维度 concat 到一起。</p>
<hr>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <categories>
        <category>技术积累</category>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>论文笔记</tag>
        <tag>Video_Segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title>ViLLa</title>
    <url>/posts/28a6509c/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>题目：</strong> ViLLa: Video Reasoning Segmentation with Large<br>Language Model<br><strong>链接：</strong> <a href="https://arxiv.org/pdf/2407.14500">https://arxiv.org/pdf/2407.14500</a><br><strong>代码：</strong> <a href="https://github.com/rkzheng99/ViLLa">https://github.com/rkzheng99/ViLLa</a><br><strong>来源：</strong> arxiv<br><strong>单位：</strong> The University of Hong Kong；University of California, Merced；Shanghai Artificial Intelligence Laboratory；SenseTime Research</p>
<hr>
<h4 id="Video-Reasoning-Segmentation"><a href="#Video-Reasoning-Segmentation" class="headerlink" title="Video Reasoning Segmentation"></a>Video Reasoning Segmentation</h4><p>根据文本，输出视频对应的分割 mask。其中，文本可能是很复杂的描述，例如：a type of African mammal known for their distinctive black and white striped coat patterns walking at the end of its herd。</p>
<hr>
<h4 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h4><p><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-12-25%2013.35.50.png" alt="截屏2024-12-25 13.35.50.png"></p>
<h5 id="Encoders"><a href="#Encoders" class="headerlink" title="Encoders"></a>Encoders</h5><ul>
<li>视觉编码器提取多尺度视觉特征 $F_t=\{F_t^l\}_{l=1}^L$；</li>
<li>文本编码器以文本和 visual embedding $F_t^L$ 作为输入。</li>
</ul>
<h5 id="Context-Aggregation-Module"><a href="#Context-Aggregation-Module" class="headerlink" title="Context Aggregation Module"></a>Context Aggregation Module</h5>
$$
\begin{align}
E_t&=FFN(CrossAttn(x_{txt}, F_t^L, F_t^L)) \\
E_t^c&=Concat\{E_t^i\}_{i=1}^K
\end{align}
$$

<h5 id="Multi-level-Segmentation-Tokens"><a href="#Multi-level-Segmentation-Tokens" class="headerlink" title="Multi-level Segmentation Tokens"></a>Multi-level Segmentation Tokens</h5><ul>
<li>采用两个 segmentation token，一个是 frame level token，一个是 video level token；</li>
<li>两个 token 与图像特征和文本特征一起输入 LLM；</li>
<li>LLM 除了会生成文本输出，还会生成计算后的 segmentation token。</li>
</ul>
<h5 id="Video-Frame-Decoder"><a href="#Video-Frame-Decoder" class="headerlink" title="Video-Frame Decoder"></a>Video-Frame Decoder</h5>
$$
\begin{align}
Q^{l+1,s}&=FFN^s(h_{SA}^s(h_{CA}^s(Q^{l,s},F_t^l))) \\
Q^{l+1,v'}&=\gamma\cdot Softmax((Q^{l+1,v}\times Q^{l+1,f^T})\times Q^{l+1,f}+(1-\gamma)\cdot Q^{l+1,v}) 
\end{align}
$$

<p>其中 decoder block 参考了 Mask2Former。</p>
<hr>
<h4 id="个人看法"><a href="#个人看法" class="headerlink" title="个人看法"></a>个人看法</h4><p>论文里有部分没有理解的地方，在流程图里，传给 decoder 的是多帧图像提取出的特征，但是在文章内容里，并没有说这是怎么用的，不确定是我没理解还是论文就是没讲。</p>
<hr>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <categories>
        <category>技术积累</category>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>论文笔记</tag>
        <tag>Video_Segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title>RT-DETR</title>
    <url>/posts/befdae7d/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>题目：</strong> DETRs Beat YOLOs on Real-time Object Detection<br><strong>链接：</strong> <a href="https://arxiv.org/abs/2304.08069">https://arxiv.org/abs/2304.08069</a><br><strong>代码：</strong> <a href="https://github.com/lyuwenyu/RT-DETR">https://github.com/lyuwenyu/RT-DETR</a><br><strong>来源：</strong> CVPR 2023<br><strong>单位：</strong> Baidu Inc, Beijing, China；School of Electronic and Computer Engineering, Peking University, Shenzhen, China</p>
<hr>
<h4 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h4><p>YOLO 一直是实时检测领域应用最广的检测框架，但是 YOLO 得到的结果需要经过 NMS 处理才能得到最终的输出结果，如 <a href="https://zhuanlan.zhihu.com/p/626659049">《目标检测大杂烩》-第14章-浅析RT-DETR - 知乎</a>分析，NMS 在后处理流程中会带来很多问题。反之，DETR 系列工作不需要 NMS 作为后处理，但 DETR 速度不够快，因此在实时检测领域无法应用。这篇文章在 DETR 的基础上，提出了一种能实现实时检测的 Transformer 检测框架，解决了前面提到的问题。</p>
<hr>
<h4 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h4><h5 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h5><p><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-05-24%2014.13.04.png" alt="截屏2024-05-24 14.13.04.png"><br>模型整体由 backbone、encoder、decoder 和 head 构成。本文的主要改进点在于 encoder 和 decoder 部分。</p>
<h5 id="Efficient-Hybrid-Encoder"><a href="#Efficient-Hybrid-Encoder" class="headerlink" title="Efficient Hybrid Encoder"></a>Efficient Hybrid Encoder</h5><h6 id="Computational-bottleneck-analysis"><a href="#Computational-bottleneck-analysis" class="headerlink" title="Computational bottleneck analysis"></a>Computational bottleneck analysis</h6><p>论文里面首先通过实验证明了 Deformable DETR 中的 encoder 方式存在冗余部分。<br><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-05-24%2014.42.03.png" alt="截屏2024-05-24 14.42.03.png"><br>论文设计了不同的模块：<br>A：没有 encoder 模块；<br>B：在 S3、S4、S5 三个不同分辨率的 feature map 上，分别执行 single scale encoder，然后把 encoder 后的结果拼起来；<br>C：类似于 Deformable DETR 中的 encoder 方法，先拼起来，然后进行 multi scale encoder；<br>D：在 S3、S4、S5 三个不同分辨率的 feature map 上，分别执行 single scale encoder，然后用类似于 PANet 的结构将特征融合；<br>E：仅在 S5 上执行 single scale encoder（AIFI），以减少计算量，然后用 CCFF 融合不同分辨率的 feature map。<br><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-05-24%2015.05.15.png" alt="截屏2024-05-24 15.05.15.png|475"></p>
<p>实验中可以看到，由 A 到 B 再到 C，模型性能逐渐提升的同时，计算延迟也大幅提升；由 C 到 D，性能提升的同时，小幅度减少了延迟，这说明不同 scale 的 feature 间的交互是必要的，但是不需要采用 multi scale encoder 的方式交互。由 D 到 $D_{S_5}$ 在延迟大幅度降低的情况下，准确率有了微小的提升，说明仅在 S5 上进行 encoder 就可以，不需要在更大尺度的 future map 上进行 encoder 操作。</p>
<h6 id="CCFF"><a href="#CCFF" class="headerlink" title="CCFF"></a>CCFF</h6><p><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-05-24%2015.52.35.png" alt="截屏2024-05-24 15.52.35.png|475"><br>融合模块采用了上图的设计。有一点疑问，RepVGG 的设计思路是希望网络在 inference 阶段是一个单 branch 的结构，CCFF 的设计是否一定程度上违背了 RepVGG 的思路？</p>
<h5 id="Uncertainty-minimal-Query-Selection"><a href="#Uncertainty-minimal-Query-Selection" class="headerlink" title="Uncertainty-minimal Query Selection"></a>Uncertainty-minimal Query Selection</h5><p>这部分描述的是选择 query 的方式，这里在论文里的描述有点绕，但是结合代码和别人的博客，我认为这里的实现实际上采用的是在 loss 中加入一个 vfl loss 的方式。Vfl loss 希望模型预测的类别概率与 iou 有关，iou 越高，类别概率也越高；反之亦然。<br>之前 detr-like 的方法里面，decoder 模型的 query 选择通常是按照类别概率高低来选择的，但是 query 实际上包含两个方面：类别和位置，仅考虑类别选出的 query 很可能并不是最优的。因此，rt-detr 在 loss 里面加入了 vfl loss，希望经过这样的训练，类别概率高的 query，iou 也能比较高。<br>论文里进行了实验：<br><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-06-12%2016.20.25.png" alt="截屏2024-06-12 16.20.25.png|300"><br>紫色是 rt-detr 用的方法，绿色是直接选择类别概率更高的 query。可以看出相比于原本的方法 rt-detr 可以使 query 在类别概率高的同时，iou 更高，即 rt-detr 可以选出质量更高的 query。</p>
<hr>
<h4 id="与-SOTA-方法比较"><a href="#与-SOTA-方法比较" class="headerlink" title="与 SOTA 方法比较"></a>与 SOTA 方法比较</h4><p><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-06-12%2016.37.59.png" alt="截屏2024-06-12 16.37.59.png"><br>与 detr-like 的方法比较，rt-detr 性能和速度上都有提升；与最好的 YOLO 方法比较，性能略微提升的情况下速度也有提升。</p>
<hr>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <categories>
        <category>技术积累</category>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>Image_Object_Detection</tag>
        <tag>CVPR2024</tag>
        <tag>DETR</tag>
      </tags>
  </entry>
  <entry>
    <title>格雷编码</title>
    <url>/posts/662fe68c/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h4 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h4><p>n 位格雷码序列是一个由 2n 个整数组成的序列，其中：</p>
<ul>
<li>每个整数都在范围 [0, 2n - 1] 内（含 0 和 2n - 1）</li>
<li>第一个整数是 0</li>
<li>一个整数在序列中出现不超过一次</li>
<li>每对相邻整数的二进制表示恰好一位不同，且第一个和最后一个整数的二进制表示恰好一位不同</li>
</ul>
<p>给你一个整数 n ，返回任一有效的 n 位格雷码序列。</p>
<hr>
<h4 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h4><p>一个找规律的题，n 位格雷编码可以由 n-1 位编码快速生成。其中前一半就是 n-1 位格雷编码，后一半由 n-1 位格雷编码倒排，且前面加 1 构成。</p>
<hr>
<h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">grayCode</span>(<span class="params">self, n: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        <span class="keyword">if</span> n == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> [<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> n == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> [<span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">        </span><br><span class="line">        temp = <span class="variable language_">self</span>.grayCode(n - <span class="number">1</span>)</span><br><span class="line">        result = []</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> temp:</span><br><span class="line">            result.append(t)</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> temp[::-<span class="number">1</span>]:</span><br><span class="line">            result.append(<span class="built_in">pow</span>(<span class="number">2</span>, n-<span class="number">1</span>) + t)</span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <categories>
        <category>技术积累</category>
        <category>编程技巧</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>Decoder-Only优势</title>
    <url>/posts/44b7472c/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h4 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h4><p>为什么目前的模型大多数采用 decoder only 架构而非 encoder decoder 模型？</p>
<hr>
<h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><p>为什么现在的 LLM 都是 Decoder only 的架构？ - Sam 聊算法的回答 - 知乎<br><a href="https://www.zhihu.com/question/588325646/answer/3357252612">https://www.zhihu.com/question/588325646/answer/3357252612</a></p>
<hr>
<h4 id="实验证据"><a href="#实验证据" class="headerlink" title="实验证据"></a>实验证据</h4><ul>
<li>Zero shot 任务里，decoder only 模型泛化性更好 <a href="https://proceedings.mlr.press/v162/wang22u/wang22u.pdf">wang22u.pdf</a></li>
<li>Few shot 任务里，也是一样 <a href="https://arxiv.org/pdf/2212.10559">2212.10559</a></li>
</ul>
<hr>
<h4 id="理论说明"><a href="#理论说明" class="headerlink" title="理论说明"></a>理论说明</h4><ul>
<li><a href="https://kexue.fm/archives/9529">为什么现在的LLM都是Decoder-only的架构？ - 科学空间|Scientific Spaces</a> 里认为可能是由于 encoder decoder 模型可能导致 attention map 是低秩的，但 decoder only 模型里采用 causal attention 的 attention map 一定是满秩的，所以有更强的表达能力；</li>
<li>为什么现在的 LLM 都是 Decoder only 的架构？ - yili 的回答 - 知乎<br><a href="https://www.zhihu.com/question/588325646/answer/3173454912">https://www.zhihu.com/question/588325646/answer/3173454912</a> 里提到双向 attention 降低了学习难度，decoder only 模型仅采用单向 attention，学习时难度更高，训好后效果更好</li>
<li>Causal attention 具有隐式的位置编码功能 <a href="https://arxiv.org/pdf/2203.16634">2203.16634</a></li>
</ul>
<hr>
<h4 id="个人看法"><a href="#个人看法" class="headerlink" title="个人看法"></a>个人看法</h4><p>目前似乎没有特别完备的理论说明证明 decoder only 一定是最好的，不过暂时采用这样架构的会比较多。</p>
<hr>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <categories>
        <category>技术积累</category>
        <category>杂七杂八</category>
      </categories>
      <tags>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title>Decoder-Only or Encoder-Decoder</title>
    <url>/posts/49eb3a2b/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>题目：</strong> Decoder-Only or Encoder-Decoder? Interpreting Language Model as a Regularized Encoder-Decoder<br><strong>链接：</strong> <a href="https://arxiv.org/pdf/2304.04052">https://arxiv.org/pdf/2304.04052</a><br><strong>来源：</strong> arxiv<br><strong>单位：</strong> Language Technology Lab, University of Cambridge；The Chinese University of Hong Kong；JD. Com；Department of Computer Science and Technology, Tsinghua University</p>
<hr>
<h4 id="本文目的"><a href="#本文目的" class="headerlink" title="本文目的"></a>本文目的</h4><p>以前的论文里有的认为 decoder only 结构更好，有的认为 encoder decoder 结构更好，本文希望进一步分析比较这两种模型。</p>
<hr>
<h4 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h4><h5 id="Regularized-Encoder-Decoder"><a href="#Regularized-Encoder-Decoder" class="headerlink" title="Regularized Encoder-Decoder"></a>Regularized Encoder-Decoder</h5><h6 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h6><p>对传统 encoder decoder 模型进行了变体，使模型在行为上更接近 decoder only 模型。</p>
<h6 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h6><p><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-12-23%2016.09.47.png" alt="截屏2024-12-23 16.09.47.png"></p>
<ul>
<li>Unidirectional Cross Attention：encoder decoder 在 encoder 部分采用 self attention，在 decoder 部分采用 cross attention；RED 模型采用了统一的 cross attention 结构模拟 decoder only 模型；</li>
<li>Source Auto-Encoder：decoder only 会预测所有文本的 next token，即对于 source text 也会预测。为了模拟这种情况，RED 采用 source auto-encoder 预测 source text；</li>
<li>Parameter Sharing：encoder 与 decoder 参数共享；</li>
<li>Layer-Wise Coordination：encoder decoder 模型里，在 decoder 执行过程中每层拿到的 encoder 的结果是不变的，但在 decoder only 的模型里，这部分是一直变化的；因此 RED 采用对应层的 encoder 特征作为输入；</li>
<li>Consecutive Positional Encoding：对 target sequence 的位置编码接着 source sequence 继续，而非从 1 开始。</li>
</ul>
<hr>
<h4 id="Decoder-only-模型的缺陷"><a href="#Decoder-only-模型的缺陷" class="headerlink" title="Decoder only 模型的缺陷"></a>Decoder only 模型的缺陷</h4><p>随着输出序列长度的增加，source sequence 的影响在逐渐减弱。直观理解来说，encoder decoder 模型里，source sequence 的特征是计算好的，但是在 decoder only 模型里是需要和 target sequence 拼起来计算 attention，因此随着 target sequence 长度增加，source sequence 影响会减弱。论文里用了数学公式证明这一点。</p>
<hr>
<h4 id="修改后的结构"><a href="#修改后的结构" class="headerlink" title="修改后的结构"></a>修改后的结构</h4><p><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-12-23%2017.41.10.png" alt="截屏2024-12-23 17.41.10.png"></p>
<ul>
<li>增加了一个以 source sequence embedding 作为 key 和 value 的 attention 层；</li>
<li>采用 bidirectional attention mask（上图红线部分）；</li>
<li>Source sequence 与 target sequence 分开编码；</li>
<li>加入 language embedding。</li>
</ul>
<hr>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <categories>
        <category>技术积累</category>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>论文笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>Scaling on Scales</title>
    <url>/posts/2deeafe3/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>题目：</strong> When Do We Not Need Larger Vision Models?<br><strong>链接：</strong> <a href="https://arxiv.org/pdf/2403.13043">https://arxiv.org/pdf/2403.13043</a><br><strong>代码：</strong> <a href="https://github.com/bfshi/scaling_on_scales">https://github.com/bfshi/scaling_on_scales</a><br><strong>来源：</strong> ECCV 2024<br><strong>单位：</strong> UC Berkeley；Microsoft Research</p>
<hr>
<h4 id="研究的问题"><a href="#研究的问题" class="headerlink" title="研究的问题"></a>研究的问题</h4><p>计算机视觉领域大模型是否真的带来了更好的效果？</p>
<hr>
<h4 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h4><p><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-12-20%2018.22.33.png" alt="截屏2024-12-20 18.22.33.png"></p>
<ul>
<li>把低分辨率图像插值得到高分辨率图像（防止直接用高分辨率图像引入额外信息）；</li>
<li>高分辨率图像裁成与低分辨率图像一样大小的 patch，并用网络进行特征提取；</li>
<li>不同 patch 的 feature 拼成整个 feature map 后，pooling 得到与低分辨率图像尺寸相同的 feature map；</li>
<li>不同 feature map 拼起来。</li>
</ul>
<hr>
<h4 id="在不同任务和不同-backbone-上与增大模型尺寸对比"><a href="#在不同任务和不同-backbone-上与增大模型尺寸对比" class="headerlink" title="在不同任务和不同 backbone 上与增大模型尺寸对比"></a>在不同任务和不同 backbone 上与增大模型尺寸对比</h4><p><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-12-20%2018.54.12.png" alt="截屏2024-12-20 18.54.12.png"></p>
<ul>
<li>在图像分割和深度预测上，Scaling on Scales 效果都比较好，这也比较符合直觉，在这些任务上增加分辨率能提供更多细节，从而提升效果；</li>
<li>在分类任务的部分 backbone 上，Scaling on Scales 比增大模型大小效果差。</li>
</ul>
<p><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-12-20%2018.56.34.png" alt="截屏2024-12-20 18.56.34.png"></p>
<ul>
<li>在 LLM 相关任务上，Scaling on Scales 效果都更好。</li>
</ul>
<hr>
<h4 id="个人看法"><a href="#个人看法" class="headerlink" title="个人看法"></a>个人看法</h4><p>这篇通过实验验证了在 GFLOPS 相近的情况下，小模型+多尺度输入通常略优于或等效于大模型的性能，但是在附录的实验里，小模型+多尺度输入的图像处理速度低于单个大模型，因此似乎并没有明显的优势。</p>
<hr>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <categories>
        <category>技术积累</category>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>ECCV2024</tag>
      </tags>
  </entry>
  <entry>
    <title>柱状图中最大的矩形</title>
    <url>/posts/7fcd667a/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h4 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h4><p>给定 n 个非负整数，用来表示柱状图中各个柱子的高度。每个柱子彼此相邻，且宽度为 1。<br>求在该柱状图中，能够勾勒出来的矩形的最大面积。</p>
<hr>
<h4 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h4><p>递增栈。当高度值逐渐增加就不断入栈，否则出栈并计算最大面积。</p>
<hr>
<h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">largestRectangleArea</span>(<span class="params">self, heights: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        stack = [-<span class="number">1</span>]</span><br><span class="line">        result = <span class="number">0</span></span><br><span class="line">        heights.append(-<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i, val <span class="keyword">in</span> <span class="built_in">enumerate</span>(heights):</span><br><span class="line">            <span class="keyword">while</span> val &lt; heights[stack[-<span class="number">1</span>]]:</span><br><span class="line">                h = heights[stack.pop()]</span><br><span class="line">                result = <span class="built_in">max</span>(result, (i - stack[-<span class="number">1</span>] - <span class="number">1</span>) * h)</span><br><span class="line">                stack.append(i)</span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>

<hr>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <categories>
        <category>技术积累</category>
        <category>编程技巧</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>栈</tag>
      </tags>
  </entry>
  <entry>
    <title>latent diffusion</title>
    <url>/posts/16f61640/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>题目：</strong> High-Resolution Image Synthesis with Latent Diffusion Models<br><strong>链接：</strong> <a href="https://arxiv.org/pdf/2112.10752">https://arxiv.org/pdf/2112.10752</a><br><strong>代码：</strong> <a href="https://github.com/CompVis/latent-diffusion">https://github.com/CompVis/latent-diffusion</a><br><strong>来源：</strong> CVPR 2022<br><strong>单位：</strong> Ludwig Maximilian University of Munich &amp; IWR, Heidelberg University, Germany；Runway ML</p>
<hr>
<h4 id="本文希望解决的问题及解决方案"><a href="#本文希望解决的问题及解决方案" class="headerlink" title="本文希望解决的问题及解决方案"></a>本文希望解决的问题及解决方案</h4><p>本文想要解决的问题是 Diffusion Model 训练和推理成本都过高。文章中认为 diffusion model 在每个像素上计算是没必要的，因此希望找到一个等效空间，在该空间上进行 diffusion 操作。</p>
<hr>
<h4 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h4><p><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-12-20%2010.58.33.png" alt="截屏2024-12-20 10.58.33.png"></p>
<h5 id="Perceptual-Image-Compression"><a href="#Perceptual-Image-Compression" class="headerlink" title="Perceptual Image Compression"></a>Perceptual Image Compression</h5><p>训练一个 autoencoder 把图像压缩到 latent space 中。因为 diffusion model 本身是在二维空间上操作的，因此本文压缩后的 $z$ 也是一个二维的，相比于那些压缩到一维的方法，能保留更多信息，效果更好。</p>
<h5 id="Conditioning-Mechanisms"><a href="#Conditioning-Mechanisms" class="headerlink" title="Conditioning Mechanisms"></a>Conditioning Mechanisms</h5><p>在 UNet backbone 中加入 cross attention 机制，其中 Q 是原本的 latent feature，K、V 是 condition feature，从而实现控制。</p>
<hr>
<h4 id="代码解读"><a href="#代码解读" class="headerlink" title="代码解读"></a>代码解读</h4><p><a href="https://blog.csdn.net/yusijinfs/article/details/134684608">一文详解 Latent Diffusion官方源码_diagonalgaussiandistribution-CSDN博客</a></p>
<hr>
<h4 id="个人看法"><a href="#个人看法" class="headerlink" title="个人看法"></a>个人看法</h4><p>相比于原始的 diffusion，latent diffusion 主要有两点改进：</p>
<ul>
<li>通过 autoencoder，在 latent 空间进行 diffusion 操作；</li>
<li>用 cross attention 对生成内容进行控制。</li>
</ul>
<hr>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <categories>
        <category>技术积累</category>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>Diffusion</tag>
        <tag>Image_Generation</tag>
        <tag>高引</tag>
        <tag>CVPR2022</tag>
      </tags>
  </entry>
  <entry>
    <title>比较好的技术解析文章</title>
    <url>/posts/1528f15d/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h4 id="Vae-的原理"><a href="#Vae-的原理" class="headerlink" title="Vae 的原理"></a>Vae 的原理</h4><p><strong>链接：</strong> <a href="https://zhuanlan.zhihu.com/p/628604566">https://zhuanlan.zhihu.com/p/628604566</a><br>比较浅显易懂地说明了 vae 模型设计的出发点、基本原理以及损失函数的推导。</p>
<hr>
<h4 id="DDIM-原理"><a href="#DDIM-原理" class="headerlink" title="DDIM 原理"></a>DDIM 原理</h4><p><strong>链接：</strong> <a href="https://www.zhangzhenhu.com/aigc/ddim.html#equation-eq-ddim-216">3. 去噪扩散隐式模型（Denoising Diffusion Implicit Models,DDIM） — 张振虎的博客 张振虎 文档</a><br>DDIM 的解析里比较好懂的一个。</p>
<hr>
<h4 id="不同的-normalization"><a href="#不同的-normalization" class="headerlink" title="不同的 normalization"></a>不同的 normalization</h4><p><strong>链接：</strong> <a href="https://zhuanlan.zhihu.com/p/86765356">https://zhuanlan.zhihu.com/p/86765356</a><br>说明了 BN、LN、IN 以及 GN 的区别。</p>
<hr>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <categories>
        <category>技术积累</category>
        <category>杂七杂八</category>
      </categories>
  </entry>
  <entry>
    <title>qwen2-vl</title>
    <url>/posts/5fae19d/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>题目：</strong> Qwen 2-VL: Enhancing Vision-Language Model’s Perception of the World at Any Resolution<br><strong>链接：</strong> <a href="https://arxiv.org/pdf/2409.12191">https://arxiv.org/pdf/2409.12191</a><br><strong>代码：</strong> <a href="https://github.com/QwenLM/Qwen2-VL">https://github.com/QwenLM/Qwen2-VL</a><br><strong>来源：</strong> arxiv<br><strong>单位：</strong> Qwen Team Alibaba Group</p>
<hr>
<h4 id="目前方法的缺陷及-qwen2-vl-的解决方法"><a href="#目前方法的缺陷及-qwen2-vl-的解决方法" class="headerlink" title="目前方法的缺陷及 qwen2-vl 的解决方法"></a>目前方法的缺陷及 qwen2-vl 的解决方法</h4><ul>
<li>当前大多数 LVLM 会固定输入图像的尺寸，这会导致高分辨率图像信息的丢失。Qwen2-vl 在训练过程中引入动态分辨率；</li>
<li>当前大多数模型在处理视频时，会把视频帧看作互不关联的图像。Qwen2-vl 设计了 multimodal rotary position embedding 处理时空维度的信息。</li>
</ul>
<hr>
<h4 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h4><h5 id="模型尺寸"><a href="#模型尺寸" class="headerlink" title="模型尺寸"></a>模型尺寸</h5><p><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-12-18%2018.21.52.png" alt=""></p>
<h5 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h5><p><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-12-18%2018.26.03.png" alt=""></p>
<ul>
<li>Naive Dynamic Resolution：采用 Navit 以及 2D-RoPE；</li>
<li>M-RoPE：<img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-12-18%2018.53.00.png" alt=""></li>
<li>Unified Image and Video Understanding：输入是单张图时，会把图像复制一份，看作一个 2 帧的视频，从而实现视频和图像的统一。第一层卷积会采用 3D 卷积，使模型可以提取视频三维特征。</li>
</ul>
<h5 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h5><p>分为三个阶段：</p>
<ul>
<li>仅训练 ViT：语言模型从 qwen2 初始化，视觉模型从 DFN 的 ViT 权重初始化；训练的主要任务包括：learning image-text relationships、textual content recognition within images through OCR、image classification tasks；</li>
<li>训练所有模块：引入更多任务和数据；</li>
<li>仅训练语言模型：构建 instruction-following data 用于训练。</li>
</ul>
<hr>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <categories>
        <category>技术积累</category>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>论文笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>git使用</title>
    <url>/posts/4896de77/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h4 id="把本地分支上传到远程指定分支"><a href="#把本地分支上传到远程指定分支" class="headerlink" title="把本地分支上传到远程指定分支"></a>把本地分支上传到远程指定分支</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git push origin &lt;本地分支名&gt;:&lt;远程分支名&gt;</span><br></pre></td></tr></table></figure>

<h4 id="用-lfs-上传大文件"><a href="#用-lfs-上传大文件" class="headerlink" title="用 lfs 上传大文件"></a>用 lfs 上传大文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git lfs track [文件名]</span><br><span class="line">git add .gitattributes</span><br><span class="line">git add [文件名]</span><br><span class="line">git commit -m</span><br><span class="line">git push</span><br></pre></td></tr></table></figure>

<hr>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <categories>
        <category>技术积累</category>
        <category>杂七杂八</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>最小覆盖子串</title>
    <url>/posts/1c771f88/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h4 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h4><p>给你一个字符串 s、一个字符串 t。返回 s 中涵盖 t 所有字符的最小子串。如果 s 中不存在涵盖 t 所有字符的子串，则返回空字符串 “”。</p>
<hr>
<h4 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h4><p>双指针，先把后一个指针逐渐向后搜索直到找到可以覆盖 t 的字符串，然后把前面的指针逐渐向后挪，直到两个指针中间的字符串无法覆盖 t。<br>另一个核心是如何判定字符串是否覆盖 t。可以采用 collections，当所有字符的计数小于等于 0 时，意味着覆盖了。</p>
<hr>
<h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">minWindow</span>(<span class="params">self, s: <span class="built_in">str</span>, t: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        <span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">        </span><br><span class="line">        sub_counter = Counter(t)</span><br><span class="line">        required = <span class="built_in">len</span>(t)</span><br><span class="line">        start = <span class="number">0</span></span><br><span class="line">        end = <span class="number">0</span></span><br><span class="line">        min_length = <span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>)</span><br><span class="line">        min_start = <span class="literal">None</span></span><br><span class="line">        match_count = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span> end &lt; <span class="built_in">len</span>(s):</span><br><span class="line">            char = s[end]</span><br><span class="line">            <span class="keyword">if</span> sub_counter[char] &gt; <span class="number">0</span>:</span><br><span class="line">                match_count += <span class="number">1</span></span><br><span class="line">            sub_counter[char] -= <span class="number">1</span></span><br><span class="line">            end += <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">            <span class="keyword">while</span> match_count == required:</span><br><span class="line">                <span class="keyword">if</span> end - start &lt; min_length:</span><br><span class="line">                    min_length = end - start</span><br><span class="line">                    min_start = start</span><br><span class="line">                char = s[start]</span><br><span class="line">                sub_counter[char] += <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> sub_counter[char] &gt; <span class="number">0</span>:</span><br><span class="line">                    match_count -= <span class="number">1</span></span><br><span class="line">                start += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> min_length == <span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>):</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> s[min_start:min_start + min_length]</span><br></pre></td></tr></table></figure>

<hr>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <categories>
        <category>技术积累</category>
        <category>编程技巧</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>双指针</tag>
      </tags>
  </entry>
  <entry>
    <title>矩阵置零</title>
    <url>/posts/18e1d79c/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h4 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h4><p>给定一个 m x n 的矩阵，如果一个元素为 0，则将其所在行和列的所有元素都设为 0。请使用原地算法。</p>
<hr>
<h4 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h4><p>题目本身比较简单，但进阶要求里要求空间复杂度为 $O(1)$。这样就无法记录每一行每一列的状态，否则空间复杂度就是 $O(m+n)$。<br>思路是先把第一行和第一列是否包含 0 的状态用变量记录下来，然后用第一行和第一列的元素记录该行该列是否含 0，之后根据记录将对应的行或列设为 0，最后根据最开始的记录修改第一行第一列的值。</p>
<hr>
<h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">setZeroes</span>(<span class="params">self, matrix: <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        first_row = <span class="literal">False</span></span><br><span class="line">        first_col = <span class="literal">False</span></span><br><span class="line">        h, w = <span class="built_in">len</span>(matrix), <span class="built_in">len</span>(matrix[<span class="number">0</span>])</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> matrix[<span class="number">0</span>]:</span><br><span class="line">            <span class="keyword">if</span> t == <span class="number">0</span>:</span><br><span class="line">                first_row = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(h):</span><br><span class="line">            <span class="keyword">if</span> matrix[i][<span class="number">0</span>] == <span class="number">0</span>:</span><br><span class="line">                first_col = <span class="literal">True</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, h):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, w):</span><br><span class="line">                <span class="keyword">if</span> matrix[i][j] == <span class="number">0</span>:</span><br><span class="line">                    matrix[i][<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">                    matrix[<span class="number">0</span>][j] = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, h):</span><br><span class="line">            <span class="keyword">if</span> matrix[i][<span class="number">0</span>] == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, w):</span><br><span class="line">                    matrix[i][j] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, w):</span><br><span class="line">            <span class="keyword">if</span> matrix[<span class="number">0</span>][i] == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, h):</span><br><span class="line">                    matrix[j][i] = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> first_row:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(w):</span><br><span class="line">                matrix[<span class="number">0</span>][i] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> first_col:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(h):</span><br><span class="line">                matrix[i][<span class="number">0</span>] = <span class="number">0</span></span><br></pre></td></tr></table></figure>

<hr>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <categories>
        <category>技术积累</category>
        <category>编程技巧</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>编辑距离</title>
    <url>/posts/ca4861d0/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h4 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h4><p>给你两个单词 word1 和 word2，请返回将 word1 转换成 word2 所使用的最少操作数。</p>
<p>你可以对一个单词进行如下三种操作：</p>
<ul>
<li>插入一个字符</li>
<li>删除一个字符</li>
<li>替换一个字符</li>
</ul>
<hr>
<h4 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h4><p>动态规划。考虑 word1 到第 i 个字符及 word2 到第 j 个字符的子问题，可分为两种情况：</p>
<ul>
<li>两个子字符串最后一个字符相同，则当前两个字符串的编辑距离与 word1 到第 i-1 个字符及 word2 到第 j-1 个字符的编辑距离相同；</li>
<li>最后一个字符不同，则可以进行以下三种操作：<ul>
<li>修改最后一个字符使其相同，则当前两个字符串的编辑距离为 word1 到第 i-1 个字符及 word2 到第 j-1 个字符的编辑距离加 1；</li>
<li>删掉 word1 的最后一个字符，则当前两个字符串的编辑距离为 word1 到第 i-1 个字符及 word2 到第 j 个字符的编辑距离加 1；</li>
<li>在 word1 后增加一个字符，则当前两个字符串的编辑距离为 word1 到第 i 个字符及 word2 到第 j-1 个字符的编辑距离加 1。</li>
</ul>
</li>
</ul>
<hr>
<h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">minDistance</span>(<span class="params">self, word1: <span class="built_in">str</span>, word2: <span class="built_in">str</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">        result = np.zeros((<span class="built_in">len</span>(word1) + <span class="number">1</span>, <span class="built_in">len</span>(word2) + <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(word1) + <span class="number">1</span>):</span><br><span class="line">            result[i][<span class="number">0</span>] = i</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(word2) + <span class="number">1</span>):</span><br><span class="line">            result[<span class="number">0</span>][i] = i</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(word1) + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(word2) + <span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> word1[i - <span class="number">1</span>] == word2[j - <span class="number">1</span>]:</span><br><span class="line">                    result[i][j] = result[i - <span class="number">1</span>][j - <span class="number">1</span>]</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    result[i][j] = <span class="number">1</span> + <span class="built_in">min</span>(<span class="built_in">min</span>(result[i - <span class="number">1</span>][j - <span class="number">1</span>], result[i][j - <span class="number">1</span>]), result[i - <span class="number">1</span>][j])</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">int</span>(result[-<span class="number">1</span>][-<span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<hr>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <categories>
        <category>技术积累</category>
        <category>编程技巧</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title>图像读写</title>
    <url>/posts/9b91fdde/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h4 id="图像保存为-jpg-后，再次读入像素值有微小变化"><a href="#图像保存为-jpg-后，再次读入像素值有微小变化" class="headerlink" title="图像保存为 .jpg 后，再次读入像素值有微小变化"></a>图像保存为 .jpg 后，再次读入像素值有微小变化</h4><p>.jpg 会导致有损压缩，保存为 .png 可以解决该问题。</p>
<hr>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <categories>
        <category>技术积累</category>
        <category>杂七杂八</category>
      </categories>
      <tags>
        <tag>图像读写</tag>
      </tags>
  </entry>
  <entry>
    <title>MiniDrive</title>
    <url>/posts/9dfc0fd2/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>题目：</strong> MiniDrive: More Efficient Vision-Language Models with Multi-Level 2 D Features as Text Tokens for Autonomous Driving<br><strong>链接：</strong> <a href="https://arxiv.org/pdf/2409.07267">https://arxiv.org/pdf/2409.07267</a><br><strong>代码：</strong> <a href="https://github.com/EMZucas/minidrive">https://github.com/EMZucas/minidrive</a><br><strong>来源：</strong> arxiv<br><strong>单位：</strong> School of Artificial Intelligence, University of Chinese Academy of Sciences；State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences</p>
<hr>
<h4 id="目前模型缺陷"><a href="#目前模型缺陷" class="headerlink" title="目前模型缺陷"></a>目前模型缺陷</h4><ul>
<li>模型规模比较大，导致实际部署困难；</li>
<li>自动驾驶通常会采用多张图作为输入，目前的模型大多没有结合多张图的能力。</li>
</ul>
<hr>
<h4 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h4><h5 id="模型整体结构"><a href="#模型整体结构" class="headerlink" title="模型整体结构"></a>模型整体结构</h5><p><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-12-10%2018.36.05.png" alt="截屏2024-12-10 18.36.05.png"></p>
<ul>
<li>Vision encoder 提取多张图的特征，模型采用 UniRepLKNet；</li>
<li>FE-MoE 融合多张图特征（右侧），对多个 expert 的结果加权；</li>
<li>Text embedding 作为 key 和 value，vision embedding 作为 query 生成新的特征 $V’$；</li>
<li>$V’$ 和 V 融合后作为新的 vision embedding 与 text embedding 一起输入大语言模型，并生成最终的结果。</li>
</ul>
<hr>
<h4 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h4><h5 id="与其他方法比"><a href="#与其他方法比" class="headerlink" title="与其他方法比"></a>与其他方法比<img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-12-10%2019.16.29.png" alt="截屏2024-12-10 19.16.29.png"></h5><p>以 83M 的参数量取得了与更大规模模型相近的效果。</p>
<hr>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <categories>
        <category>技术积累</category>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>论文笔记</tag>
        <tag>自动驾驶</tag>
      </tags>
  </entry>
  <entry>
    <title>水母不会冻结</title>
    <url>/posts/6b36b444/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h4 id="故事梗概"><a href="#故事梗概" class="headerlink" title="故事梗概"></a>故事梗概</h4><h5 id="水母船视角"><a href="#水母船视角" class="headerlink" title="水母船视角"></a>水母船视角</h5><p>水母船测试最后一日，二月八日，测试成员威廉起床后发现费弗教授死了。</p>
<h5 id="地面视角"><a href="#地面视角" class="headerlink" title="地面视角"></a>地面视角</h5><p>警局接到水母船坠毁的报警，6 名成员死亡，且其中一名头和手脚都被砍下。根据现场情况，警方认为是紧急迫降后发生的谋杀案。</p>
<h5 id="水母船视角-1"><a href="#水母船视角-1" class="headerlink" title="水母船视角"></a>水母船视角</h5><p>其余五人讨论教授的死因，爱德华提出可能是毒杀而非疾病发作，内维尔坚持继续航行测试，测试结束后再报警。众人发现水母船偏离了预定航线且无法手动航行。</p>
<h5 id="地面视角-1"><a href="#地面视角-1" class="headerlink" title="地面视角"></a>地面视角</h5><p>查明其中一名死者身份为费弗教授，并推测其余死者是他的学生。警方搜查技术开发部时发现内维尔的值班日记中提到了 R，然而技术开发部的职员名单里找不到与 R 可以对应的人，但在一张合影里找到了 R 的照片。在搜查电脑时，警方发现制作航行程序的电脑被清空。</p>
<h5 id="水母船视角-2"><a href="#水母船视角-2" class="headerlink" title="水母船视角"></a>水母船视角</h5><p>水母船带着众人被困在雪原上，众人在水母船内等待救援。众人分水时，内维尔担心水被下毒选择了喝酒，然而依然被毒死。克里斯精神崩溃时提到了可能与瑞贝卡的死相关。</p>
<h5 id="地面视角-2"><a href="#地面视角-2" class="headerlink" title="地面视角"></a>地面视角</h5><p>警方推测出军方是制造这艘新水母船的委托人。空军提出在他们的认知里研发部的人是在测试前就失去了联系，并说出空军的委托是开发隐形水母船。空军整理的遗物内包含一张实验笔记纸，实验笔记属于瑞贝卡。</p>
<h5 id="水母船视角-3"><a href="#水母船视角-3" class="headerlink" title="水母船视角"></a>水母船视角</h5><p>剩余的四人决定搜查水母船，但没有搜到外人的痕迹。克里斯借口单独去拿烟，拿出了霰弹枪。</p>
<h5 id="地面视角-3"><a href="#地面视角-3" class="headerlink" title="地面视角"></a>地面视角</h5><p>警方推测瑞贝卡才是发明水母船的关键，教授等人在她去世后抹去了她的名字发表了成果。警方在调查中发现瑞贝卡死于一场实验意外，并前往当时她就读的大学调查那起意外。根据当时的第一发现人的描述，当时的实验室是个密室，塑料片卡住了门且窗户也是关着的，室内没有其他人，且瑞贝卡的实验笔记消失不见了。负责此案的警察表示，他们当时注意到了案件的疑点，但瑞贝卡生前曾被强暴，这可能是她自杀的理由。在无法确定是意外还是自杀的情况下，为了照顾瑞贝卡朋友的情绪，警方按照意外说进行了结案。</p>
<div class='spoiler collapsed'>
    <div class='spoiler-title'>
        瑞贝卡案的真相
    </div>
    <div class='spoiler-content'>
        <p>卡住门的塑料片是制造水母船的素体，素体在硬化前很柔软，可以塞进门缝，酸碱滴定产生的氰化氢硬化了门缝内的素体，使案发现场成为了密室。</p>

    </div>
</div>

<p>根据尸检结果，六名受害人全部是他杀。</p>
<h5 id="水母船视角-4"><a href="#水母船视角-4" class="headerlink" title="水母船视角"></a>水母船视角</h5><p>爱德华与威廉联手制服克里斯，拉扯间威廉射杀克里斯。威廉提起当初瑞贝卡是在其他地点去世，大学实验室是伪造出的现场。</p>
<h5 id="地面视角-4"><a href="#地面视角-4" class="headerlink" title="地面视角"></a>地面视角</h5><p>警方认为现在存在凶手在 6 人之中及凶手不在 6 人之中两种可能性。根据验尸结果，第一种可能性不存在。如果是第二种情况，凶手很难一直躲在水母船里不被发现，如何潜入水母船案发后如何逃脱也会成为问题。<br>一名警察提出只需要提前准备一艘水母船等在迫降地，犯案后乘水母船离开即可，但这样意味着凶手是外人无法完成自动驾驶程序的修改。<br>空军收到的测试计划里，测试时间比真正的测试时间晚了三天，警方认为这可能意味着开发部计划潜逃。</p>
<h5 id="水母船视角-5"><a href="#水母船视角-5" class="headerlink" title="水母船视角"></a>水母船视角</h5><p>威廉发现琳达与爱德华都被杀，寻找凶手时，自己也被凶手击打后脑身亡。</p>
<h5 id="地面视角-5"><a href="#地面视角-5" class="headerlink" title="地面视角"></a>地面视角</h5><p>费弗教授宅邸被烧，警方据此推测还存在第 7 人，因为否则不需要毁掉教授的宅邸，且打电话报警的人就是凶手。在别墅后的森林里发现了曾经掩埋过人的痕迹。警方确认在前五个检查点都有人下机采购，按照顺序分别是：克里斯、不明身份的人、琳达、威廉以及内维尔，水母船没有到达第六检查点就偏离了航线。</p>
<div class='spoiler collapsed'>
    <div class='spoiler-title'>
        尸检结果
    </div>
    <div class='spoiler-content'>
        <p>根据法医检查结果，被分尸的尸体身份是——西蒙。</p>

    </div>
</div>

<p>根据空军反馈结果，隐形水母船的研发成功了，但空军回收的坠毁水母船样本却没有隐形功能。</p>
<div class='spoiler collapsed'>
    <div class='spoiler-title'>
        真相
    </div>
    <div class='spoiler-content'>
        <p>凶手是爱德华。西蒙被分尸的原因是他在测试前就去世了，凶手为了搬运尸体，进行了分尸。因此机上的六个人里并不包括西蒙，而是由爱德华填补了空缺。<br>关于为什么上机的一定是六人而非五人，是从采购过程推断的，费弗教授酗酒且地位高不应参与采购，因此如果仅有 5 人，在第五检查点就应该发生轮班循环，然而 5 个检查点是 5 个不同的人负责采购，因此推断机上有 6 人。<br>多出来的一人被抹去是因为技术开发部的部分成员有另一个潜逃计划。他们计划牺牲部分成员，伪造其余成员下落不明的假象进行潜逃。这个计划被爱德华反向利用，抹去了自己的痕迹。在这个计划里，技术开发部准备了两艘水母船，一艘是新造的水母船，另一艘是存放在费弗教授家里的水母船样机，后一艘不在出售清单上，因此搜查时没有查到。”亡命组”计划控制旧水母船坠毁，新水母船上的成员潜逃，而爱德华使两艘水母船迫降，并搭乘其中一艘逃离雪山。在检查点，两艘水母船交替降落，因此检查点误认为一直是同一艘。</p>

    </div>
</div>


<hr>
<h4 id="个人评价"><a href="#个人评价" class="headerlink" title="个人评价"></a>个人评价</h4><p>这本的大体框架我都挺喜欢的，不管是大梗还是多线叙事的架构都挺吸引我的。但是读下来整体感觉很碎，文笔不是最差的那种，但是也没啥优点，这么多人物看完都没留下很深刻的印象，不管是被害人、凶手还是侦探。</p>
<hr>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <categories>
        <category>休闲娱乐</category>
        <category>书籍阅读</category>
      </categories>
      <tags>
        <tag>推理小说</tag>
        <tag>市川忧人</tag>
        <tag>叙述性诡计</tag>
        <tag>多线叙事</tag>
        <tag>鲇川哲也奖</tag>
      </tags>
  </entry>
  <entry>
    <title>接雨水</title>
    <url>/posts/66e39f70/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h4 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h4><p>给定 n 个非负整数表示每个宽度为 1 的柱子的高度图，计算按此排列的柱子，下雨之后能接多少雨水。</p>
<hr>
<h4 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h4><p>动态规划，找到每个位置向左和向右的最大值，根据两个最大值及当前位置的高度，计算每个位置的水量，最后求和。</p>
<hr>
<h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">trap</span>(<span class="params">self, height: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">        </span><br><span class="line">        length = <span class="built_in">len</span>(height)</span><br><span class="line">        left_height = np.zeros(length)</span><br><span class="line">        right_height = np.zeros(length)</span><br><span class="line">        </span><br><span class="line">        left_height[<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, length):</span><br><span class="line">            left_height[i] = <span class="built_in">max</span>(height[i-<span class="number">1</span>], left_height[i-<span class="number">1</span>])</span><br><span class="line">        </span><br><span class="line">        right_height[length-<span class="number">1</span>] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(length-<span class="number">2</span>, -<span class="number">1</span>, -<span class="number">1</span>):</span><br><span class="line">            right_height[i] = <span class="built_in">max</span>(right_height[i+<span class="number">1</span>], height[i+<span class="number">1</span>])</span><br><span class="line">        </span><br><span class="line">        result = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(length):</span><br><span class="line">            result += <span class="built_in">max</span>(<span class="built_in">min</span>(left_height[i], right_height[i]) - height[i], <span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">int</span>(result)</span><br></pre></td></tr></table></figure>

<hr>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <categories>
        <category>技术积累</category>
        <category>编程技巧</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title>缺失的第一个正数</title>
    <url>/posts/1cb194a8/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h4 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h4><p>给你一个未排序的整数数组 nums，请你找出其中没有出现的最小的正整数。<br>请你实现时间复杂度为 O(n) 并且只使用常数级别额外空间的解决方案。</p>
<hr>
<h4 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h4><p>从头开始遍历，如果当前值超过了数组长度范围，则跳过，否则不断与当前值对应下标上的数交换。交换结束后，如果某下标上的值与下标不符，则返回该结果。<br>需要注意重复元素的处理。</p>
<hr>
<h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">firstMissingPositive</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        length = <span class="built_in">len</span>(nums)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(length):</span><br><span class="line">            <span class="keyword">while</span> nums[i] &gt; <span class="number">0</span> <span class="keyword">and</span> nums[i] &lt;= length <span class="keyword">and</span> nums[i] != nums[nums[i] - <span class="number">1</span>]:</span><br><span class="line">                nums[nums[i] - <span class="number">1</span>], nums[i] = nums[i], nums[nums[i] - <span class="number">1</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(length):</span><br><span class="line">            <span class="keyword">if</span> nums[i] != i + <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">return</span> i + <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> length + <span class="number">1</span></span><br></pre></td></tr></table></figure>

<hr>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <categories>
        <category>技术积累</category>
        <category>编程技巧</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>花样滑冰</title>
    <url>/posts/6ce289f7/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><style>
    .cd-timeline.svelte-1t2uafz .svelte-1t2uafz,.cd-timeline.svelte-1t2uafz .svelte-1t2uafz::after,.cd-timeline.svelte-1t2uafz .svelte-1t2uafz::before{-webkit-box-sizing:border-box;box-sizing:border-box}.cd-timeline.svelte-1t2uafz{font-size:1.6rem;font-family:"Droid Serif", serif;color:#7f8c97;background-color:#e9f0f5}.cd-timeline.svelte-1t2uafz a{color:#acb7c0;text-decoration:none}.cd-timeline.svelte-1t2uafz img{max-width:100%}.cd-timeline.svelte-1t2uafz h1,h2{font-family:"Open Sans", sans-serif;font-weight:bold}.cd-timeline.svelte-1t2uafz{overflow:hidden;margin:2em auto}.cd-timeline__container.svelte-1t2uafz{position:relative;width:90%;max-width:1170px;margin:0 auto;padding:2em 0}.cd-timeline__container.svelte-1t2uafz::before{content:'';position:absolute;top:0;left:18px;height:100%;width:4px;background:#d7e4ed}@media only screen and (min-width: 1170px){.cd-timeline.svelte-1t2uafz{margin-top:3em;margin-bottom:3em}.cd-timeline__container.svelte-1t2uafz::before{left:50%;margin-left:-2px}}.cd-timeline__block.svelte-1t2uafz{position:relative;margin:2em 0}.cd-timeline__block.svelte-1t2uafz:after{content:"";display:table;clear:both}.cd-timeline__block.svelte-1t2uafz:first-child{margin-top:0}.cd-timeline__block.svelte-1t2uafz:last-child{margin-bottom:0}@media only screen and (min-width: 1170px){.cd-timeline__block.svelte-1t2uafz{margin:4em 0}}.cd-timeline__img.svelte-1t2uafz{position:absolute;top:0;left:0;width:40px;height:40px;border-radius:50%;-webkit-box-shadow:0 0 0 4px white, inset 0 2px 0 rgba(0, 0, 0, 0.08), 0 3px 0 4px rgba(0, 0, 0, 0.05);box-shadow:0 0 0 4px white, inset 0 2px 0 rgba(0, 0, 0, 0.08), 0 3px 0 4px rgba(0, 0, 0, 0.05)}.cd-timeline__img.svelte-1t2uafz{background:#75ce66}@media only screen and (min-width: 1170px){.cd-timeline__img.svelte-1t2uafz{width:60px;height:60px;left:50%;margin-left:-30px;-webkit-transform:translateZ(0);transform:translateZ(0)}}@-webkit-keyframes svelte-1t2uafz-cd-bounce-1{0%{opacity:0;-webkit-transform:scale(0.5);transform:scale(0.5)}60%{opacity:1;-webkit-transform:scale(1.2);transform:scale(1.2)}100%{-webkit-transform:scale(1);transform:scale(1)}}@keyframes svelte-1t2uafz-cd-bounce-1{0%{opacity:0;-webkit-transform:scale(0.5);transform:scale(0.5)}60%{opacity:1;-webkit-transform:scale(1.2);transform:scale(1.2)}100%{-webkit-transform:scale(1);transform:scale(1)}}.cd-timeline__content.svelte-1t2uafz{position:relative;margin-left:60px;background:white;border-radius:0.25em;padding:1em;-webkit-box-shadow:0 3px 0 #d7e4ed;box-shadow:0 3px 0 #d7e4ed}.cd-timeline__content.svelte-1t2uafz:after{content:"";display:table;clear:both}.cd-timeline__content.svelte-1t2uafz::before{content:'';position:absolute;top:16px;right:100%;height:0;width:0;border:7px solid transparent;border-right:7px solid white}.cd-timeline__content.svelte-1t2uafz h2.svelte-1t2uafz{color:#303e49}.cd-timeline__content.svelte-1t2uafz p,.cd-timeline__date.svelte-1t2uafz{font-size:1.3rem}.cd-timeline__content.svelte-1t2uafz p{margin:1em 0;line-height:1.6}.cd-timeline__date.svelte-1t2uafz{display:inline-block}.cd-timeline__date.svelte-1t2uafz{float:left;padding:.8em 0;opacity:.7}@media only screen and (min-width: 768px){.cd-timeline__content.svelte-1t2uafz h2.svelte-1t2uafz{font-size:2rem}.cd-timeline__content.svelte-1t2uafz p{font-size:1.6rem}.cd-timeline__date.svelte-1t2uafz{font-size:1.4rem}}@media only screen and (min-width: 1170px){.cd-timeline__content.svelte-1t2uafz{margin-left:0;padding:1.6em;width:45%;-webkit-transform:translateZ(0);transform:translateZ(0)}.cd-timeline__content.svelte-1t2uafz::before{top:24px;left:100%;border-color:transparent;border-left-color:white}.cd-timeline__date.svelte-1t2uafz{position:absolute;width:100%;left:122%;top:6px;font-size:1.6rem}.cd-timeline__block.svelte-1t2uafz:nth-child(even) .cd-timeline__content.svelte-1t2uafz{float:right}.cd-timeline__block.svelte-1t2uafz:nth-child(even) .cd-timeline__content.svelte-1t2uafz::before{top:24px;left:auto;right:100%;border-color:transparent;border-right-color:white}.cd-timeline__block.svelte-1t2uafz:nth-child(even) .cd-timeline__date.svelte-1t2uafz{left:auto;right:122%;text-align:right}}@-webkit-keyframes svelte-1t2uafz-cd-bounce-2{0%{opacity:0;-webkit-transform:translateX(-100px);transform:translateX(-100px)}60%{opacity:1;-webkit-transform:translateX(20px);transform:translateX(20px)}100%{-webkit-transform:translateX(0);transform:translateX(0)}}@keyframes svelte-1t2uafz-cd-bounce-2{0%{opacity:0;-webkit-transform:translateX(-100px);transform:translateX(-100px)}60%{opacity:1;-webkit-transform:translateX(20px);transform:translateX(20px)}100%{-webkit-transform:translateX(0);transform:translateX(0)}}@-webkit-keyframes svelte-1t2uafz-cd-bounce-2-inverse{0%{opacity:0;-webkit-transform:translateX(100px);transform:translateX(100px)}60%{opacity:1;-webkit-transform:translateX(-20px);transform:translateX(-20px)}100%{-webkit-transform:translateX(0);transform:translateX(0)}}@keyframes svelte-1t2uafz-cd-bounce-2-inverse{0%{opacity:0;-webkit-transform:translateX(100px);transform:translateX(100px)}60%{opacity:1;-webkit-transform:translateX(-20px);transform:translateX(-20px)}100%{-webkit-transform:translateX(0);transform:translateX(0)}}
    </style>
<section class="cd-timeline js-cd-timeline svelte-1t2uafz">
    <div class="cd-timeline__container svelte-1t2uafz">
        <div class="cd-timeline__block js-cd-block svelte-1t2uafz">
            <div class="cd-timeline__img cd-timeline__img--movie js-cd-img svelte-1t2uafz">
            </div>
            <div class="cd-timeline__content js-cd-content svelte-1t2uafz">
                <h2 class="svelte-1t2uafz">前压步</h2>
                <p>依旧是不敢压步的一天，左侧动作比较利索，但还需要借助一点外力，右侧蹬冰都不利索。</p>
<p>教练：你自己压两步看看呢</p>
<p>我：不敢 QAQ</p>

                <span class="cd-timeline__date svelte-1t2uafz">2024-11-30</span>
            </div>
        </div><div class="cd-timeline__block js-cd-block svelte-1t2uafz">
            <div class="cd-timeline__img cd-timeline__img--movie js-cd-img svelte-1t2uafz">
            </div>
            <div class="cd-timeline__content js-cd-content svelte-1t2uafz">
                <h2 class="svelte-1t2uafz">前压步、前摇滚步</h2>
                <p>压步熟练了一些，但是依旧得靠手保持平衡。
学了前摇滚，明明看起来是一种很和谐不别扭的步法，自己滑就不是这么回事了。</p>

                <span class="cd-timeline__date svelte-1t2uafz">2024-12-14</span>
            </div>
        </div><div class="cd-timeline__block js-cd-block svelte-1t2uafz">
            <div class="cd-timeline__img cd-timeline__img--movie js-cd-img svelte-1t2uafz">
            </div>
            <div class="cd-timeline__content js-cd-content svelte-1t2uafz">
                <h2 class="svelte-1t2uafz">前压步</h2>
                <p>压步似乎突然就悟了，已经没什么恐惧了，逆时针可以比较顺利，顺时针压的比较少，还有待观察。
另外，被教练带着用超快的速度蹬冰滑了几圈，感受到了起飞的感觉，非常快乐。</p>

                <span class="cd-timeline__date svelte-1t2uafz">2024-12-21</span>
            </div>
        </div><div class="cd-timeline__block js-cd-block svelte-1t2uafz">
            <div class="cd-timeline__img cd-timeline__img--movie js-cd-img svelte-1t2uafz">
            </div>
            <div class="cd-timeline__content js-cd-content svelte-1t2uafz">
                <h2 class="svelte-1t2uafz">前压步</h2>
                <p>顺时针压步果然还有大问题 TAT 也算达成两面压步都摔过成就了，希望顺时针也可以像逆时针一样突然开窍。
被教练说上身完全没有控制，果然啥运动都得练核心 QAQ</p>

                <span class="cd-timeline__date svelte-1t2uafz">2024-12-28</span>
            </div>
        </div>
    </div>
</section>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <categories>
        <category>生活琐事</category>
        <category>神奇技能</category>
      </categories>
      <tags>
        <tag>技能学习</tag>
      </tags>
  </entry>
  <entry>
    <title>解数独</title>
    <url>/posts/de1f6638/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h4 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h4><p>编写一个程序，通过填充空格来解决数独问题。</p>
<hr>
<h4 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h4><p>简单的深搜会超时，为了防止超时，需要尽可能优先搜索可能性较少的位置。</p>
<hr>
<h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">solveSudoku</span>(<span class="params">self, board: <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">str</span>]]</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        U = <span class="built_in">set</span>([<span class="built_in">str</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">10</span>)])</span><br><span class="line">        row,col,cell = defaultdict(<span class="built_in">set</span>),defaultdict(<span class="built_in">set</span>),defaultdict(<span class="built_in">set</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算每个位置的可能性</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">get</span>(<span class="params">i, j</span>):</span><br><span class="line">            <span class="keyword">return</span> (U - row[i]) &amp; (U - col[j]) &amp; (U - cell[i//<span class="number">3</span>, j//<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">cur</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> cur:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">            </span><br><span class="line">            minv, x, y = <span class="number">10</span>, <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">            <span class="comment"># 找到可能数值最少的位置</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">9</span>): </span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">9</span>):</span><br><span class="line">                    <span class="keyword">if</span> board[i][j] == <span class="string">&#x27;.&#x27;</span>:</span><br><span class="line">                        tc = <span class="built_in">len</span>(get(i, j))</span><br><span class="line">                        <span class="keyword">if</span> <span class="built_in">len</span>(get(i, j)) &lt; minv:</span><br><span class="line">                            minv, x, y = tc, i, j</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> get(x, y):</span><br><span class="line">                row[x].add(i)</span><br><span class="line">                col[y].add(i)</span><br><span class="line">                cell[x // <span class="number">3</span>, y // <span class="number">3</span>].add(i)</span><br><span class="line">                board[x][y] = i</span><br><span class="line">                </span><br><span class="line">                <span class="keyword">if</span> dfs(cur - <span class="number">1</span>):</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">                </span><br><span class="line">                row[x].remove(i)</span><br><span class="line">                col[y].remove(i)</span><br><span class="line">                cell[x // <span class="number">3</span>, y // <span class="number">3</span>].remove(i)</span><br><span class="line">                board[x][y] = <span class="string">&#x27;.&#x27;</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">            </span><br><span class="line">        <span class="comment"># 初始化</span></span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">9</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">9</span>):</span><br><span class="line">                <span class="keyword">if</span> board[i][j] != <span class="string">&#x27;.&#x27;</span>:</span><br><span class="line">                    row[i].add(board[i][j])</span><br><span class="line">                    col[j].add(board[i][j])</span><br><span class="line">                    cell[i // <span class="number">3</span>, j // <span class="number">3</span>].add(board[i][j])</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    res += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        dfs(res)</span><br></pre></td></tr></table></figure>

<hr>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <categories>
        <category>技术积累</category>
        <category>编程技巧</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>深搜</tag>
      </tags>
  </entry>
  <entry>
    <title>联愁杀</title>
    <url>/posts/b65704c0/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h4 id="故事梗概"><a href="#故事梗概" class="headerlink" title="故事梗概"></a>故事梗概</h4><p>一礼比梢绘在打开家门时被人袭击，挣扎中抓到了袭击者的学生手册，并用男子带来的哑铃反击了袭击男子。警方赶到后，在走廊遇到了公寓内的其他住户籾山庆一，该住户称听到女子惨叫后出来查看。在袭击者的学生手册上，写着连环杀人案的其他受害者名字。</p>
<p>案发后的四年里，警方确定了嫌疑人，但没有找到嫌疑人且无法确定案件动机，为了查明自己为什么被袭击，一礼比梢绘请办案刑警双侣澄树委托“恋谜会”对案件进行推理。双侣澄树向所有人公布当年嫌犯的名字——口羽公彦。</p>
<p>案犯当年，他是一名高二学生，当年二月十五日，口羽公彦从家中离开后，去向不明。八月七日到八月八日间，架谷耕次郎被害，八月十一日犯罪声明被寄给媒体；九月四日，矢头仓美乡在放学后被杀害，九月八日犯罪声明被寄给媒体；十月二日，寸八寸义文被杀害，十月六日犯罪声明被寄给媒体；十一月六日，一礼比梢绘被袭击，警方根据现场痕迹认为口羽公彦是四起案件的凶手。</p>
<div class='spoiler collapsed'>
    <div class='spoiler-title'>
        泉馆弓子的猜想
    </div>
    <div class='spoiler-content'>
        <p>泉馆弓子的切入点是凶手行凶时没有遮住面部，由此认为凶手对被害者有强烈的恨意，希望被害者知道自己是被谁杀死的。然而这与学生手册上的信息矛盾，学生手册上关于寸八寸义文写着“老头子如果秃顶怎么办”，这说明他与寸八寸义文不相识。由此弓子认为凶手杀寸八寸义文是为了掩饰真正的目标——一礼比，因为凶手袭击一礼比时没有遮住面部。<br>针对弓子的猜想，凡河平太提出反驳，认为即使只有一礼比遭到袭击，警方也不会怀疑口羽公彦，没必要用其他人做掩饰。</p>

    </div>
</div>

<div class='spoiler collapsed'>
    <div class='spoiler-title'>
        修多罗厚的猜想
    </div>
    <div class='spoiler-content'>
        <p>修多罗厚认为口羽公彦没有想杀掉特定的某个人，只是为了自己扭曲的优越感行凶，因此被害人的选择并无标准。<br>口羽公彦行凶后，籾山庆一出门查看，没有看到有人从公寓门逃走，同时阳台外侧有其他目击者确认无人从该方向出来，警方认为该目击者看漏了凶手。对此修多罗厚认为口羽公彦从一零六号房出来后迅速躲进了隔壁的一零五号房，躲开了籾山庆一的查看。<br>修多罗厚认为口羽公彦离开家里后，以舍人浩美之名躲在第一名被害人架谷耕次郎租下的与情人幽会的公寓里，舍人浩美是口羽公彦同班男生的名字，在案发当年春天因病去世。<br>针对修多罗厚的猜想，双侣澄树提出反驳，警方在案发后查看了公寓一楼的所有空房间，没有有人进入的痕迹。</p>

    </div>
</div>

<div class='spoiler collapsed'>
    <div class='spoiler-title'>
        凡河平太的猜想
    </div>
    <div class='spoiler-content'>
        <p>凡河平太认为凶手并不是口羽公彦，而是另有其人。证据是学生手册上，一礼比与其他被害人的名字写法不同，因此凡河平太认为一礼比的信息是真凶添上去的。<br>凡河平太认为真凶利用了口羽公彦使用学生手册的习惯，诱导口羽公彦在手册上写下了前几起案件的信息，并诱导口羽公彦袭击了一礼比。凡河平太提出的真凶是架谷耕次郎，警方发现的架谷耕次郎尸体是其他人伪装的，他与口羽公彦交流时使用了假的身份，杀害矢头仓美乡和寸八寸义文是为了防止被人发觉假死，被害人的选择是随机的。<br>针对凡河平太的猜想，矢集亚李沙提出了反驳，认为被害人之间是有关联的。</p>

    </div>
</div>

<div class='spoiler collapsed'>
    <div class='spoiler-title'>
        矢集亚李沙的猜想
    </div>
    <div class='spoiler-content'>
        <p>矢集亚李沙认为四名被害人被选择是因为他们都在当地报纸的读者投稿里发表过文章，口羽公彦因为自己的文章无法发表感到嫉妒所以进行了犯罪。<br>双侣澄树提出反驳，因为一礼比在报纸上公开的住址是之前的公寓，而非被袭击的公寓。</p>

    </div>
</div>

<div class='spoiler collapsed'>
    <div class='spoiler-title'>
        丁部泰典的猜想
    </div>
    <div class='spoiler-content'>
        <p>丁部泰典提出一礼比搬家与士坚亮的死亡有关。案发前一年，一礼比曾接到无声的威胁电话和可疑邮件，且曾与士坚亮提起，当时二人正在交往。某天二人去附近吃饭的路上，士坚亮因被车辆撞到去世。一礼比向警察提起，自己怀疑是威胁她的人推了士坚亮，但警察认为是酒后导致的意外。一礼比在之后搬出了原来的公寓。<br>丁部泰典认为车祸的日期与口羽公彦离家的日期一致，因此可能是口羽公彦想要杀害一礼比却误杀了士坚亮，之前发出威胁的人也是口羽公彦。<br>丁部泰典认为动机与一礼比曾经的投稿相关，口羽公彦认为投稿中提到的人是他，且这篇投稿导致了他与女生的对立。<br>泉馆弓子对此提出质疑，认为从时间看，如果口羽公彦计划杀害一礼比，应该等在家门口等一礼比到家就进行袭击。一礼比也提出那篇文章是她自己编造的，没有原型。</p>

    </div>
</div>

<div class='spoiler collapsed'>
    <div class='spoiler-title'>
        新的猜想
    </div>
    <div class='spoiler-content'>
        <p>泉馆弓子提出了新的疑问，认为籾山庆一与案发现场之间的距离不足以让他听到惨叫。修多罗厚借此提出猜想，认为籾山庆一才是真正的凶手，口羽公彦离家后一致与籾山庆一同居。口羽公彦袭击一礼比后，籾山庆一在自己的房间里藏匿了口羽公彦。<br>双侣澄树提出反驳，因为案发当天警方就搜查了籾山庆一家，并在籾山庆一家中发现了女性生理用品。</p>

    </div>
</div>

<div class='spoiler collapsed'>
    <div class='spoiler-title'>
        真相
    </div>
    <div class='spoiler-content'>
        <p>一礼比被袭击并非发生在十一月六日，而是发生在二月十五日，士坚亮去世当晚。一礼比被袭击后反击过程中失手杀死了口羽公彦，为了处理遗体向士坚亮寻求帮助，然而士坚亮主张报警，于是一礼比把他推倒了路上。为了弄清口羽公彦袭击自己的目的，一礼比联系了从学生手册上看到的架谷耕次郎，并用同样从手册上看到的名字舍人浩美与架谷耕次郎进行接触。架谷耕次郎看穿了一礼比的处境，并以此威胁一礼比与自己同居。一礼比决定根据手册上的信息实行犯罪，希望警察可以借此查清动机。<br>口羽公彦的动机是舍人浩美在报纸上发表的文章，口羽公彦认为舍人浩美是因为生病占了便宜，感到不公，并因此观点与同学发生矛盾，口羽公彦本打算把其他人作为杀舍人浩美的伪装。</p>

    </div>
</div>

<hr>
<h4 id="个人评价"><a href="#个人评价" class="headerlink" title="个人评价"></a>个人评价</h4><p>这本书整体靠每个人轮流提出推理并互相反驳推进，因此推理占了非常大的比例。同时，最后的反转比较出乎意料，虽然有癫癫的部分，但如果之前没想到还是比较震撼的。看完结局再回头看前文，也会发现作者埋了很多伏笔和暗示。</p>
<p>个人不太喜欢的点是中间在不断抛出线索，甚至互相之间的反驳有时也是根据新提出的线索进行的。</p>
<hr>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <categories>
        <category>休闲娱乐</category>
        <category>书籍阅读</category>
      </categories>
      <tags>
        <tag>推理小说</tag>
        <tag>叙述性诡计</tag>
        <tag>西泽保彦</tag>
        <tag>多重解答</tag>
      </tags>
  </entry>
  <entry>
    <title>搜索旋转排序数组</title>
    <url>/posts/32377bdc/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h4 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h4><p>整数数组 nums 按升序排列，数组中的值互不相同。<br>在传递给函数之前，nums 在预先未知的某个下标 k（0 &lt;= k &lt; nums.length）上进行了旋转，使数组变为 [nums[k], nums[k+1], …, nums[n-1], nums[0], nums[1], …, nums[k-1]]（下标从 0 开始计数）。例如，[0,1,2,4,5,6,7] 在下标 3 处经旋转后可能变为 [4,5,6,7,0,1,2]。<br>给你旋转后的数组 nums 和一个整数 target，如果 nums 中存在这个目标值 target，则返回它的下标，否则返回 -1。<br>你必须设计一个时间复杂度为 O(log n) 的算法解决此问题。</p>
<hr>
<h4 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h4><p>二分法的变形，与普通二分法的区别是需要加入更复杂的判断条件。对于旋转数组，从中间分开后，有两种可能：</p>
<ul>
<li>旋转点在前半段，这种情况后半段数组是递增的，只要判断 target 不在后半段就可以到前半段搜索；</li>
<li>旋转点在后半段，则与上述情况相反。</li>
</ul>
<p>判断旋转点只需要判断左侧元素是否小于右侧即可。</p>
<hr>
<h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">search</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], target: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        length = <span class="built_in">len</span>(nums)</span><br><span class="line">        <span class="keyword">if</span> length == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span></span><br><span class="line">            </span><br><span class="line">        mid = <span class="built_in">int</span>((length - <span class="number">1</span>) / <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">if</span> target == nums[mid]:</span><br><span class="line">            <span class="keyword">return</span> mid</span><br><span class="line">        <span class="keyword">if</span> target == nums[-<span class="number">1</span>]:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">len</span>(nums) - <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> target == nums[<span class="number">0</span>]:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> (</span><br><span class="line">            nums[<span class="number">0</span>] &lt; nums[mid] <span class="keyword">and</span> \</span><br><span class="line">            (target &gt; nums[mid] <span class="keyword">or</span> target &lt; nums[<span class="number">0</span>])) <span class="keyword">or</span> \</span><br><span class="line">            (nums[<span class="number">0</span>] &gt; nums[mid]) <span class="keyword">and</span> \</span><br><span class="line">            (target &gt; nums[mid] <span class="keyword">and</span> target &lt; nums[-<span class="number">1</span>]):</span><br><span class="line">            </span><br><span class="line">            result = <span class="variable language_">self</span>.search(nums[mid+<span class="number">1</span>:], target)</span><br><span class="line">            <span class="keyword">if</span> result == -<span class="number">1</span>:</span><br><span class="line">                <span class="keyword">return</span> result</span><br><span class="line">            <span class="keyword">return</span> result + mid + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="variable language_">self</span>.search(nums[:mid], target)</span><br></pre></td></tr></table></figure>

<hr>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <categories>
        <category>技术积累</category>
        <category>编程技巧</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>分治</tag>
      </tags>
  </entry>
  <entry>
    <title>串联所有单词的子串</title>
    <url>/posts/44658eea/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h4 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h4><p>给定一个字符串 s 和一个字符串数组 words。words 中所有字符串长度相同。<br>s 中的串联子串是指一个包含 words 中所有字符串以任意顺序排列连接起来的子串。<br>返回所有串联子串在 s 中的开始索引。你可以以任意顺序返回答案。</p>
<hr>
<h4 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h4><p>利用所有字符串等长的特点，逐位置搜索。</p>
<hr>
<h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">findSubstring</span>(<span class="params">self, s: <span class="built_in">str</span>, words: <span class="type">List</span>[<span class="built_in">str</span>]</span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        <span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> s <span class="keyword">or</span> <span class="keyword">not</span> words:</span><br><span class="line">            <span class="keyword">return</span> []</span><br><span class="line">        </span><br><span class="line">        one_word = <span class="built_in">len</span>(words[<span class="number">0</span>])</span><br><span class="line">        all_len = <span class="built_in">len</span>(words) * one_word</span><br><span class="line">        n = <span class="built_in">len</span>(s)</span><br><span class="line">        </span><br><span class="line">        words = Counter(words)</span><br><span class="line">        res = []</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, n - all_len + <span class="number">1</span>):</span><br><span class="line">            tmp = s[i:i+all_len]</span><br><span class="line">            c_tmp = []</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, all_len, one_word):</span><br><span class="line">                c_tmp.append(tmp[j:j+one_word])</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> Counter(c_tmp) == words:</span><br><span class="line">                res.append(i)</span><br><span class="line"><span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>

<hr>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <categories>
        <category>技术积累</category>
        <category>编程技巧</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>Build a LLM From Scratch</title>
    <url>/posts/881244c3/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h4 id="Understanding-LLM"><a href="#Understanding-LLM" class="headerlink" title="Understanding LLM"></a>Understanding LLM</h4><h5 id="LLM-与传统方法相比的优势"><a href="#LLM-与传统方法相比的优势" class="headerlink" title="LLM 与传统方法相比的优势"></a>LLM 与传统方法相比的优势</h5><p>之前的方法在比较复杂的理解和生成任务中表现不佳，例如：上下文分析、生成连贯文本，同时只适用于一些特定的任务。而 LLM 可以在一系列任务上表现出较好的性能。LLM 成功的原因包括 transformer 结构以及大量的训练数据。</p>
<h5 id="Stages-of-building-and-using-LLMS"><a href="#Stages-of-building-and-using-LLMS" class="headerlink" title="Stages of building and using LLMS"></a>Stages of building and using LLMS</h5><p>目前的研究表明，模型在特定任务或特定领域上训练后可以表现出超过通用 LLM 的性能。因此 LLM 训练通常包括两步：</p>
<ul>
<li>Pretrain 指模型在大规模数据库上训练，使模型具有语言理解能力，这一阶段的数据是不需要标注的；</li>
<li>Finetune 指模型在细分领域/细分任务的数据上训练，目前比较常用的 finetune 方式包括两类：<ul>
<li>Instruction finetuning：标注数据由 instruction 和 answer 组成，例如：要求模型翻译某段话的 query 以及正确的翻译后的文本；</li>
<li>Classification finetuning：标注数据由文本以及对应的类别组成，例如：邮件内容以及该邮件是否是垃圾邮件的标注。<br><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-11-26%2014.09.22.png" alt="截屏2024-11-26 14.09.22.png"></li>
</ul>
</li>
</ul>
<hr>
<h4 id="Working-with-text-data"><a href="#Working-with-text-data" class="headerlink" title="Working with text data"></a>Working with text data</h4><h5 id="Understanding-word-embeddings"><a href="#Understanding-word-embeddings" class="headerlink" title="Understanding word embeddings"></a>Understanding word embeddings</h5><p>Text 需要被转换成 vector 才能被模型处理，这个过程被称为 embedding。Embedding 有很多方法：</p>
<ul>
<li>Word2vec：核心思路是在相近的上下文中出现的词应该具有相似的含义；</li>
<li>作为模型的一部分在训练期间不断更新：这种方法的优势是训练出的 embedding 与自己的任务和数据更适配。</li>
</ul>
<h5 id="Tokenizing-text"><a href="#Tokenizing-text" class="headerlink" title="Tokenizing text"></a>Tokenizing text</h5><p>每个单词，每个标点符号都作为一个单独的 token。空格是否保留可以根据需要，如果是对格式非常严格的任务，例如代码生成，那么缩进和空格就有保留的必要，对于对格式要求不严格的任务，也可以删掉以节省存储空间。<br><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-11-26%2015.49.54.png" alt="截屏2024-11-26 15.49.54.png"></p>
<h5 id="Converting-tokens-into-token-IDs"><a href="#Converting-tokens-into-token-IDs" class="headerlink" title="Converting tokens into token IDs"></a>Converting tokens into token IDs</h5><p>给文本里所有出现过的 token 一个编码，这样每次读到一段 text，就可以通过转换把 text 转换成编码。LLM 的输出结果可以通过反向转换，转换到 text。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleTokenizerV1</span>: </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab</span>): </span><br><span class="line">        <span class="variable language_">self</span>.str_to_int = vocab </span><br><span class="line">        <span class="variable language_">self</span>.int_to_str = &#123;i:s <span class="keyword">for</span> s,i <span class="keyword">in</span> vocab.items()&#125; </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, text</span>): </span><br><span class="line">        preprocessed = re.split(<span class="string">r&#x27;([,.?_!&quot;()\&#x27;]|--|\s)&#x27;</span>, text)</span><br><span class="line">        preprocessed = [ </span><br><span class="line">            item.strip() <span class="keyword">for</span> item <span class="keyword">in</span> preprocessed <span class="keyword">if</span> item.strip() </span><br><span class="line">        ] </span><br><span class="line">        ids = [<span class="variable language_">self</span>.str_to_int[s] <span class="keyword">for</span> s <span class="keyword">in</span> preprocessed] </span><br><span class="line">        <span class="keyword">return</span> ids </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode</span>(<span class="params">self, ids</span>): </span><br><span class="line">        text = <span class="string">&quot; &quot;</span>.join([<span class="variable language_">self</span>.int_to_str[i] <span class="keyword">for</span> i <span class="keyword">in</span> ids])</span><br><span class="line">        <span class="comment"># 处理标点符号前的空格 </span></span><br><span class="line">        text = re.sub(<span class="string">r&#x27;\s+([,.?!&quot;()\&#x27;])&#x27;</span>, <span class="string">r&#x27;\1&#x27;</span>, text) </span><br><span class="line">        <span class="keyword">return</span> text</span><br></pre></td></tr></table></figure>

<h5 id="Adding-special-context-tokens"><a href="#Adding-special-context-tokens" class="headerlink" title="Adding special context tokens"></a>Adding special context tokens</h5><p>为了应对 text 里可能包含的没见过的词，需要增加 $&lt;|unk|&gt;$ token。同时也可以增加一些用于帮助文本理解的 token，例如用 $&lt;|endoftext|&gt;$ 表明文档的末尾，这样在用大量文本训练时，可以使模型更好地判断哪些文本是有关联的。</p>
<h5 id="Byte-pair-encoding"><a href="#Byte-pair-encoding" class="headerlink" title="Byte pair encoding"></a>Byte pair encoding</h5><p>在 tiktoken 库（ <a href="https://github.com/openai/tiktoken">https://github.com/openai/tiktoken</a> ）里有实现，可用 pip 安装。BPE 遇到没见过的单词会把这些词拆解成 subword 后再进行 encode，这样就不需要 $&lt;|unk|&gt;$ token 进行处理。BPE 的基本原理是首先把每个字母加入 vocabulary，然后把同时出现的概率较高的字母组合为 subword，逐步生成 word。<br><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-11-27%2010.51.33.png" alt="截屏2024-11-27 10.51.33.png"></p>
<h5 id="Data-sampling-with-a-sliding-window"><a href="#Data-sampling-with-a-sliding-window" class="headerlink" title="Data sampling with a sliding window"></a>Data sampling with a sliding window</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader </span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GPTDatasetV1</span>(<span class="title class_ inherited__">Dataset</span>): </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, txt, tokenizer, max_length, stride</span>):</span><br><span class="line">        <span class="variable language_">self</span>.input_ids = [] </span><br><span class="line">        <span class="variable language_">self</span>.target_ids = [] </span><br><span class="line">        </span><br><span class="line">        token_ids = tokenizer.encode(txt)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(token_ids) - max_length, stride):</span><br><span class="line">            input_chunk = token_ids[i:i + max_length] </span><br><span class="line">            target_chunk = token_ids[i + <span class="number">1</span>: i + max_length + <span class="number">1</span>]</span><br><span class="line">            <span class="variable language_">self</span>.input_ids.append(torch.tensor(input_chunk))</span><br><span class="line">            <span class="variable language_">self</span>.target_ids.append(torch.tensor(target_chunk))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>): </span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.input_ids) </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>): </span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.input_ids[idx], <span class="variable language_">self</span>.target_ids[idx]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_dataloader_v1</span>(<span class="params">txt, batch_size=<span class="number">4</span>, max_length=<span class="number">256</span>, </span></span><br><span class="line"><span class="params">                         stride=<span class="number">128</span>, shuffle=<span class="literal">True</span>, drop_last=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">                         num_workers=<span class="number">0</span></span>): </span><br><span class="line">    tokenizer = tiktoken.get_encoding(<span class="string">&quot;gpt2&quot;</span>) </span><br><span class="line">    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)</span><br><span class="line">    dataloader = DataLoader( </span><br><span class="line">        dataset, </span><br><span class="line">        batch_size=batch_size, </span><br><span class="line">        shuffle=shuffle, </span><br><span class="line">        drop_last=drop_last, </span><br><span class="line">        num_workers=num_workers </span><br><span class="line">    ) </span><br><span class="line">    <span class="keyword">return</span> dataloader</span><br></pre></td></tr></table></figure>

<h5 id="Creating-token-embeddings"><a href="#Creating-token-embeddings" class="headerlink" title="Creating token embeddings"></a>Creating token embeddings</h5><p>用 $torch.nn.Embedding$ 将 index 映射为可训练的 embedding。</p>
<h5 id="Embedding-word-positions"><a href="#Embedding-word-positions" class="headerlink" title="Embedding word positions"></a>Embedding word positions</h5><p>Position embedding 主要分为两类：</p>
<ul>
<li>Absolute position embedding：根据 token 在 sequence 里的绝对位置 embedding；</li>
<li>Relative position embedding：学习 tokens 之间的相对距离，可以使模型在不同长度的 sequence 上泛化地更好。</li>
</ul>
<hr>
<h4 id="Coding-attention-mechanisms"><a href="#Coding-attention-mechanisms" class="headerlink" title="Coding attention mechanisms"></a>Coding attention mechanisms</h4><h5 id="The-problem-with-modeling-long-sequences"><a href="#The-problem-with-modeling-long-sequences" class="headerlink" title="The problem with modeling long sequences"></a>The problem with modeling long sequences</h5><p>在采用 attention 结构前，比较常用的语言处理模型是 RNN 结构。在 RNN 结构中，encoder 把输入文本的全部内容保存在一个 hidden state 内部，然后 decoder 用这个 hidden state 生成输出。<br><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-11-27%2015.26.48.png" alt="截屏2024-11-27 15.26.48.png"><br>这种结构的问题是在 decoder 过程中，encoder 早期的 hidden state 被全部丢弃了，仅采用了最后一个 hidden state，这会导致信息丢失。</p>
<h5 id="Implementing-self-attention-with-trainable-weights"><a href="#Implementing-self-attention-with-trainable-weights" class="headerlink" title="Implementing self-attention with trainable weights"></a>Implementing self-attention with trainable weights</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SelfAttention_v2</span>(nn.Module): </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_in, d_out, qkv_bias=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__() </span><br><span class="line">        <span class="variable language_">self</span>.W_query = nn.Linear(d_in, d_out, bias=qkv_bias) </span><br><span class="line">        <span class="variable language_">self</span>.W_key = nn.Linear(d_in, d_out, bias=qkv_bias) </span><br><span class="line">        <span class="variable language_">self</span>.W_value = nn.Linear(d_in, d_out, bias=qkv_bias) </span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>): </span><br><span class="line">        keys = <span class="variable language_">self</span>.W_key(x) </span><br><span class="line">        queries = <span class="variable language_">self</span>.W_query(x) </span><br><span class="line">        values = <span class="variable language_">self</span>.W_value(x) </span><br><span class="line">        attn_scores = queries @ keys.T </span><br><span class="line">        attn_weights = torch.softmax( </span><br><span class="line">            attn_scores / keys.shape[-<span class="number">1</span>]**<span class="number">0.5</span>, dim=-<span class="number">1</span> </span><br><span class="line">        ) </span><br><span class="line">        context_vec = attn_weights @ values </span><br><span class="line">        <span class="keyword">return</span> context_vec</span><br></pre></td></tr></table></figure>

<p>Normalize 是为了防止点积结果过大，从而落在梯度较低的区域，影响训练速度。</p>
<h5 id="Hiding-future-words-with-causal-attention"><a href="#Hiding-future-words-with-causal-attention" class="headerlink" title="Hiding future words with causal attention"></a>Hiding future words with causal attention</h5><p>自然语言任务里，经常需要仅与前面的输入做 attention。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CausalAttention</span>(nn.Module): </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_in, d_out, context_length, </span></span><br><span class="line"><span class="params">                dropout, qkv_bias=<span class="literal">False</span></span>): </span><br><span class="line">        <span class="built_in">super</span>().__init__() </span><br><span class="line">        <span class="variable language_">self</span>.d_out = d_out </span><br><span class="line">        <span class="variable language_">self</span>.W_query = nn.Linear(d_in, d_out, bias=qkv_bias) </span><br><span class="line">        <span class="variable language_">self</span>.W_key = nn.Linear(d_in, d_out, bias=qkv_bias) </span><br><span class="line">        <span class="variable language_">self</span>.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout) </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># register_buffer可以使变量随着模型在不同device上移动，不需要手动处理</span></span><br><span class="line">        <span class="variable language_">self</span>.register_buffer( </span><br><span class="line">            <span class="string">&#x27;mask&#x27;</span>, </span><br><span class="line">            torch.triu(torch.ones(context_length, context_length),</span><br><span class="line">            diagonal=<span class="number">1</span>) </span><br><span class="line">        ) </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>): </span><br><span class="line">        b, num_tokens, d_in = x.shape </span><br><span class="line">        keys = <span class="variable language_">self</span>.W_key(x) </span><br><span class="line">        queries = <span class="variable language_">self</span>.W_query(x) </span><br><span class="line">        values = <span class="variable language_">self</span>.W_value(x) </span><br><span class="line">        </span><br><span class="line">        attn_scores = queries @ keys.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 加入mask防止与后面的token计算attention</span></span><br><span class="line">        attn_scores.masked_fill_( </span><br><span class="line">            <span class="variable language_">self</span>.mask.<span class="built_in">bool</span>()[:num_tokens, :num_tokens], -torch.inf</span><br><span class="line">        ) </span><br><span class="line">        attn_weights = torch.softmax( </span><br><span class="line">            attn_scores / keys.shape[-<span class="number">1</span>]**<span class="number">0.5</span>, dim=-<span class="number">1</span> </span><br><span class="line">        ) </span><br><span class="line">        <span class="comment"># 加入dropout防止过拟合</span></span><br><span class="line">        attn_weights = <span class="variable language_">self</span>.dropout(attn_weights)</span><br><span class="line">        </span><br><span class="line">        context_vec = attn_weights @ values </span><br><span class="line">        <span class="keyword">return</span> context_vec</span><br></pre></td></tr></table></figure>

<h5 id="Extending-single-head-attention-to-multi-head-attention"><a href="#Extending-single-head-attention-to-multi-head-attention" class="headerlink" title="Extending single-head attention to multi-head attention"></a>Extending single-head attention to multi-head attention</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module): </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_in, d_out, </span></span><br><span class="line"><span class="params">                 context_length, dropout, num_heads, qkv_bias=<span class="literal">False</span></span>): </span><br><span class="line">        <span class="built_in">super</span>().__init__() </span><br><span class="line">        <span class="keyword">assert</span> (d_out % num_heads == <span class="number">0</span>), \ </span><br><span class="line">            <span class="string">&quot;d_out must be divisible by num_heads&quot;</span> </span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.d_out = d_out </span><br><span class="line">        <span class="variable language_">self</span>.num_heads = num_heads </span><br><span class="line">        <span class="variable language_">self</span>.head_dim = d_out // num_heads </span><br><span class="line">        <span class="variable language_">self</span>.W_query = nn.Linear(d_in, d_out, bias=qkv_bias) </span><br><span class="line">        <span class="variable language_">self</span>.W_key = nn.Linear(d_in, d_out, bias=qkv_bias) </span><br><span class="line">        <span class="variable language_">self</span>.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)</span><br><span class="line">        <span class="variable language_">self</span>.out_proj = nn.Linear(d_out, d_out) </span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout) </span><br><span class="line">        <span class="variable language_">self</span>.register_buffer( </span><br><span class="line">            <span class="string">&quot;mask&quot;</span>, </span><br><span class="line">            torch.triu(torch.ones(context_length, context_length),</span><br><span class="line">                       diagonal=<span class="number">1</span>) </span><br><span class="line">        ) </span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>): </span><br><span class="line">        b, num_tokens, d_in = x.shape </span><br><span class="line">        keys = <span class="variable language_">self</span>.W_key(x) </span><br><span class="line">        queries = <span class="variable language_">self</span>.W_query(x) </span><br><span class="line">        values = <span class="variable language_">self</span>.W_value(x)</span><br><span class="line">        keys = keys.view(b, num_tokens, <span class="variable language_">self</span>.num_heads, <span class="variable language_">self</span>.head_dim)</span><br><span class="line">        values = values.view(</span><br><span class="line">            b, num_tokens, <span class="variable language_">self</span>.num_heads, <span class="variable language_">self</span>.head_dim</span><br><span class="line">        ) </span><br><span class="line">        queries = queries.view( </span><br><span class="line">            b, num_tokens, <span class="variable language_">self</span>.num_heads, <span class="variable language_">self</span>.head_dim </span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        keys = keys.transpose(<span class="number">1</span>, <span class="number">2</span>) </span><br><span class="line">        queries = queries.transpose(<span class="number">1</span>, <span class="number">2</span>) </span><br><span class="line">        values = values.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        attn_scores = queries @ keys.transpose(<span class="number">2</span>, <span class="number">3</span>) </span><br><span class="line">        mask_bool = <span class="variable language_">self</span>.mask.<span class="built_in">bool</span>()[:num_tokens, :num_tokens]</span><br><span class="line">        </span><br><span class="line">        attn_scores.masked_fill_(mask_bool, -torch.inf)</span><br><span class="line">        </span><br><span class="line">        attn_weights = torch.softmax( </span><br><span class="line">            attn_scores / keys.shape[-<span class="number">1</span>]**<span class="number">0.5</span>, dim=-<span class="number">1</span>) </span><br><span class="line">        attn_weights = <span class="variable language_">self</span>.dropout(attn_weights)</span><br><span class="line">        </span><br><span class="line">        context_vec = (attn_weights @ values).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        context_vec = context_vec.contiguous().view( </span><br><span class="line">            b, num_tokens, <span class="variable language_">self</span>.d_out </span><br><span class="line">        ) </span><br><span class="line">        context_vec = <span class="variable language_">self</span>.out_proj(context_vec) </span><br><span class="line">        <span class="keyword">return</span> context_vec</span><br></pre></td></tr></table></figure>

<hr>
<h4 id="Implementing-a-GPT-model-from-scratch-to-generate-text"><a href="#Implementing-a-GPT-model-from-scratch-to-generate-text" class="headerlink" title="Implementing a GPT model from scratch to generate text"></a>Implementing a GPT model from scratch to generate text</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerBlock</span>(nn.Module): </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, cfg</span>): </span><br><span class="line">        <span class="built_in">super</span>().__init__() </span><br><span class="line">        <span class="variable language_">self</span>.att = MultiHeadAttention( </span><br><span class="line">            d_in=cfg[<span class="string">&quot;emb_dim&quot;</span>], </span><br><span class="line">            d_out=cfg[<span class="string">&quot;emb_dim&quot;</span>], </span><br><span class="line">            context_length=cfg[<span class="string">&quot;context_length&quot;</span>],</span><br><span class="line">            num_heads=cfg[<span class="string">&quot;n_heads&quot;</span>], </span><br><span class="line">            dropout=cfg[<span class="string">&quot;drop_rate&quot;</span>], </span><br><span class="line">            qkv_bias=cfg[<span class="string">&quot;qkv_bias&quot;</span>]) </span><br><span class="line">        <span class="variable language_">self</span>.ff = FeedForward(cfg) </span><br><span class="line">        <span class="variable language_">self</span>.norm1 = LayerNorm(cfg[<span class="string">&quot;emb_dim&quot;</span>]) </span><br><span class="line">        <span class="variable language_">self</span>.norm2 = LayerNorm(cfg[<span class="string">&quot;emb_dim&quot;</span>]) </span><br><span class="line">        <span class="variable language_">self</span>.drop_shortcut = nn.Dropout(cfg[<span class="string">&quot;drop_rate&quot;</span>]) </span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>): </span><br><span class="line">        shortcut = x </span><br><span class="line">        x = <span class="variable language_">self</span>.norm1(x) </span><br><span class="line">        x = <span class="variable language_">self</span>.att(x) </span><br><span class="line">        x = <span class="variable language_">self</span>.drop_shortcut(x) </span><br><span class="line">        x = x + shortcut </span><br><span class="line">        </span><br><span class="line">        shortcut = x </span><br><span class="line">        x = <span class="variable language_">self</span>.norm2(x) </span><br><span class="line">        x = <span class="variable language_">self</span>.ff(x) </span><br><span class="line">        x = <span class="variable language_">self</span>.drop_shortcut(x) </span><br><span class="line">        x = x + shortcut </span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GPTModel</span>(nn.Module): </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, cfg</span>): </span><br><span class="line">        <span class="built_in">super</span>().__init__() </span><br><span class="line">        <span class="variable language_">self</span>.tok_emb = nn.Embedding(cfg[<span class="string">&quot;vocab_size&quot;</span>], cfg[<span class="string">&quot;emb_dim&quot;</span>])</span><br><span class="line">        <span class="variable language_">self</span>.pos_emb = nn.Embedding(</span><br><span class="line">            cfg[<span class="string">&quot;context_length&quot;</span>], cfg[<span class="string">&quot;emb_dim&quot;</span>]</span><br><span class="line">        ) </span><br><span class="line">        <span class="variable language_">self</span>.drop_emb = nn.Dropout(cfg[<span class="string">&quot;drop_rate&quot;</span>]) </span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.trf_blocks = nn.Sequential( </span><br><span class="line">            *[TransformerBlock(cfg) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(cfg[<span class="string">&quot;n_layers&quot;</span>])])</span><br><span class="line">            </span><br><span class="line">        <span class="variable language_">self</span>.final_norm = LayerNorm(cfg[<span class="string">&quot;emb_dim&quot;</span>]) </span><br><span class="line">        <span class="variable language_">self</span>.out_head = nn.Linear( </span><br><span class="line">            cfg[<span class="string">&quot;emb_dim&quot;</span>], cfg[<span class="string">&quot;vocab_size&quot;</span>], bias=<span class="literal">False</span> </span><br><span class="line">        ) </span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, in_idx</span>): </span><br><span class="line">        batch_size, seq_len = in_idx.shape </span><br><span class="line">        tok_embeds = <span class="variable language_">self</span>.tok_emb(in_idx) </span><br><span class="line">        </span><br><span class="line">        pos_embeds = <span class="variable language_">self</span>.pos_emb( </span><br><span class="line">            torch.arange(seq_len, device=in_idx.device) </span><br><span class="line">        ) </span><br><span class="line">        x = tok_embeds + pos_embeds </span><br><span class="line">        x = <span class="variable language_">self</span>.drop_emb(x) </span><br><span class="line">        x = <span class="variable language_">self</span>.trf_blocks(x) </span><br><span class="line">        x = <span class="variable language_">self</span>.final_norm(x) </span><br><span class="line">        logits = <span class="variable language_">self</span>.out_head(x) </span><br><span class="line">        <span class="keyword">return</span> logits</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_text_simple</span>(<span class="params">model, idx, max_new_tokens, context_size</span>): </span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(max_new_tokens): </span><br><span class="line">        idx_cond = idx[:, -context_size:] </span><br><span class="line">        <span class="keyword">with</span> torch.no_grad(): </span><br><span class="line">            logits = model(idx_cond) </span><br><span class="line">        </span><br><span class="line">        logits = logits[:, -<span class="number">1</span>, :] </span><br><span class="line">        probas = torch.softmax(logits, dim=-<span class="number">1</span>) </span><br><span class="line">        idx_next = torch.argmax(probas, dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>) </span><br><span class="line">        idx = torch.cat((idx, idx_next), dim=<span class="number">1</span>) </span><br><span class="line">    <span class="keyword">return</span> idx</span><br></pre></td></tr></table></figure>

<hr>
<h4 id="Pretraining-on-unlabeled-data"><a href="#Pretraining-on-unlabeled-data" class="headerlink" title="Pretraining on unlabeled data"></a>Pretraining on unlabeled data</h4><h5 id="Evaluating-generative-text-models"><a href="#Evaluating-generative-text-models" class="headerlink" title="Evaluating generative text models"></a>Evaluating generative text models</h5><ul>
<li>采用 cross entropy loss；</li>
<li>Perplexity 也被用来评估模型，该指标用 $perplexity=torch.exp(loss)$ 计算，该指标的含义是模型认为有多少个可能且合理的备选，该指标越大，表示模型认为有更多可能的备选，模型不确定性越高。</li>
</ul>
<h5 id="Training-an-LLM"><a href="#Training-an-LLM" class="headerlink" title="Training an LLM"></a>Training an LLM</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_model_simple</span>(<span class="params">model, train_loader, val_loader, </span></span><br><span class="line"><span class="params">                       optimizer, device, num_epochs, </span></span><br><span class="line"><span class="params">                       eval_freq, eval_iter, start_context, tokenizer</span>):</span><br><span class="line">    train_losses, val_losses, track_tokens_seen = [], [], []</span><br><span class="line">    tokens_seen, global_step = <span class="number">0</span>, -<span class="number">1</span> </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs): </span><br><span class="line">        model.train() </span><br><span class="line">        <span class="keyword">for</span> input_batch, target_batch <span class="keyword">in</span> train_loader:</span><br><span class="line">            optimizer.zero_grad() </span><br><span class="line">            loss = calc_loss_batch( </span><br><span class="line">                input_batch, target_batch, model, device </span><br><span class="line">            ) </span><br><span class="line">            loss.backward() </span><br><span class="line">            optimizer.step() </span><br><span class="line">            tokens_seen += input_batch.numel() </span><br><span class="line">            global_step += <span class="number">1</span> </span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> global_step % eval_freq == <span class="number">0</span>: </span><br><span class="line">                train_loss, val_loss = evaluate_model( </span><br><span class="line">                    model, train_loader, val_loader, device, eval_iter)</span><br><span class="line">                train_losses.append(train_loss)</span><br><span class="line">                val_losses.append(val_loss)</span><br><span class="line">                track_tokens_seen.append(tokens_seen) </span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;Ep <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span> (Step <span class="subst">&#123;global_step:06d&#125;</span>): &quot;</span> </span><br><span class="line">                      <span class="string">f&quot;Train loss <span class="subst">&#123;train_loss:<span class="number">.3</span>f&#125;</span>, &quot;</span> </span><br><span class="line">                      <span class="string">f&quot;Val loss <span class="subst">&#123;val_loss:<span class="number">.3</span>f&#125;</span>&quot;</span> </span><br><span class="line">                )</span><br><span class="line">        generate_and_print_sample( </span><br><span class="line">            model, tokenizer, device, start_context </span><br><span class="line">        ) </span><br><span class="line">    <span class="keyword">return</span> train_losses, val_losses, track_tokens_seen</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calc_loss_batch</span>(<span class="params">input_batch, target_batch, model, device</span>):</span><br><span class="line">    input_batch = input_batch.to(device) </span><br><span class="line">    target_batch = target_batch.to(device) </span><br><span class="line">    logits = model(input_batch) </span><br><span class="line">    loss = torch.nn.functional.cross_entropy( </span><br><span class="line">        logits.flatten(<span class="number">0</span>, <span class="number">1</span>), target_batch.flatten() </span><br><span class="line">    ) </span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calc_loss_loader</span>(<span class="params">data_loader, model, device, num_batches=<span class="literal">None</span></span>):</span><br><span class="line">    total_loss = <span class="number">0.</span> </span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(data_loader) == <span class="number">0</span>: </span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">float</span>(<span class="string">&quot;nan&quot;</span>) </span><br><span class="line">    <span class="keyword">elif</span> num_batches <span class="keyword">is</span> <span class="literal">None</span>: </span><br><span class="line">        num_batches = <span class="built_in">len</span>(data_loader) </span><br><span class="line">    <span class="keyword">else</span>: </span><br><span class="line">        num_batches = <span class="built_in">min</span>(num_batches, <span class="built_in">len</span>(data_loader)) </span><br><span class="line">    <span class="keyword">for</span> i, (input_batch, target_batch) <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_loader): </span><br><span class="line">        <span class="keyword">if</span> i &lt; num_batches: </span><br><span class="line">            loss = calc_loss_batch( </span><br><span class="line">                input_batch, target_batch, model, device </span><br><span class="line">            ) </span><br><span class="line">            total_loss += loss.item() </span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            <span class="keyword">break</span> </span><br><span class="line">    <span class="keyword">return</span> total_loss / num_batches</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_model</span>(<span class="params">model, train_loader, val_loader, device, eval_iter</span>):</span><br><span class="line">    model.<span class="built_in">eval</span>() </span><br><span class="line">    <span class="keyword">with</span> torch.no_grad(): </span><br><span class="line">        train_loss = calc_loss_loader( </span><br><span class="line">            train_loader, model, device, num_batches=eval_iter</span><br><span class="line">        ) </span><br><span class="line">        val_loss = calc_loss_loader( </span><br><span class="line">            val_loader, model, device, num_batches=eval_iter </span><br><span class="line">        ) </span><br><span class="line">    model.train() </span><br><span class="line">    <span class="keyword">return</span> train_loss, val_loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_and_print_sample</span>(<span class="params">model, tokenizer, device, start_context</span>):</span><br><span class="line">    model.<span class="built_in">eval</span>() </span><br><span class="line">    context_size = model.pos_emb.weight.shape[<span class="number">0</span>] </span><br><span class="line">    encoded = text_to_token_ids(start_context, tokenizer).to(device)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad(): </span><br><span class="line">        token_ids = generate_text_simple( </span><br><span class="line">            model=model, idx=encoded, </span><br><span class="line">            max_new_tokens=<span class="number">50</span>, context_size=context_size </span><br><span class="line">        ) </span><br><span class="line">    decoded_text = token_ids_to_text(token_ids, tokenizer)</span><br><span class="line">    <span class="built_in">print</span>(decoded_text.replace(<span class="string">&quot;\n&quot;</span>, <span class="string">&quot; &quot;</span>)) </span><br><span class="line">    model.train()</span><br></pre></td></tr></table></figure>

<h5 id="Decoding-strategies-to-control-randomness"><a href="#Decoding-strategies-to-control-randomness" class="headerlink" title="Decoding strategies to control randomness"></a>Decoding strategies to control randomness</h5><p>之前的生成模型每次的生成结果都是一致的，为了增加多样性的同时保证合理性，可以采取两个措施：</p>
<ul>
<li>把固定选择概率最大的词作为输出，改为根据概率采样，即：$torch.argmax$ 改为 $torch.multinomial$；</li>
<li>仅在 $top\ k$ 样本中进行采样，防止采到特别离谱的结果，用 $torch.where$ 把非 $top\ k$ 的概率置为 $-\inf$，这样 $softmax$ 后这些位置的采样概率就会变成 0。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">generate</span>(<span class="params">model, idx, max_new_tokens, context_size, </span></span><br><span class="line"><span class="params">             temperature=<span class="number">0.0</span>, top_k=<span class="literal">None</span>, eos_id=<span class="literal">None</span></span>): </span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(max_new_tokens): </span><br><span class="line">        idx_cond = idx[:, -context_size:] </span><br><span class="line">        <span class="keyword">with</span> torch.no_grad(): </span><br><span class="line">            logits = model(idx_cond) </span><br><span class="line">        logits = logits[:, -<span class="number">1</span>, :]</span><br><span class="line">        <span class="keyword">if</span> top_k <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: </span><br><span class="line">            top_logits, _ = torch.topk(logits, top_k) </span><br><span class="line">            min_val = top_logits[:, -<span class="number">1</span>] </span><br><span class="line">            logits = torch.where( </span><br><span class="line">                logits &lt; min_val, </span><br><span class="line">                torch.tensor(<span class="built_in">float</span>(<span class="string">&#x27;-inf&#x27;</span>)).to(logits.device), </span><br><span class="line">                logits </span><br><span class="line">            ) </span><br><span class="line">        <span class="keyword">if</span> temperature &gt; <span class="number">0.0</span>: </span><br><span class="line">            logits = logits / temperature </span><br><span class="line">            probs = torch.softmax(logits, dim=-<span class="number">1</span>) </span><br><span class="line">            idx_next = torch.multinomial(probs, num_samples=<span class="number">1</span>) </span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            idx_next = torch.argmax(logits, dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>) </span><br><span class="line">        <span class="keyword">if</span> idx_next == eos_id: </span><br><span class="line">            <span class="keyword">break</span> </span><br><span class="line">        idx = torch.cat((idx, idx_next), dim=<span class="number">1</span>) </span><br><span class="line">    <span class="keyword">return</span> idx</span><br></pre></td></tr></table></figure>

<hr>
<h4 id="Fine-tuning-for-classification"><a href="#Fine-tuning-for-classification" class="headerlink" title="Fine-tuning for classification"></a>Fine-tuning for classification</h4><p>可以把最后的 token 预测层替换为分类 head，并且每个 sample 仅取最后一个预测结果作为分类结果（文本生成需要循环预测每个 token，但是分类其实不需要，只要有一个预测结果就可以，所以可以取最后一个）。使用交叉熵损失函数计算 loss。</p>
<hr>
<h4 id="Fine-tuning-to-follow-instructions"><a href="#Fine-tuning-to-follow-instructions" class="headerlink" title="Fine-tuning to follow instructions"></a>Fine-tuning to follow instructions</h4><p>其实可以看做文本生成预测 next token 的任务，可以 mask 掉 instruction，这样模型就会 focus 在 output 上，也可以选择保留 instruction，部分论文认为这样会更好。</p>
<hr>
<h4 id="个人评价"><a href="#个人评价" class="headerlink" title="个人评价"></a>个人评价</h4><p>整体上是一本比较好的大模型入门书，不需要有太多基础知识，只要大概熟悉 python 就能看懂。用比较简单的例子，说明了大模型的各个模块的功能以及代码。但是整体确实比较浅，如果已经有过一定经验的话，再看意义就不大。</p>
<hr>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <categories>
        <category>技术积累</category>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title>那种可能性早已料及</title>
    <url>/posts/8215b0f1/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h4 id="故事梗概"><a href="#故事梗概" class="headerlink" title="故事梗概"></a>故事梗概</h4><p>自称渡良濑莉世的委托人怀疑自己十年前杀过人，希望委托侦探上苙丞调查确认。十年前，莉世和堂仁被各自的母亲带到教团”血赎”的居住地，二人都希望从村里逃走。村里的出入口只有一个，除交易日外不常打开，悬崖的岩石很脆不能攀爬，村里材料不足无法制造梯子或高台等工具，但堂仁依旧在为逃走准备，并承诺带莉世一起。村里共三十三人，一名教祖，其余是信徒。</p>
<p>村里的猪身上都带有号码牌，按照顺序决定下次吃那一头。某天堂仁发现莉世把 12 号的号码牌藏了起来，莉世回答自己喜欢 12 号，希望它能活久一点。同时，莉世偷偷藏了一头小猪崽，被堂仁发现后，二人把猪崽藏在祭坛下面。</p>
<p>村里突发地震，地震后村里的瀑布断流，水车停止运作。教祖认为这是一种征兆，因此堵住了村里唯一的出入口，并举行了最后的晚餐。在莉世记忆中，最后的晚餐后，教祖在堂仁的服侍下举行了祓禊，祓禊结束后教祖用斧头砍杀了信徒。堂仁带着莉世从门口跑出去并反锁了门，村子着火且火势开始蔓延，等莉世再次有意识的时候，她发现自己身处祠堂，旁边是堂仁被砍下的头和尸体。</p>
<p>莉世怀疑是自己杀了少年，因为除了莉世和堂仁其他人的遗体都在参拜堂内部，且门是从外部上锁的，因此里面的人无法在无人帮助的情况下出来或自己锁上门，同时门锁很重莉世也无法锁上。但是，矛盾点在于，堂仁的头是被村里处理家畜的断头台砍下的，断头台距离祠堂有一段距离，莉世当时腿部骨折，绑着石膏，无论是移动尸体还是移动断头台都不可能。</p>
<p>调查后侦探认为这起事件是——奇迹。</p>
<h5 id="大门的推理"><a href="#大门的推理" class="headerlink" title="大门的推理"></a>大门的推理</h5><p>大门认为当时村里的水车是可以靠人力转动的那种，将猪放入水车，用火加热铁质水车就可以用猪驱动水车移动断头台。动机是堂仁要杀死小猪崽，少女精神崩溃下杀了人，为了制造奇迹，少女伪造了现场。</p>
<div class='spoiler collapsed'>
    <div class='spoiler-title'>
        侦探的反驳
    </div>
    <div class='spoiler-content'>
        <p>堂仁看到号码牌就知道编号是 12 而不是 21，说明猪的数量小于等于 20，同时 11 号及之前的都死掉了，说明最多有 9 只猪。33 名成员都吃到了猪脚，说明 9 只猪都被杀了，因此没有能用来驱动水车的猪了。</p>

    </div>
</div>

<h5 id="宋俪西的推理"><a href="#宋俪西的推理" class="headerlink" title="宋俪西的推理"></a>宋俪西的推理</h5><p>堂仁为了从村里逃走利用村里的材料做了投石机，事情发生后，堂仁想出去求援并带人回来救莉世。莉世误以为自己要被抛弃，因此趁堂仁不注意杀了堂仁，并利用投石机将自己和堂仁的尸体抛出去，正巧落入祠堂。莉世腿上的石膏没有破损是因为祠堂里的祭坛是用泡沫做的，泡沫做了缓冲。</p>
<div class='spoiler collapsed'>
    <div class='spoiler-title'>
        侦探的反驳
    </div>
    <div class='spoiler-content'>
        <p>莉世醒后先看到了太阳，然后看到了堂仁的头，背光情况下应该看不清脸。莉世能够立刻判断这是堂仁是因为最后的晚餐前莉世打扮自己的时候把镜子角度调向下了，阳光照到镜子上又被反射到堂仁的脸上，这说明祭坛没有被破坏。</p>

    </div>
</div>

<h5 id="八星联的推理"><a href="#八星联的推理" class="headerlink" title="八星联的推理"></a>八星联的推理</h5><p>村子里的神是有对应实体的——一具木乃伊。教祖在祓禊前杀了堂仁，之后看到的堂仁都是教祖伪装，看到的教祖是木乃伊，堂仁的尸体用冰箱保存，冰箱所需的电力由重力带动水车发电。教祖完成诡计后，从监控死角逃离村子，完成假死计划，设计复杂计划是需要莉世作为证人证明教祖的死亡。</p>
<div class='spoiler collapsed'>
    <div class='spoiler-title'>
        侦探的反驳
    </div>
    <div class='spoiler-content'>
        <p>藏小猪的地方仅有莉世和堂仁知道，且在藏小猪的地方发现了搬运来的食物。祓禊前堂仁无法搬出食物，因为食材库被教祖严格管理，且只有祓禊时堂仁可以趁教祖沐浴拿到钥匙。因此如果堂仁在祓禊前被杀，藏小猪的位置不会出现食物。</p>

    </div>
</div>

<div class='spoiler collapsed'>
    <div class='spoiler-title'>
        矛盾
    </div>
    <div class='spoiler-content'>
        <p>在第一段反驳里，侦探认为参加晚餐的有三十三人，也就是教祖参加了晚餐会，说明教祖还没有开始祓禊，即：祓禊在晚餐会后。<br>在第二段反驳里，侦探认为莉世进行打扮后，祭坛一直保留了打扮时的状态。这说明堂仁挪开祭坛藏食物在莉世进行打扮前。<br>也就是堂仁先放置了食物，然后莉世进行打扮参加晚餐，最后教祖进行祓禊。<br>然而根据第三段反驳，祓禊前堂仁无法拿到食物。侦探的三段反驳相互矛盾。</p>

    </div>
</div>

<div class='spoiler collapsed'>
    <div class='spoiler-title'>
        侦探的反驳
    </div>
    <div class='spoiler-content'>
        <p>矛盾成立的前提是侦探的否定必须是正确的，如果侦探的否定是正确的，八星联的假说就是错误的。因此无法从八星联的假说中假定的教祖想杀堂仁为前提进行推导。如果教祖无意杀害堂仁，则可能在祓禊前将食物交给堂仁，这样就不存在矛盾了。</p>

    </div>
</div>

<div class='spoiler collapsed'>
    <div class='spoiler-title'>
        侦探提出的可能性
    </div>
    <div class='spoiler-content'>
        <p>教祖支持堂仁逃走，因此在祓禊前给了堂仁食物，但两人的母亲不想让两人逃走。堂仁在离开参拜堂前被自己的母亲重伤，带伤抱着莉世到了祠堂，堂仁知道自己命不久矣，希望创造奇迹给莉世一个活下去的希望。因此，堂仁与教主合谋布置了现场，教主利用绳子使门闩从上方落下制造了密室。</p>

    </div>
</div>

<hr>
<h4 id="个人评价"><a href="#个人评价" class="headerlink" title="个人评价"></a>个人评价</h4><p>很有趣的侦探形象，通常侦探是负责证明奇迹不存在的，然而这本书的侦探希望否定所有可能性以证明奇迹存在。故事的发展就是不断由其他人提出可能性，侦探对这些可能一一进行否定推动。在这样的设定里，提出可能性的人不需要证明这种可能性是真实存在的，只要有可能性就算成功，所以很多解答看起来非常牵强。个人最喜欢的环节是否定之否定那里，最初看到的时候非常震撼。</p>
<p>比较明显的缺点应该是文风，文风和人物形象都过于轻小说化，个人不是很喜欢。</p>
<hr>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <categories>
        <category>休闲娱乐</category>
        <category>书籍阅读</category>
      </categories>
      <tags>
        <tag>推理小说</tag>
        <tag>多重解答</tag>
        <tag>井上真伪</tag>
        <tag>逻辑流</tag>
        <tag>轻小说风</tag>
        <tag>推荐书单</tag>
      </tags>
  </entry>
  <entry>
    <title>P-Tuning v2</title>
    <url>/posts/9feacd8a/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>题目：</strong> P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks<br><strong>链接：</strong> <a href="https://arxiv.org/pdf/2110.07602">https://arxiv.org/pdf/2110.07602</a><br><strong>代码：</strong> <a href="https://github.com/THUDM/P-tuning-v2">https://github.com/THUDM/P-tuning-v2</a><br><strong>来源：</strong> ACL 2022<br><strong>单位：</strong> Tsinghua University, KEG；Beijing Academy of Artificial Intelligence (BAAI)；Shanghai Qi Zhi Institute</p>
<hr>
<h4 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h4><p>对于大模型 finetune 所有参数很困难，类似 P-Tuning 的 prompt tuning 方法性能比 finetune 所有参数差，尤其在模型参数规模不足 100 亿时效果差的更明显。本文希望找到一种泛化性更强的 prompt tuning 方法。</p>
<hr>
<h4 id="P-Tuning-缺陷"><a href="#P-Tuning-缺陷" class="headerlink" title="P-Tuning 缺陷"></a>P-Tuning 缺陷</h4><ol>
<li>在小模型上效果不好，当模型参数量大于 100 亿时，与 finetune 效果相近；但当模型参数量不足 100 亿时，效果会变差；</li>
<li>在不同任务上通用性不足，在一些更难的任务上效果比较差。</li>
</ol>
<hr>
<h4 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h4><p><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-11-22%2019.10.51.png" alt="截屏2024-11-22 19.10.51.png"><br>与 P-Tuning 相比，v2 在每一层都加入了可学习的 prompt，增加了可学习的参数量，同时添加到深层的 prompt 可以更直接地影响模型预测结果。</p>
<hr>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <categories>
        <category>技术积累</category>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>论文笔记</tag>
        <tag>高引</tag>
        <tag>finetune</tag>
        <tag>ACL2022</tag>
      </tags>
  </entry>
  <entry>
    <title>P-Tuning</title>
    <url>/posts/8c1493e0/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>题目：</strong> GPT Understands, Too<br><strong>链接：</strong> <a href="https://arxiv.org/pdf/2103.10385">https://arxiv.org/pdf/2103.10385</a><br><strong>代码：</strong> <a href="https://github.com/THUDM/P-tuning">https://github.com/THUDM/P-tuning</a><br><strong>来源：</strong> AI Open 2024<br><strong>单位：</strong> Tsinghua University；Massachusetts Institute of Technology</p>
<hr>
<h4 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h4><p>Prompt 可以提升大语言模型性能，然而，人工提供的离散 prompt 会导致很大的不确定性，几个单词的变动就会导致结果发生大幅度变化。P-Tuning 希望在离散的 prompt 前加入一个可训练的连续 prompt，提升稳定性。</p>
<hr>
<h4 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h4><p><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-11-22%2017.31.24.png" alt="截屏2024-11-22 17.31.24.png"></p>
<hr>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <categories>
        <category>技术积累</category>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>论文笔记</tag>
        <tag>高引</tag>
        <tag>finetune</tag>
        <tag>AI_Open2024</tag>
      </tags>
  </entry>
  <entry>
    <title>LoRA</title>
    <url>/posts/3a5bfa54/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>题目：</strong> LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS<br><strong>链接：</strong> <a href="https://arxiv.org/pdf/2106.09685">https://arxiv.org/pdf/2106.09685</a><br><strong>代码：</strong> <a href="https://github.com/microsoft/LoRA">https://github.com/microsoft/LoRA</a><br><strong>来源：</strong> ICLR 2022<br><strong>单位：</strong> Microsoft Corporation</p>
<hr>
<h4 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h4><p>随着模型规模越来越大，在所有参数上 finetune 越来越不现实。之前提出的解决方案都有一定缺陷：</p>
<ul>
<li>增加 adapter layer 会引入额外的延迟，且当 batch size 减小时这种延迟变得不可忽视；</li>
<li>Optimizing prompt 难以训练，且保留一部分 prompt 会导致下游任务可用的 prompt 长度变小，从而影响模型整体性能。</li>
</ul>
<hr>
<h4 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h4><p>假设在 finetune 过程中权重矩阵的变化量是低秩的，即：$W_0+\Delta W=W_0+BA$，其中：$B\in \mathbb{R}^{d\times r}$，$A\in \mathbb{R}^{r\times k}$ 且 $r\ll \min(d, k)$。训练过程中 $W_0$ 保持不变，仅更新 $A$ 和 $B$。</p>
<hr>
<h4 id="LoRA-的优势"><a href="#LoRA-的优势" class="headerlink" title="LoRA 的优势"></a>LoRA 的优势</h4><ul>
<li>方法灵活，可以根据需要调整 $r$，当逐渐增大 $r$ 时，LoRA 会逐渐接近 finetune 所有参数；</li>
<li>当有多个下游子任务时，LoRA 可以通过更新 $BA$ 的方式在不同任务间切换，且 LoRA 相比于原始模型不会引入额外的计算延迟。</li>
</ul>
<hr>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <categories>
        <category>技术积累</category>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>论文笔记</tag>
        <tag>高引</tag>
        <tag>ICLR2022</tag>
        <tag>finetune</tag>
      </tags>
  </entry>
  <entry>
    <title>马拉车算法</title>
    <url>/posts/c3cacb08/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h4 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h4><p>用于解决最长回文子串的问题，可以把算法复杂度降低到 O(N)。</p>
<hr>
<h4 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h4><ul>
<li>在字符串的每个字符之间以及字符串首尾插入同一个字符串内不可能出现的字符，以保证所有字符串的长度都是奇数，这样可以不用对长度为奇数的字符串以及长度为偶数的字符串分类处理；</li>
<li>在字符串开头和结尾分别加入不同的且不可能在字符串内出现的字符，这样保证回文子串搜索到该位置的时候自动退出，不需要处理边界情况；</li>
<li>该算法在每个位置都希望计算以该位置为中心的最长回文子串的长度。同时会记录一个当前回文子串对应的最靠右的右边界位置 R，该位置对应的回文子串中心 c，该位置对应的回文子串 s。这样对于一个新的位置，可能出现几种情况：<ul>
<li>该位置在 R 的右边，需要重新搜索该位置的回文子串；</li>
<li>该位置在 R 的左边，此时可以看该位置以 c 为中心对称位置的回文子串情况，这也有两种情况：<ul>
<li>对称位置的回文子串完全落在 s 内，则在当前位置的回文子串长度与对称位置相同（回文子串的性质）；</li>
<li>对称位置的回文子串有一部分落在 s 外，则当前位置到 R 肯定可以构成回文子串，需要继续向外搜索。</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h4 id="算法分析"><a href="#算法分析" class="headerlink" title="算法分析"></a>算法分析</h4><p>在上述算法过程中 R 是不断向右的，也就是在算法过程中不会重复比较字符是否相等，因此可以大幅度降低时间复杂度。</p>
<hr>
<h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">longestPalindrome</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        new_s = <span class="string">&#x27;^#&#x27;</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> s:</span><br><span class="line">            new_s = new_s + i + <span class="string">&#x27;#&#x27;</span></span><br><span class="line">        new_s += <span class="string">&#x27;$&#x27;</span></span><br><span class="line">		</span><br><span class="line">        result = numpy.zeros(<span class="built_in">len</span>(new_s))</span><br><span class="line">        right = <span class="number">0</span></span><br><span class="line">        center = <span class="number">0</span></span><br><span class="line">		</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(new_s) - <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> i &gt; right:</span><br><span class="line">                r = <span class="number">0</span></span><br><span class="line">                <span class="keyword">while</span> new_s[i + r] == new_s[i - r]:</span><br><span class="line">                    r += <span class="number">1</span></span><br><span class="line">                r -= <span class="number">1</span></span><br><span class="line">                center = i</span><br><span class="line">                right = center + r</span><br><span class="line">                result[i] = r</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                other_i = <span class="number">2</span> * center - i</span><br><span class="line">                <span class="keyword">if</span> result[other_i] &lt; right - i:</span><br><span class="line">                    result[i] = result[other_i]</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    r = right - i</span><br><span class="line">                    <span class="keyword">while</span> new_s[i + r] == new_s[i - r]:</span><br><span class="line">                        r += <span class="number">1</span></span><br><span class="line">                    r -= <span class="number">1</span></span><br><span class="line">                    center = i</span><br><span class="line">                    right = center + r</span><br><span class="line">                    result[i] = r</span><br><span class="line">					</span><br><span class="line">        index = <span class="built_in">int</span>(numpy.argmax(result))</span><br><span class="line">        result_s = new_s[<span class="built_in">int</span>(index - result[index]):<span class="built_in">int</span>(index + result[index] + <span class="number">1</span>)]</span><br><span class="line">        result_s = result_s.replace(<span class="string">&#x27;#&#x27;</span>, <span class="string">&#x27;&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> result_s</span><br></pre></td></tr></table></figure>

<hr>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <categories>
        <category>技术积累</category>
        <category>编程技巧</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>特殊算法</tag>
      </tags>
  </entry>
  <entry>
    <title>PoliFormer</title>
    <url>/posts/a5f5aaf2/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>题目：</strong> PoliFormer: Scaling On-Policy RL with Transformers Results in Masterful Navigators<br><strong>链接：</strong> <a href="https://arxiv.org/pdf/2406.20083">https://arxiv.org/pdf/2406.20083</a><br><strong>代码：</strong> <a href="https://github.com/allenai/poliformer">https://github.com/allenai/poliformer</a><br><strong>来源：</strong> CoRL 2024 (Outstanding Paper Award)<br><strong>单位：</strong> Allen Institute for AI</p>
<hr>
<h4 id="之前方法的缺陷"><a href="#之前方法的缺陷" class="headerlink" title="之前方法的缺陷"></a>之前方法的缺陷</h4><ul>
<li>基于 GRU 的结构在简单的数据集上取得了很好的效果，但在较难的数据集上结果比较差；<font color="#a5a5a5">（However, this approach fails to result in the same breakthroughs for harder navigation problems like Object Goal Navigation (ObjectNav) where an agent must explore its environment to locate and navigate to an object of the requested type.）</font></li>
<li>增大网络规模会导致模型训练不稳定且训练时间增加；<font color="#a5a5a5">（RL approaches for ObjectNav have generally not advanced beyond shallow GRU architectures due to challenges presented by training instability and unreasonably long training times with wider and deeper models, such as scaled-up transformers.）</font></li>
<li>Imitation Learning 可能存在状态空间探索不足的问题。<font color="#a5a5a5">（we suspect this is a consequence of insufficient state-space exploration as expert trajectory datasets frequently contain few examples of error recovery, which can lead to sub-optimal performance due to compounding errors or non-trivial domain shifts during inference.）</font></li>
</ul>
<hr>
<h4 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h4><p>从各个方面增大 RL 的 scale。</p>
<hr>
<h4 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h4><p><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-11-14%2013.57.35.png" alt="截屏2024-11-14 13.57.35.png"></p>
<ul>
<li>Encoder 根据当前输入的 RGB 图像以及 goal 编码生成当前的状态 $s^t$，decoder 用于预测 action；</li>
<li>Vision Transformer Model：DINOv2，训练时，该模块参数冻结，以保证模型在真实环境中的泛化性，否则视觉模块在模拟场景中的训练会影响到真实环境的性能；</li>
<li>Goal Encoder：为了方便比较，在不同 benchmark 上用了不同的编码器；</li>
<li>Causal Transformer Decoder：用 KV-cache 加速，使计算用时与时间成正比而非平方正比，加速计算。</li>
</ul>
<hr>
<h4 id="个人总结"><a href="#个人总结" class="headerlink" title="个人总结"></a>个人总结</h4><p>这篇文章通过增大 scale 大幅提升了 embodied navigation 任务的准确性。增大 scale 既包括从模型上采用 transformer 结构增大了模型的 scale；也包括从数据层面上，采用了一系列方法加快了数据的模拟和读取；还包括直接增加训练节点数量，提升训练速度。</p>
<hr>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <categories>
        <category>技术积累</category>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>Reinforcement_Learning</tag>
        <tag>Embodied_Navigation</tag>
        <tag>CoRL2024</tag>
      </tags>
  </entry>
</search>
